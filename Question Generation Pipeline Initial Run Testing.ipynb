{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c53a73e-1ced-4d81-8b3a-837a421674b7",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook implements a pipeline for generating high-quality question–answer (QA) pairs from technical telecom documents (in .docx format). The process is designed to prepare training data for extractive QA tasks using domain-specific text. The workflow includes the following steps:\n",
    "\n",
    "Text Cleaning:\n",
    "Raw text from a Word document is cleaned to remove boilerplate content such as figure/table captions, headers, page numbers, and irrelevant metadata (e.g., “draft”, “confidential”). This ensures that only meaningful technical sentences remain.\n",
    "\n",
    "Text Chunking\n",
    "The cleaned text is split into overlapping word-based chunks (default: 150 words with 30-word overlap). This helps models handle long documents while preserving context continuity.\n",
    "\n",
    "Model Loading:\n",
    "\n",
    "A T5-based Question Generation (QG) model generates candidate questions by highlighting key sentences in each chunk.\n",
    "\n",
    "A RoBERTa-based Question Answering (QA) model (fine-tuned on telecom data) extracts precise answer spans from the same context.\n",
    "\n",
    "QA Pair Generation\n",
    "For each chunk:\n",
    "\n",
    "Candidate sentences are selected and highlighted.\n",
    "\n",
    "The QG model proposes questions.\n",
    "\n",
    "The QA model extracts answer spans.\n",
    "\n",
    "Generated pairs are validated and low-quality outputs (too short, low confidence, trivial answers like “yes/no”) are skipped.\n",
    "Each valid pair is saved with metadata such as confidence score and source document.\n",
    "\n",
    "Filtering High-Quality QA Pairs\n",
    "After generation, the pairs are further filtered using rules (minimum confidence, context–answer consistency, vague question checks). Only “good” pairs are retained.\n",
    "\n",
    "Output\n",
    "The final QA pairs are saved in JSONL format, ready for downstream tasks such as fine-tuning extractive QA models or building telecom-specific LLM datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775b580-f542-49ec-9aed-e91577837628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Libraries\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e53db6-cadf-4feb-aed3-7c42385ed8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean Text\n",
    "def clean_technical_text_v2(raw_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line or len(line.split()) < 5:\n",
    "            continue\n",
    "        if re.match(r\"^(figure|fig\\.|table)\\s*\\d+\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.match(r\"^\\d+(\\.\\d+){0,4}\\s+[A-Z]\", line):\n",
    "            continue\n",
    "        if any(kw in line.lower() for kw in [\n",
    "            \"3gpp\", \"etsi\", \"confidential\", \"page\", \"table of contents\", \n",
    "            \"index\", \"appendix\", \"draft\", \"document history\"\n",
    "        ]):\n",
    "            continue\n",
    "        if line.isupper() and len(line.split()) < 10:\n",
    "            continue\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\(ETSI[^)]+\\)\", \"\", line)\n",
    "        cleaned_lines.append(line)\n",
    "    return \" \".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3759eda1-0d5c-40e6-a684-89aa58b1d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Chunking\n",
    "def split_with_overlap(text: str, max_words: int = 150, overlap: int = 30) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_words]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_words - overlap\n",
    "    return chunks\n",
    "\n",
    "def prepare_chunks_from_docx(docx_path: str, max_words=150, overlap=30) -> List[str]:\n",
    "    doc = Document(docx_path)\n",
    "    raw_text = \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
    "    cleaned = clean_technical_text_v2(raw_text)\n",
    "    chunks = split_with_overlap(cleaned, max_words=max_words, overlap=overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0710a5e4-9b4d-426f-9b90-c9fff2bae480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Models\n",
    "qg_model = pipeline(\"text2text-generation\", model=\"mrm8488/t5-base-finetuned-question-generation-ap\", device=0, batch_size=8)\n",
    "model_path = \"/home/ec2-user/qa_roberta_telecom\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "qa_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce815d86-bda1-43d3-87da-18d479b35acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate QA\n",
    "def generate_qa_pairs_from_chunks(chunks: List[str], source: str, batch_size=16) -> List[dict]:\n",
    "    qa_pairs = []\n",
    "    counters = {\"total_chunks\": len(chunks), \"qa_generated\": 0, \"qa_skipped\": 0, \"errors\": 0}\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Generating QA pairs\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        q_inputs = []\n",
    "        for chunk in batch:\n",
    "            sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip())\n",
    "            sentences = [s.strip() for s in sentences if len(s.split()) >= 5]\n",
    "            if len(sentences) < 2:\n",
    "                continue\n",
    "            top_sentences = sorted(sentences, key=len, reverse=True)[:2]\n",
    "            for sent in top_sentences:\n",
    "                highlighted = chunk.replace(sent, f\"<hl> {sent} <hl>\")\n",
    "                q_inputs.append(f\"highlight: {highlighted}\")\n",
    "\n",
    "        if not q_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions = qg_model(q_inputs, max_new_tokens=64)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QG batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(q_inputs)\n",
    "            continue\n",
    "\n",
    "        qa_inputs = [\n",
    "            {\"question\": q[\"generated_text\"], \"context\": ctx}\n",
    "            for q, ctx in zip(questions, batch * 2)\n",
    "            if not q[\"generated_text\"].lower().startswith(\"what is the highlight\")\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            question_list = [qa[\"question\"] for qa in qa_inputs]\n",
    "            context_list = [qa[\"context\"] for qa in qa_inputs]\n",
    "            answers = qa_model(question=question_list, context=context_list)\n",
    "            if isinstance(answers, dict):\n",
    "                answers = [answers]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QA batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(qa_inputs)\n",
    "            continue\n",
    "\n",
    "        for question, context, result in zip(question_list, context_list, answers):\n",
    "            answer = result[\"answer\"].strip()\n",
    "            score = result[\"score\"]\n",
    "\n",
    "            if score < 0.2 or len(answer) < 4 or not any(c.isalnum() for c in answer):\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in [\"yes\", \"no\", \"maybe\"]:\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in question.lower():\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() not in context.lower():\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "\n",
    "            qa = {\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{context}\\n\\n### Question:\\n{question}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"confidence\": round(score, 3)\n",
    "            }\n",
    "            qa_pairs.append(qa)\n",
    "            counters[\"qa_generated\"] += 1\n",
    "\n",
    "    print(f\" Total Chunks: {counters['total_chunks']}\")\n",
    "    print(f\" QA Pairs Generated: {counters['qa_generated']}\")\n",
    "    print(f\" Skipped: {counters['qa_skipped']} |  Errors: {counters['errors']}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa48fe0-2119-4bfe-b6dc-ea75ee4ac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full Pipeline Runner\n",
    "def process_docx_file(input_docx_path: str, output_jsonl_path: str):\n",
    "    input_path = Path(input_docx_path)\n",
    "    chunks = prepare_chunks_from_docx(input_docx_path)\n",
    "    qa_pairs = generate_qa_pairs_from_chunks(chunks, source=input_path.name)\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\" Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f618bc-bf99-4c2f-9665-ff34be279298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA pairs:  38%|████████▍             | 10/26 [00:35<00:59,  3.74s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating QA pairs: 100%|██████████████████████| 26/26 [01:35<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Total Chunks: 415\n",
      "✅ QA Pairs Generated: 48\n",
      "🚫 Skipped: 732 | ❌ Errors: 0\n",
      "✅ Saved 48 QA pairs to /mnt/data/qa_output_refactored.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_docx_file(\"/mnt/data/Datasets/38104-j00.docx\", \"/mnt/data/qa_output_refactored.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05afef73-21de-454d-a960-ab1d44133688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_good_qa_pairs(input_jsonl_path: str, output_jsonl_path: str, min_confidence: float = 0.4):\n",
    "    \"\"\"\n",
    "    Filter high-quality QA pairs using confidence, answer-context match, and vague question checks.\n",
    "    Saves only 'good' pairs to a new JSONL file.\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "    # Extract question and context for filtering\n",
    "    qa_df[\"question\"] = qa_df[\"input\"].str.extract(r\"### Question:\\n(.+?)\\n\\n### Answer:\")\n",
    "    qa_df[\"context\"] = qa_df[\"input\"].str.extract(r\"### Context:\\n(.+?)\\n\\n### Question:\", flags=re.DOTALL)\n",
    "\n",
    "    # Flags\n",
    "    qa_df[\"low_confidence\"] = qa_df[\"confidence\"].astype(float) < min_confidence\n",
    "    qa_df[\"mismatch\"] = ~qa_df.apply(lambda row: row[\"output\"].lower() in row[\"context\"].lower(), axis=1)\n",
    "    qa_df[\"vague_q\"] = qa_df[\"question\"].str.lower().str.startswith(\"what is the highlight\")\n",
    "    qa_df[\"short_q\"] = qa_df[\"question\"].str.split().str.len() < 4\n",
    "    qa_df[\"answer_in_question\"] = qa_df.apply(lambda row: row[\"output\"].lower() in row[\"question\"].lower(), axis=1)\n",
    "\n",
    "    # Final flag\n",
    "    qa_df[\"quality\"] = \"good\"\n",
    "    qa_df.loc[\n",
    "        qa_df[\"low_confidence\"] | qa_df[\"mismatch\"] | qa_df[\"vague_q\"] | qa_df[\"short_q\"] | qa_df[\"answer_in_question\"],\n",
    "        \"quality\"\n",
    "    ] = \"poor\"\n",
    "\n",
    "    # Filter and save\n",
    "    good_df = qa_df[qa_df[\"quality\"] == \"good\"]\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in good_df.to_dict(orient=\"records\"):\n",
    "            json.dump(record, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    print(f\"Filtered {len(good_df)} high-quality QA pairs saved to {output_jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f4ac13-f8d6-4348-979b-770d0a5c73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered 32 high-quality QA pairs saved to /mnt/data/qa_output_filtered_good.jsonl\n"
     ]
    }
   ],
   "source": [
    "filter_good_qa_pairs(\n",
    "    input_jsonl_path=\"/mnt/data/qa_output_refactored.jsonl\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output_filtered_good.jsonl\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
