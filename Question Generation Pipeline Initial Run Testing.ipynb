{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c53a73e-1ced-4d81-8b3a-837a421674b7",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook implements a pipeline for generating high-quality questionâ€“answer (QA) pairs from technical telecom documents (in .docx format). The process is designed to prepare training data for extractive QA tasks using domain-specific text. The workflow includes the following steps:\n",
    "\n",
    "Text Cleaning:\n",
    "Raw text from a Word document is cleaned to remove boilerplate content such as figure/table captions, headers, page numbers, and irrelevant metadata (e.g., â€œdraftâ€, â€œconfidentialâ€). This ensures that only meaningful technical sentences remain.\n",
    "\n",
    "Text Chunking\n",
    "The cleaned text is split into overlapping word-based chunks (default: 150 words with 30-word overlap). This helps models handle long documents while preserving context continuity.\n",
    "\n",
    "Model Loading:\n",
    "\n",
    "A T5-based Question Generation (QG) model generates candidate questions by highlighting key sentences in each chunk.\n",
    "\n",
    "A RoBERTa-based Question Answering (QA) model (fine-tuned on telecom data) extracts precise answer spans from the same context.\n",
    "\n",
    "QA Pair Generation\n",
    "For each chunk:\n",
    "\n",
    "Candidate sentences are selected and highlighted.\n",
    "\n",
    "The QG model proposes questions.\n",
    "\n",
    "The QA model extracts answer spans.\n",
    "\n",
    "Generated pairs are validated and low-quality outputs (too short, low confidence, trivial answers like â€œyes/noâ€) are skipped.\n",
    "Each valid pair is saved with metadata such as confidence score and source document.\n",
    "\n",
    "Filtering High-Quality QA Pairs\n",
    "After generation, the pairs are further filtered using rules (minimum confidence, contextâ€“answer consistency, vague question checks). Only â€œgoodâ€ pairs are retained.\n",
    "\n",
    "Output\n",
    "The final QA pairs are saved in JSONL format, ready for downstream tasks such as fine-tuning extractive QA models or building telecom-specific LLM datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775b580-f542-49ec-9aed-e91577837628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Libraries\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e53db6-cadf-4feb-aed3-7c42385ed8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean Text\n",
    "def clean_technical_text_v2(raw_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line or len(line.split()) < 5:\n",
    "            continue\n",
    "        if re.match(r\"^(figure|fig\\.|table)\\s*\\d+\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.match(r\"^\\d+(\\.\\d+){0,4}\\s+[A-Z]\", line):\n",
    "            continue\n",
    "        if any(kw in line.lower() for kw in [\n",
    "            \"3gpp\", \"etsi\", \"confidential\", \"page\", \"table of contents\", \n",
    "            \"index\", \"appendix\", \"draft\", \"document history\"\n",
    "        ]):\n",
    "            continue\n",
    "        if line.isupper() and len(line.split()) < 10:\n",
    "            continue\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\(ETSI[^)]+\\)\", \"\", line)\n",
    "        cleaned_lines.append(line)\n",
    "    return \" \".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3759eda1-0d5c-40e6-a684-89aa58b1d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Chunking\n",
    "def split_with_overlap(text: str, max_words: int = 150, overlap: int = 30) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_words]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_words - overlap\n",
    "    return chunks\n",
    "\n",
    "def prepare_chunks_from_docx(docx_path: str, max_words=150, overlap=30) -> List[str]:\n",
    "    doc = Document(docx_path)\n",
    "    raw_text = \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
    "    cleaned = clean_technical_text_v2(raw_text)\n",
    "    chunks = split_with_overlap(cleaned, max_words=max_words, overlap=overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0710a5e4-9b4d-426f-9b90-c9fff2bae480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Models\n",
    "qg_model = pipeline(\"text2text-generation\", model=\"mrm8488/t5-base-finetuned-question-generation-ap\", device=0, batch_size=8)\n",
    "model_path = \"/home/ec2-user/qa_roberta_telecom\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "qa_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce815d86-bda1-43d3-87da-18d479b35acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate QA\n",
    "def generate_qa_pairs_from_chunks(chunks: List[str], source: str, batch_size=16) -> List[dict]:\n",
    "    qa_pairs = []\n",
    "    counters = {\"total_chunks\": len(chunks), \"qa_generated\": 0, \"qa_skipped\": 0, \"errors\": 0}\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Generating QA pairs\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        q_inputs = []\n",
    "        for chunk in batch:\n",
    "            sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip())\n",
    "            sentences = [s.strip() for s in sentences if len(s.split()) >= 5]\n",
    "            if len(sentences) < 2:\n",
    "                continue\n",
    "            top_sentences = sorted(sentences, key=len, reverse=True)[:2]\n",
    "            for sent in top_sentences:\n",
    "                highlighted = chunk.replace(sent, f\"<hl> {sent} <hl>\")\n",
    "                q_inputs.append(f\"highlight: {highlighted}\")\n",
    "\n",
    "        if not q_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions = qg_model(q_inputs, max_new_tokens=64)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QG batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(q_inputs)\n",
    "            continue\n",
    "\n",
    "        qa_inputs = [\n",
    "            {\"question\": q[\"generated_text\"], \"context\": ctx}\n",
    "            for q, ctx in zip(questions, batch * 2)\n",
    "            if not q[\"generated_text\"].lower().startswith(\"what is the highlight\")\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            question_list = [qa[\"question\"] for qa in qa_inputs]\n",
    "            context_list = [qa[\"context\"] for qa in qa_inputs]\n",
    "            answers = qa_model(question=question_list, context=context_list)\n",
    "            if isinstance(answers, dict):\n",
    "                answers = [answers]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QA batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(qa_inputs)\n",
    "            continue\n",
    "\n",
    "        for question, context, result in zip(question_list, context_list, answers):\n",
    "            answer = result[\"answer\"].strip()\n",
    "            score = result[\"score\"]\n",
    "\n",
    "            if score < 0.2 or len(answer) < 4 or not any(c.isalnum() for c in answer):\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in [\"yes\", \"no\", \"maybe\"]:\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in question.lower():\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "            if answer.lower() not in context.lower():\n",
    "                counters[\"qa_skipped\"] += 1\n",
    "                continue\n",
    "\n",
    "            qa = {\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{context}\\n\\n### Question:\\n{question}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"confidence\": round(score, 3)\n",
    "            }\n",
    "            qa_pairs.append(qa)\n",
    "            counters[\"qa_generated\"] += 1\n",
    "\n",
    "    print(f\" Total Chunks: {counters['total_chunks']}\")\n",
    "    print(f\" QA Pairs Generated: {counters['qa_generated']}\")\n",
    "    print(f\" Skipped: {counters['qa_skipped']} |  Errors: {counters['errors']}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa48fe0-2119-4bfe-b6dc-ea75ee4ac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full Pipeline Runner\n",
    "def process_docx_file(input_docx_path: str, output_jsonl_path: str):\n",
    "    input_path = Path(input_docx_path)\n",
    "    chunks = prepare_chunks_from_docx(input_docx_path)\n",
    "    qa_pairs = generate_qa_pairs_from_chunks(chunks, source=input_path.name)\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\" Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f618bc-bf99-4c2f-9665-ff34be279298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA pairs:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 10/26 [00:35<00:59,  3.74s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating QA pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [01:35<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Total Chunks: 415\n",
      "âœ… QA Pairs Generated: 48\n",
      "ðŸš« Skipped: 732 | âŒ Errors: 0\n",
      "âœ… Saved 48 QA pairs to /mnt/data/qa_output_refactored.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_docx_file(\"/mnt/data/Datasets/38104-j00.docx\", \"/mnt/data/qa_output_refactored.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05afef73-21de-454d-a960-ab1d44133688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_good_qa_pairs(input_jsonl_path: str, output_jsonl_path: str, min_confidence: float = 0.4):\n",
    "    \"\"\"\n",
    "    Filter high-quality QA pairs using confidence, answer-context match, and vague question checks.\n",
    "    Saves only 'good' pairs to a new JSONL file.\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "    # Extract question and context for filtering\n",
    "    qa_df[\"question\"] = qa_df[\"input\"].str.extract(r\"### Question:\\n(.+?)\\n\\n### Answer:\")\n",
    "    qa_df[\"context\"] = qa_df[\"input\"].str.extract(r\"### Context:\\n(.+?)\\n\\n### Question:\", flags=re.DOTALL)\n",
    "\n",
    "    # Flags\n",
    "    qa_df[\"low_confidence\"] = qa_df[\"confidence\"].astype(float) < min_confidence\n",
    "    qa_df[\"mismatch\"] = ~qa_df.apply(lambda row: row[\"output\"].lower() in row[\"context\"].lower(), axis=1)\n",
    "    qa_df[\"vague_q\"] = qa_df[\"question\"].str.lower().str.startswith(\"what is the highlight\")\n",
    "    qa_df[\"short_q\"] = qa_df[\"question\"].str.split().str.len() < 4\n",
    "    qa_df[\"answer_in_question\"] = qa_df.apply(lambda row: row[\"output\"].lower() in row[\"question\"].lower(), axis=1)\n",
    "\n",
    "    # Final flag\n",
    "    qa_df[\"quality\"] = \"good\"\n",
    "    qa_df.loc[\n",
    "        qa_df[\"low_confidence\"] | qa_df[\"mismatch\"] | qa_df[\"vague_q\"] | qa_df[\"short_q\"] | qa_df[\"answer_in_question\"],\n",
    "        \"quality\"\n",
    "    ] = \"poor\"\n",
    "\n",
    "    # Filter and save\n",
    "    good_df = qa_df[qa_df[\"quality\"] == \"good\"]\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in good_df.to_dict(orient=\"records\"):\n",
    "            json.dump(record, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    print(f\"Filtered {len(good_df)} high-quality QA pairs saved to {output_jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f4ac13-f8d6-4348-979b-770d0a5c73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filtered 32 high-quality QA pairs saved to /mnt/data/qa_output_filtered_good.jsonl\n"
     ]
    }
   ],
   "source": [
    "filter_good_qa_pairs(\n",
    "    input_jsonl_path=\"/mnt/data/qa_output_refactored.jsonl\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output_filtered_good.jsonl\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
