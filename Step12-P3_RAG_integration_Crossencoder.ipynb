{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e299e0-b95d-414f-8b9b-aa5a4bf391a1",
   "metadata": {},
   "source": [
    "# Cross-Encoder(evaluates the pair (query + chunk) using a BERT-style encoder with attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be6348-d349-4eb5-a3d4-878be66ca111",
   "metadata": {},
   "source": [
    "## Notebook Summary: Hybrid QA with Cross-Encoder Reranking + Compound + Procedural QA\n",
    "\n",
    "This notebook implements a robust RAG-based extractive QA system for telecom documents, integrating multiple enhancements to improve factual accuracy and interpretability.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Cross-Encoder Reranking**  \n",
    "   Improves retrieval quality by reranking initial FAISS results using `cross-encoder/ms-marco-MiniLM-L-6-v2`, allowing deeper semantic alignment between query and chunks.\n",
    "\n",
    "2. **Compound Question Decomposition**  \n",
    "   Automatically splits multi-clause questions (e.g., with \"and\", \"or\") and answers each clause individually using separate prompts.\n",
    "\n",
    "3. **Procedural Multi-Span Extraction**  \n",
    "   For procedural or stepwise queries, uses regex-based patterns to extract actionable steps directly from context.\n",
    "\n",
    "4. **Auto-Routing Strategy**  \n",
    "   The pipeline selects between standard extractive QA and procedural span extraction based on query structure.\n",
    "\n",
    "5. **Evaluation Metrics**  \n",
    "   Evaluated on 100 curated QA pairs using:\n",
    "   - **SQuAD (EM / F1)**\n",
    "   - **ROUGE-L**\n",
    "   - **BLEU**\n",
    "\n",
    "This setup demonstrates the most advanced variant of the centralized RAG system in the project, optimized for telecom-specific factual and procedural queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2557e82b-b090-4162-b353-1ab38d42f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ce146f-6014-4cc1-bb89-c285bd489446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index and chunks\n",
    "index_path = \"/mnt/data/RAG/3gpp_index.faiss\"\n",
    "chunks_path = \"/mnt/data/RAG/3gpp_chunks.pkl\"\n",
    "\n",
    "index = faiss.read_index(index_path)\n",
    "with open(chunks_path, \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# Load embedding model used for indexing\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "def retrieve_context(query, top_k=3):\n",
    "    query_emb = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    D, I = index.search(query_emb.astype(\"float32\"), top_k)\n",
    "    return [documents[i] for i in I[0]]\n",
    "\n",
    "def retrieve_with_rerank(query, top_k=5):\n",
    "    # Step 1 — initial FAISS search\n",
    "    query_emb = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    D, I = index.search(np.array(query_emb).astype(\"float32\"), top_k * 2)  # wider net\n",
    "\n",
    "    initial_results = [documents[i] for i in I[0]]\n",
    "\n",
    "    # Step 2 — prepare (query, chunk) pairs\n",
    "    pairs = [(query, doc[\"content\"]) for doc in initial_results]\n",
    "\n",
    "    # Step 3 — rerank with cross-encoder\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(scores, initial_results), key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "    return [doc for _, doc in reranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f35911-a316-4e0a-9bef-fc3d3e048da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load Cross-Encoder model once\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1161f2be-274f-4fb5-ac51-530ff340c774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466151d1ac784281b0e152b2df47823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "    \"Do not paraphrase, summarize, or add extra information. \"\n",
    "    \"The answer must appear exactly in the context.\"\n",
    "    \"If the context lists multiple conditions, actions, or branches, include them all as written. \"\n",
    "    \"Do not summarize or paraphrase — copy the exact text from the context, line by line.\"\n",
    ")\n",
    "\n",
    "def build_rag_prompt(context_chunks, question):\n",
    "    combined_context = \"\\n\\n\".join([chunk['content'] for chunk in context_chunks])\n",
    "    user_prompt = (\n",
    "        f\"Context: {combined_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer from the context only:\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output5/final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18737767-aac6-4352-8d6d-c3e0442a443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(raw_text):\n",
    "    # Remove everything before the last [INST]\n",
    "    answer = raw_text.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    # Remove strange characters\n",
    "    answer = re.sub(r\"[^\\w\\s\\-.,:/()]\", \"\", answer)\n",
    "\n",
    "    # Remove repeating phrases like \"The key is... The key is... The key is...\"\n",
    "    answer = re.sub(r'(\\b.+?:)(\\s*\\1)+', r'\\1', answer)\n",
    "\n",
    "    # Trim repetitive word loops (e.g., \"structured as follows\" x 5)\n",
    "    tokens = answer.split()\n",
    "    for i in range(1, len(tokens) // 2):\n",
    "        if tokens[:i] == tokens[i:2*i]:\n",
    "            answer = \" \".join(tokens[:i])\n",
    "            break\n",
    "\n",
    "    # Optionally truncate to sentence boundary\n",
    "    sentence_end = re.search(r'[.?!]', answer)\n",
    "    if sentence_end:\n",
    "        answer = answer[:sentence_end.end()]\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd4b0ae-6c14-44cf-b615-10a222ab9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    return re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "def lexical_overlap(query, chunk):\n",
    "    q_tokens = set(normalize(query).split()) - STOPWORDS\n",
    "    c_tokens = set(normalize(chunk).split()) - STOPWORDS\n",
    "    return len(q_tokens & c_tokens) / (len(q_tokens | c_tokens) + 1e-5)\n",
    "\n",
    "def tfidf_score(query, chunk, vectorizer=None):\n",
    "    docs = [query, chunk]\n",
    "    if not vectorizer:\n",
    "        vectorizer = TfidfVectorizer().fit(docs)\n",
    "    vecs = vectorizer.transform(docs)\n",
    "    return (vecs[0] @ vecs[1].T).A[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a587137-69a8-4a00-bc89-c6b9c19cf04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_compound_question(q):\n",
    "    parts = re.split(r\"\\band\\b|\\bor\\b|[,;]\", q)\n",
    "    return [p.strip() for p in parts if len(p.strip().split()) > 3]\n",
    "\n",
    "\n",
    "def answer_with_rag_llama(question, top_k=5, verbose=False):\n",
    "    retrieved = retrieve_with_rerank(question, top_k=top_k)\n",
    "\n",
    "    sub_qs = split_compound_question(question)\n",
    "\n",
    "    # Handle compound question (multi-prompt)\n",
    "    if len(sub_qs) > 1:\n",
    "        answers = []\n",
    "        for sq in sub_qs:\n",
    "            sub_prompt = build_rag_prompt(retrieved, sq)\n",
    "            raw = qa_pipeline(\n",
    "                sub_prompt, \n",
    "                max_new_tokens=160, \n",
    "                do_sample=False, \n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )[0][\"generated_text\"]\n",
    "\n",
    "            ans = clean_prediction(raw)\n",
    "            answers.append(f\"→ {sq}: {ans}\")\n",
    "\n",
    "        full_answer = \"\\n\".join(answers)\n",
    "\n",
    "        # Context containment check (on full answer)\n",
    "        all_context = \" \".join([c[\"content\"] for c in retrieved])\n",
    "        if not any(ans.split(\": \", 1)[-1] in all_context for ans in answers):\n",
    "            print(\"🚨 One or more sub-answers not found in context — check retrieval or generation.\")\n",
    "        return full_answer, retrieved\n",
    "\n",
    "    # Handle simple (single-clause) question\n",
    "    prompt = build_rag_prompt(retrieved, question)\n",
    "    raw_output = qa_pipeline(\n",
    "        prompt, \n",
    "        max_new_tokens=160, \n",
    "        do_sample=False, \n",
    "        eos_token_id=tokenizer.eos_token_id, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    answer = clean_prediction(raw_output)\n",
    "\n",
    "    # Sanity check\n",
    "    if len(answer.split()) < 2 or len(answer.split()) > 40:\n",
    "        print(\"⚠️ Warning: Possibly bad output. Check content or retrieval.\")\n",
    "\n",
    "    # ✅ Context containment validation\n",
    "    all_context = \" \".join([c[\"content\"] for c in retrieved])\n",
    "    if answer not in all_context:\n",
    "        print(\"🚨 Answer not found in retrieved context — check prompt or retrieval quality.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"📌 Prompt:\\n\", prompt)\n",
    "        print(\"\\n🧾 Raw Output:\\n\", raw_output)\n",
    "        print(\"\\n✅ Cleaned Answer:\", answer)\n",
    "        for i, chunk in enumerate(retrieved):\n",
    "            print(f\"\\n--- Context {i+1} ---\")\n",
    "            print(chunk[\"content\"])\n",
    "\n",
    "    return answer, retrieved\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_multi_spans(context: str) -> list:\n",
    "    \"\"\"Extract procedural-style sentences from telecom context using regex patterns.\"\"\"\n",
    "    spans = []\n",
    "\n",
    "    # Patterns that catch conditional and action rules\n",
    "    patterns = [\n",
    "        r\"(?i)(?:upon|when|if|after|before).+?shall.+?[.;]\",  # conditional + shall\n",
    "        r\"(?i)the (?:ue|amf|network|nas|gnb).+?shall.+?[.;]\",  # direct instructions\n",
    "        r\"(?i)-\\s*.+?shall.+?[.;]\",  # bullet points with 'shall'\n",
    "        r\"(?i)the (?:ue|amf|nas|network).+?enters.+?[.;]\",     # entry triggers\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, context)\n",
    "        spans.extend(matches)\n",
    "\n",
    "    import difflib\n",
    "\n",
    "    def is_similar(a, b, threshold=0.85):\n",
    "        return difflib.SequenceMatcher(None, a, b).ratio() > threshold\n",
    "    \n",
    "    unique_spans = []\n",
    "    for s in spans:\n",
    "        cleaned = s.strip()\n",
    "        if not any(is_similar(cleaned, u) for u in unique_spans):\n",
    "            unique_spans.append(cleaned)\n",
    "    \n",
    "    return unique_spans[:6]\n",
    "\n",
    "def answer_with_rag_llama_multispan(question, top_k=5, verbose=False):\n",
    "    retrieved = retrieve_with_rerank(question, top_k=top_k)\n",
    "    combined_context = \" \".join([re.sub(r'\\s+', ' ', c[\"content\"]) for c in retrieved])\n",
    "    spans = extract_multi_spans(combined_context)  # uses regex\n",
    "\n",
    "    if not spans:\n",
    "        return \"⚠️ No clear steps found in context.\", retrieved\n",
    "\n",
    "    final = \"\\n\".join([f\"• {s.strip()}\" for s in spans])\n",
    "    return final, retrieved\n",
    "\n",
    "def route_question_to_best_strategy(question, top_k=5, verbose=False):\n",
    "    # Use multi-span for procedural questions\n",
    "    if any(q in question.lower() for q in [\"steps\", \"procedures\", \"when does\", \"what happens if\", \"if the ue\"]):\n",
    "        return answer_with_rag_llama_multispan(question, top_k=top_k, verbose=verbose)\n",
    "    else:\n",
    "        return answer_with_rag_llama(question, top_k=top_k, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca93b4a-d7fd-40a4-ae85-a269074f0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  1%|▍                                          | 1/100 [00:03<06:09,  3.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n",
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                          | 2/100 [00:05<03:45,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                         | 3/100 [00:12<07:51,  4.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                         | 4/100 [00:20<09:37,  6.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                        | 5/100 [00:24<08:21,  5.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  6%|██▌                                        | 6/100 [00:32<09:33,  6.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  7%|███                                        | 7/100 [00:40<10:21,  6.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                       | 8/100 [00:46<09:56,  6.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  9%|███▊                                       | 9/100 [00:54<10:29,  6.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 10%|████▏                                     | 10/100 [01:01<10:44,  7.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▌                                     | 11/100 [01:09<10:51,  7.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████                                     | 12/100 [01:18<11:19,  7.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n",
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▍                                    | 13/100 [01:26<11:13,  7.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▉                                    | 14/100 [01:33<11:06,  7.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▎                                   | 15/100 [01:41<11:01,  7.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▋                                   | 16/100 [01:42<07:59,  5.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 17%|███████▏                                  | 17/100 [01:50<08:39,  6.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 18%|███████▌                                  | 18/100 [01:57<09:11,  6.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 19%|███████▉                                  | 19/100 [02:06<09:38,  7.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▍                                 | 20/100 [02:13<09:43,  7.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▊                                 | 21/100 [02:15<07:35,  5.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 22%|█████████▏                                | 22/100 [02:23<08:16,  6.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▋                                | 23/100 [02:31<08:40,  6.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████                                | 24/100 [02:38<08:50,  6.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▌                               | 25/100 [02:46<09:07,  7.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 26%|██████████▉                               | 26/100 [02:47<06:40,  5.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 27%|███████████▎                              | 27/100 [02:55<07:34,  6.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▊                              | 28/100 [03:03<08:05,  6.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▏                             | 29/100 [03:11<08:19,  7.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 30%|████████████▌                             | 30/100 [03:19<08:26,  7.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 31%|█████████████                             | 31/100 [03:20<06:10,  5.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n",
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████▍                            | 32/100 [03:21<04:32,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▊                            | 33/100 [03:21<03:22,  3.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 34%|██████████████▎                           | 34/100 [03:28<04:22,  3.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 35%|██████████████▋                           | 35/100 [03:29<03:29,  3.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████                           | 36/100 [03:37<04:51,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████▌                          | 37/100 [03:44<05:45,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▉                          | 38/100 [03:45<04:04,  3.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 39%|████████████████▍                         | 39/100 [03:48<03:40,  3.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▊                         | 40/100 [03:50<03:07,  3.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 41%|█████████████████▏                        | 41/100 [03:57<04:25,  4.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 42%|█████████████████▋                        | 42/100 [04:05<05:19,  5.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 43%|██████████████████                        | 43/100 [04:10<05:07,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████▍                       | 44/100 [04:18<05:43,  6.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████▉                       | 45/100 [04:26<06:03,  6.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████▎                      | 46/100 [04:28<04:41,  5.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████▋                      | 47/100 [04:35<05:12,  5.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████████████▏                     | 48/100 [04:36<03:47,  4.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████▌                     | 49/100 [04:37<02:55,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████                     | 50/100 [04:38<02:14,  2.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 51%|█████████████████████▍                    | 51/100 [04:39<01:47,  2.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████▊                    | 52/100 [04:47<03:05,  3.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 53%|██████████████████████▎                   | 53/100 [04:48<02:20,  2.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████▋                   | 54/100 [04:56<03:27,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 55%|███████████████████████                   | 55/100 [05:01<03:31,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████▌                  | 56/100 [05:03<02:41,  3.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 58%|████████████████████████▎                 | 58/100 [05:10<02:25,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 59%|████████████████████████▊                 | 59/100 [05:18<03:16,  4.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▏                | 60/100 [05:26<03:45,  5.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▌                | 61/100 [05:33<04:02,  6.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████                | 62/100 [05:34<02:57,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 63%|██████████████████████████▍               | 63/100 [05:42<03:25,  5.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████▉               | 64/100 [05:50<03:44,  6.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████▎              | 65/100 [05:57<03:50,  6.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 66%|███████████████████████████▋              | 66/100 [06:05<03:55,  6.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 67%|████████████████████████████▏             | 67/100 [06:10<03:26,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████▌             | 68/100 [06:10<02:23,  4.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 69%|████████████████████████████▉             | 69/100 [06:18<02:49,  5.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 71%|█████████████████████████████▊            | 71/100 [06:19<01:25,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 72%|██████████████████████████████▏           | 72/100 [06:27<02:04,  4.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 74%|███████████████████████████████           | 74/100 [06:29<01:07,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████▌          | 75/100 [06:36<01:41,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 76%|███████████████████████████████▉          | 76/100 [06:44<02:03,  5.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n",
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████▎         | 77/100 [06:47<01:40,  4.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 78%|████████████████████████████████▊         | 78/100 [06:51<01:38,  4.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 79%|█████████████████████████████████▏        | 79/100 [06:59<01:56,  5.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████▌        | 80/100 [07:07<02:04,  6.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████        | 81/100 [07:15<02:06,  6.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████▍       | 82/100 [07:16<01:29,  4.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 83%|██████████████████████████████████▊       | 83/100 [07:17<01:05,  3.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████▎      | 84/100 [07:23<01:08,  4.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n",
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████▋      | 85/100 [07:30<01:18,  5.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 86%|████████████████████████████████████      | 86/100 [07:33<01:02,  4.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████▌     | 87/100 [07:41<01:11,  5.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 88%|████████████████████████████████████▉     | 88/100 [07:43<00:53,  4.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████▍    | 89/100 [07:51<01:00,  5.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████▊    | 90/100 [07:58<01:01,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████▏   | 91/100 [07:58<00:39,  4.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [08:02<00:32,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████   | 93/100 [08:10<00:36,  5.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████▍  | 94/100 [08:17<00:35,  5.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Possibly bad output. Check content or retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████▉  | 95/100 [08:21<00:26,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████▎ | 96/100 [08:25<00:19,  4.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████▋ | 97/100 [08:27<00:12,  4.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [08:29<00:06,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████▌| 99/100 [08:32<00:03,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|█████████████████████████████████████████| 100/100 [08:33<00:00,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Answer not found in retrieved context — check prompt or retrieval quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Evaluation Results (Setup 4 — Cross-Encoder + Compound + Procedural):\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score        : 19.51\n",
      "ROUGE-L         : 0.2018\n",
      "BLEU            : 0.0092\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "\n",
    "# Load QA pairs\n",
    "def load_qa_pairs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "qa_pairs = load_qa_pairs(\"3gpp_qa_100_pairs.jsonl\")\n",
    "\n",
    "# Load metrics\n",
    "squad_metric = load(\"squad\")\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "bleu_predictions = []\n",
    "bleu_references = []\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(qa_pairs):\n",
    "    question = sample[\"question\"]\n",
    "    reference = sample[\"answer\"]\n",
    "\n",
    "    try:\n",
    "        prediction, _ = route_question_to_best_strategy(question)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error on: {question}\\n{e}\")\n",
    "        prediction = \"\"\n",
    "\n",
    "    # Add to metrics\n",
    "    squad_metric.add(\n",
    "        prediction={\"id\": str(hash(question)), \"prediction_text\": prediction},\n",
    "        reference={\"id\": str(hash(question)), \"answers\": {\"text\": [reference], \"answer_start\": [0]}}\n",
    "    )\n",
    "    rouge.add(prediction=prediction, reference=reference)\n",
    "    bleu_predictions.append(prediction)\n",
    "    bleu_references.append([reference])\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "\n",
    "# Compute final scores\n",
    "squad_scores = squad_metric.compute()\n",
    "rouge_scores = rouge.compute()\n",
    "bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)[\"bleu\"]\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 Final Evaluation Results (Setup 4 — Cross-Encoder + Compound + Procedural):\")\n",
    "print(f\"Exact Match (EM): {squad_scores['exact_match']:.2f}\")\n",
    "print(f\"F1 Score        : {squad_scores['f1']:.2f}\")\n",
    "print(f\"ROUGE-L         : {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"BLEU            : {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
