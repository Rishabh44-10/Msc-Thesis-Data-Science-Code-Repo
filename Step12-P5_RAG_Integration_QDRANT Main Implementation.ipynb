{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793964a7-6e8d-46da-968b-5bc88198c0e6",
   "metadata": {},
   "source": [
    "# 1) Qdrant for persistent, scalable vector retrieval\n",
    "# 2) Cross-Encoder to rerank retrieved chunks\n",
    "# 3) Your fine-tuned LLaMA-2 model (via Hugging Face pipeline) for precise extractive QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503cdae-3de5-4efe-a47f-54ebc3fca27f",
   "metadata": {},
   "source": [
    "## Notebook Summary: Qdrant + Cross-Encoder RAG Evaluation (Setup 5)\n",
    "\n",
    "This notebook evaluates a Qdrant-powered RAG pipeline for telecom-specific extractive QA using a fine-tuned LLaMA-2 model.\n",
    "\n",
    "### Pipeline Overview:\n",
    "\n",
    "1. **Qdrant Retrieval**  \n",
    "   Retrieves top-k chunks from a persistent Qdrant vector store using MiniLM embeddings and cosine similarity.\n",
    "\n",
    "2. **Cross-Encoder Reranking**  \n",
    "   Applies `cross-encoder/ms-marco-MiniLM-L-6-v2` to rerank retrieved chunks for better alignment with the query intent.\n",
    "\n",
    "3. **Compound Question Support**  \n",
    "   Splits multi-part queries into subquestions and answers them independently. Combines sub-answers into a final output.\n",
    "\n",
    "4. **Prompting and Inference**  \n",
    "   Uses an extractive prompt format compatible with LLaMA-2 and performs generation using a LoRA-fine-tuned QA model on GPU.\n",
    "\n",
    "5. **Evaluation Metrics**  \n",
    "   Computes:\n",
    "   - **Exact Match (EM)** and **F1** (SQuAD)\n",
    "   - **ROUGE-L**\n",
    "   - **BLEU**\n",
    "\n",
    "This setup demonstrates a high-precision, production-ready RAG architecture using Qdrant and cross-encoder reranking for improved factual grounding in telecom QA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842b5d8a-1eac-4ca2-9a15-566f566c9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf4b72b-5764-4406-b5fd-d57f36de10e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289937f5759f43db9f52d66bbb950579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch, re\n",
    "\n",
    "# Qdrant connection\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Embedding model (same as used for indexing)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Cross-Encoder for reranking\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Load fine-tuned LLaMA-2 QA model\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output5/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "qa_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text-generation\", model=qa_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e0e5aa-05ef-4647-b4b1-1d6b41381951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_qdrant_rerank(question, top_k=5):\n",
    "    query_vec = embedding_model.encode(question, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # Search wider in Qdrant (top_k * 2)\n",
    "    results = client.search(\n",
    "        collection_name=\"3gpp_chunks\",\n",
    "        query_vector=query_vec,\n",
    "        limit=top_k * 2\n",
    "    )\n",
    "\n",
    "    initial = [{\n",
    "        \"content\": r.payload[\"content\"],\n",
    "        \"source\": r.payload[\"source\"]\n",
    "    } for r in results]\n",
    "\n",
    "    # Rerank using Cross-Encoder\n",
    "    pairs = [(question, doc[\"content\"]) for doc in initial]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(scores, initial), key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "    return [doc for _, doc in reranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa270206-2efa-4be6-96be-a05f5114caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "    \"Do not paraphrase, summarize, or add extra information. \"\n",
    "    \"The answer must appear exactly in the context. \"\n",
    "    \"If the context lists multiple conditions, actions, or branches, include them all as written.\"\n",
    ")\n",
    "\n",
    "def build_rag_prompt(context_chunks, question):\n",
    "    combined_context = \"\\n\\n\".join([chunk['content'] for chunk in context_chunks])\n",
    "    user_prompt = (\n",
    "        f\"Context: {combined_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer from the context only:\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54ab66f-ee4b-4215-a495-3d5504a2c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(raw_text):\n",
    "    answer = raw_text.split(\"[/INST]\")[-1].strip()\n",
    "    answer = re.sub(r\"[^\\w\\s\\-.,:/()]\", \"\", answer)\n",
    "    answer = re.sub(r'(\\b.+?:)(\\s*\\1)+', r'\\1', answer)\n",
    "\n",
    "    tokens = answer.split()\n",
    "    for i in range(1, len(tokens) // 2):\n",
    "        if tokens[:i] == tokens[i:2*i]:\n",
    "            return \" \".join(tokens[:i])\n",
    "\n",
    "    sentence_end = re.search(r'[.?!]', answer)\n",
    "    if sentence_end:\n",
    "        answer = answer[:sentence_end.end()]\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bee472-2331-4587-92bd-0d155ae7ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_compound_question(q):\n",
    "    parts = re.split(r\"\\band\\b|\\bor\\b|[,;]\", q)\n",
    "    return [p.strip() for p in parts if len(p.strip().split()) > 3]\n",
    "\n",
    "def answer_with_qdrant_llama(question, top_k=5, verbose=False):\n",
    "    retrieved = retrieve_with_qdrant_rerank(question, top_k=top_k)\n",
    "    sub_qs = split_compound_question(question)\n",
    "\n",
    "    if len(sub_qs) > 1:\n",
    "        answers = []\n",
    "        for sq in sub_qs:\n",
    "            sub_prompt = build_rag_prompt(retrieved, sq)\n",
    "            raw = qa_pipeline(sub_prompt, max_new_tokens=160, do_sample=False,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "            ans = clean_prediction(raw)\n",
    "            answers.append(f\"→ {sq}: {ans}\")\n",
    "\n",
    "        final = \"\\n\".join(answers)\n",
    "        if verbose:\n",
    "            print(\"\\n\".join([f\"Context {i+1}:\\n{c['content']}\" for i, c in enumerate(retrieved)]))\n",
    "        return final, retrieved\n",
    "\n",
    "    # Simple case\n",
    "    prompt = build_rag_prompt(retrieved, question)\n",
    "    raw = qa_pipeline(prompt, max_new_tokens=160, do_sample=False,\n",
    "                      eos_token_id=tokenizer.eos_token_id,\n",
    "                      pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "    answer = clean_prediction(raw)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"📌 Prompt:\\n\", prompt)\n",
    "        print(\"\\n🧾 Raw Output:\\n\", raw)\n",
    "        print(\"\\n✅ Cleaned Answer:\", answer)\n",
    "        for i, chunk in enumerate(retrieved):\n",
    "            print(f\"\\n--- Context {i+1} ---\\n{chunk['content']}\")\n",
    "\n",
    "    return answer, retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1682db22-0b1b-4efc-8fb0-987453e46737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_3944/1202240318.py:5: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  1%|▍                                          | 1/100 [00:08<13:27,  8.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  2%|▊                                          | 2/100 [00:15<12:54,  7.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  3%|█▎                                         | 3/100 [00:23<12:37,  7.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  4%|█▋                                         | 4/100 [00:31<12:26,  7.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  5%|██▏                                        | 5/100 [00:39<12:22,  7.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  6%|██▌                                        | 6/100 [00:46<12:11,  7.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  7%|███                                        | 7/100 [00:54<12:04,  7.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  8%|███▍                                       | 8/100 [00:55<08:32,  5.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  9%|███▊                                       | 9/100 [01:03<09:25,  6.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 10%|████▏                                     | 10/100 [01:10<09:58,  6.65s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 11%|████▌                                     | 11/100 [01:18<10:23,  7.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 12%|█████                                     | 12/100 [01:19<07:38,  5.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 13%|█████▍                                    | 13/100 [01:27<08:38,  5.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 14%|█████▉                                    | 14/100 [01:35<09:19,  6.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 15%|██████▎                                   | 15/100 [01:42<09:37,  6.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 16%|██████▋                                   | 16/100 [01:43<07:10,  5.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 17%|███████▏                                  | 17/100 [01:51<08:13,  5.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 18%|███████▌                                  | 18/100 [01:59<08:52,  6.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 19%|███████▉                                  | 19/100 [02:07<09:16,  6.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 20%|████████▍                                 | 20/100 [02:08<07:03,  5.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 21%|████████▊                                 | 21/100 [02:09<05:13,  3.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 22%|█████████▏                                | 22/100 [02:10<03:58,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 23%|█████████▋                                | 23/100 [02:11<03:06,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 24%|██████████                                | 24/100 [02:13<02:42,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 25%|██████████▌                               | 25/100 [02:20<04:49,  3.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 26%|██████████▉                               | 26/100 [02:28<06:10,  5.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 27%|███████████▎                              | 27/100 [02:29<04:40,  3.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 28%|███████████▊                              | 28/100 [02:37<05:59,  5.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 29%|████████████▏                             | 29/100 [02:44<06:47,  5.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 30%|████████████▌                             | 30/100 [02:46<05:19,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 31%|█████████████                             | 31/100 [02:54<06:15,  5.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 32%|█████████████▍                            | 32/100 [03:01<06:55,  6.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 33%|█████████████▊                            | 33/100 [03:09<07:19,  6.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 34%|██████████████▎                           | 34/100 [03:10<05:20,  4.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 35%|██████████████▋                           | 35/100 [03:18<06:11,  5.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 36%|███████████████                           | 36/100 [03:19<04:36,  4.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 37%|███████████████▌                          | 37/100 [03:27<05:38,  5.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 38%|███████████████▉                          | 38/100 [03:34<06:12,  6.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 39%|████████████████▍                         | 39/100 [03:35<04:34,  4.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 40%|████████████████▊                         | 40/100 [03:43<05:25,  5.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 41%|█████████████████▏                        | 41/100 [03:50<05:58,  6.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 42%|█████████████████▋                        | 42/100 [03:58<06:21,  6.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 43%|██████████████████                        | 43/100 [04:01<05:12,  5.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 44%|██████████████████▍                       | 44/100 [04:02<03:49,  4.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 45%|██████████████████▉                       | 45/100 [04:09<04:42,  5.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 46%|███████████████████▎                      | 46/100 [04:17<05:18,  5.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 47%|███████████████████▋                      | 47/100 [04:25<05:42,  6.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 48%|████████████████████▏                     | 48/100 [04:33<05:58,  6.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 49%|████████████████████▌                     | 49/100 [04:40<06:04,  7.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 50%|█████████████████████                     | 50/100 [04:48<06:07,  7.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 51%|█████████████████████▍                    | 51/100 [04:50<04:33,  5.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 52%|█████████████████████▊                    | 52/100 [04:59<05:16,  6.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 53%|██████████████████████▎                   | 53/100 [05:07<05:28,  6.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 54%|██████████████████████▋                   | 54/100 [05:14<05:33,  7.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 55%|███████████████████████                   | 55/100 [05:16<04:10,  5.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 56%|███████████████████████▌                  | 56/100 [05:17<03:08,  4.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 57%|███████████████████████▉                  | 57/100 [05:18<02:23,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 58%|████████████████████████▎                 | 58/100 [05:26<03:17,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 59%|████████████████████████▊                 | 59/100 [05:34<03:49,  5.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 60%|█████████████████████████▏                | 60/100 [05:42<04:08,  6.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 61%|█████████████████████████▌                | 61/100 [05:49<04:21,  6.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 62%|██████████████████████████                | 62/100 [05:50<03:08,  4.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 63%|██████████████████████████▍               | 63/100 [05:58<03:37,  5.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 64%|██████████████████████████▉               | 64/100 [06:01<02:51,  4.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 65%|███████████████████████████▎              | 65/100 [06:08<03:17,  5.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 66%|███████████████████████████▋              | 66/100 [06:16<03:35,  6.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 67%|████████████████████████████▏             | 67/100 [06:17<02:35,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 68%|████████████████████████████▌             | 68/100 [06:25<03:01,  5.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 69%|████████████████████████████▉             | 69/100 [06:33<03:14,  6.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 70%|█████████████████████████████▍            | 70/100 [06:34<02:22,  4.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 71%|█████████████████████████████▊            | 71/100 [06:41<02:41,  5.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 72%|██████████████████████████████▏           | 72/100 [06:43<01:59,  4.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 73%|██████████████████████████████▋           | 73/100 [06:51<02:25,  5.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 74%|███████████████████████████████           | 74/100 [06:58<02:37,  6.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 75%|███████████████████████████████▌          | 75/100 [06:59<01:53,  4.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 76%|███████████████████████████████▉          | 76/100 [07:01<01:29,  3.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 77%|████████████████████████████████▎         | 77/100 [07:04<01:17,  3.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 78%|████████████████████████████████▊         | 78/100 [07:12<01:43,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 79%|█████████████████████████████████▏        | 79/100 [07:20<02:00,  5.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 80%|█████████████████████████████████▌        | 80/100 [07:27<02:06,  6.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 81%|██████████████████████████████████        | 81/100 [07:36<02:11,  6.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 82%|██████████████████████████████████▍       | 82/100 [07:43<02:07,  7.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 83%|██████████████████████████████████▊       | 83/100 [07:45<01:34,  5.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 84%|███████████████████████████████████▎      | 84/100 [07:53<01:37,  6.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 85%|███████████████████████████████████▋      | 85/100 [08:00<01:37,  6.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 86%|████████████████████████████████████      | 86/100 [08:06<01:27,  6.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 87%|████████████████████████████████████▌     | 87/100 [08:07<01:02,  4.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 88%|████████████████████████████████████▉     | 88/100 [08:15<01:08,  5.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [08:23<01:09,  6.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 90%|█████████████████████████████████████▊    | 90/100 [08:31<01:07,  6.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [08:38<01:03,  7.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [08:46<00:58,  7.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 93%|███████████████████████████████████████   | 93/100 [08:47<00:37,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 94%|███████████████████████████████████████▍  | 94/100 [08:55<00:37,  6.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [08:56<00:23,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 96%|████████████████████████████████████████▎ | 96/100 [08:58<00:15,  3.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [09:06<00:14,  4.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [09:07<00:07,  3.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [09:15<00:05,  5.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|█████████████████████████████████████████| 100/100 [09:17<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Evaluation Results (Setup 5 — Qdrant + Cross-Encoder + Compound):\n",
      "Exact Match (EM): 1.00\n",
      "F1 Score        : 19.21\n",
      "ROUGE-L         : 0.2118\n",
      "BLEU            : 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "\n",
    "# Load QA pairs\n",
    "def load_qa_pairs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "qa_pairs = load_qa_pairs(\"3gpp_qa_100_pairs.jsonl\")\n",
    "\n",
    "# Load metrics\n",
    "squad_metric = load(\"squad\")\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "bleu_predictions = []\n",
    "bleu_references = []\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(qa_pairs):\n",
    "    question = sample[\"question\"]\n",
    "    reference = sample[\"answer\"]\n",
    "\n",
    "    try:\n",
    "        prediction, _ = answer_with_qdrant_llama(question)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error on: {question}\\n{e}\")\n",
    "        prediction = \"\"\n",
    "\n",
    "    # Add to metrics\n",
    "    squad_metric.add(\n",
    "        prediction={\"id\": str(hash(question)), \"prediction_text\": prediction},\n",
    "        reference={\"id\": str(hash(question)), \"answers\": {\"text\": [reference], \"answer_start\": [0]}}\n",
    "    )\n",
    "    rouge.add(prediction=prediction, reference=reference)\n",
    "    bleu_predictions.append(prediction)\n",
    "    bleu_references.append([reference])\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "\n",
    "# Compute final scores\n",
    "squad_scores = squad_metric.compute()\n",
    "rouge_scores = rouge.compute()\n",
    "bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)[\"bleu\"]\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 Final Evaluation Results (Setup 5 — Qdrant + Cross-Encoder + Compound):\")\n",
    "print(f\"Exact Match (EM): {squad_scores['exact_match']:.2f}\")\n",
    "print(f\"F1 Score        : {squad_scores['f1']:.2f}\")\n",
    "print(f\"ROUGE-L         : {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"BLEU            : {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
