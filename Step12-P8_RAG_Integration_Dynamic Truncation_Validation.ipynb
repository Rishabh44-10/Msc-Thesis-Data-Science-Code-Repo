{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe869ae3-3b95-4c4e-a2be-fcd2e3c47814",
   "metadata": {},
   "source": [
    "# Dynamic Truncation + Validation + Cross-Encoder + Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459cf34-15ad-4d61-88e3-44b10786a1f1",
   "metadata": {},
   "source": [
    "## Notebook Summary: Truncation + Validation Fusion RAG (Setup 8)\n",
    "\n",
    "This notebook implements an optimized RAG pipeline for extractive QA over telecom documents, integrating robust safeguards against hallucination and prompt overload.\n",
    "\n",
    "### Key Enhancements:\n",
    "\n",
    "1. **Cross-Encoder Reranking**  \n",
    "   Initial chunk retrieval is reranked using `cross-encoder/ms-marco-MiniLM-L-6-v2` for deep semantic alignment.\n",
    "\n",
    "2. **Dynamic Truncation**  \n",
    "   Top-ranked chunks are split into overlapping word windows (150 tokens, stride 75), and re-scored using a weighted combination of TF-IDF and lexical overlap. Only the most relevant spans are retained.\n",
    "\n",
    "3. **Fusion Prompting**  \n",
    "   Selected spans are fused into a single prompt, annotated with source identifiers, and passed to a LoRA-fine-tuned LLaMA-2 model for extractive generation.\n",
    "\n",
    "4. **Fuzzy Validation**  \n",
    "   Ensures that the final answer text is approximately contained within the retrieved spans, using token similarity. Warnings are issued if validation fails.\n",
    "\n",
    "5. **Evaluation**  \n",
    "   Assessed on 100 QA pairs with:\n",
    "   - **SQuAD (Exact Match, F1)**\n",
    "   - **ROUGE-L**\n",
    "   - **BLEU**\n",
    "\n",
    "This setup provides the most robust centralized RAG architecture so far — balancing precision, safety, and context richness — and is a strong candidate for downstream deployment or FL extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea16f20-349c-4f53-8533-0325a77da7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ec0275-1b74-44e7-a343-41d2cd014a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def is_similar(a, b, threshold=0.75):\n",
    "    return SequenceMatcher(None, a, b).ratio() >= threshold\n",
    "\n",
    "def truncate_and_filter_chunks(chunks, query, window_size=150, stride=75, max_windows=5):\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def normalize(text):\n",
    "        return re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    def lexical_overlap(query, span):\n",
    "        q_tokens = set(normalize(query).split()) - STOPWORDS\n",
    "        c_tokens = set(normalize(span).split()) - STOPWORDS\n",
    "        return len(q_tokens & c_tokens) / (len(q_tokens | c_tokens) + 1e-5)\n",
    "\n",
    "    def tfidf_score(query, span):\n",
    "        vec = TfidfVectorizer().fit([query, span])\n",
    "        X = vec.transform([query, span])\n",
    "        return (X[0] @ X[1].T).A[0][0]\n",
    "\n",
    "    scored_spans = []\n",
    "    for chunk in chunks:\n",
    "        words = chunk[\"content\"].split()\n",
    "        for i in range(0, len(words), stride):\n",
    "            span_words = words[i:i + window_size]\n",
    "            if len(span_words) < 30:\n",
    "                continue\n",
    "            span = \" \".join(span_words)\n",
    "            score = 0.6 * lexical_overlap(query, span) + 0.4 * tfidf_score(query, span)\n",
    "            scored_spans.append({\n",
    "                \"content\": span,\n",
    "                \"score\": score,\n",
    "                \"source\": chunk.get(\"source\", \"unknown\")\n",
    "            })\n",
    "\n",
    "    return sorted(scored_spans, key=lambda x: x[\"score\"], reverse=True)[:max_windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fe514a-6df6-40b4-bbf4-9fd61a2e3f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f23dc66eb94be1a2f15663b2bec2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load FAISS index and chunked docs\n",
    "index = faiss.read_index(\"/mnt/data/RAG/3gpp_index.faiss\")\n",
    "with open(\"/mnt/data/RAG/3gpp_chunks.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# Load embedding + cross-encoder models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# FAISS + Cross-Encoder Retrieval\n",
    "def retrieve_with_rerank(query, top_k=5):\n",
    "    query_vec = embedding_model.encode(query, normalize_embeddings=True)\n",
    "    query_vec = np.array(query_vec).reshape(1, -1).astype(\"float32\") \n",
    "\n",
    "    D, I = index.search(query_vec, top_k * 2)\n",
    "\n",
    "    initial_results = [documents[i] for i in I[0]]\n",
    "    pairs = [(query, doc[\"content\"]) for doc in initial_results]\n",
    "\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(scores, initial_results), key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "    return [doc for _, doc in reranked]\n",
    "\n",
    "# Multi-Chunk Fusion Prompt Builder\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "    \"Do not paraphrase, summarize, or add extra information. \"\n",
    "    \"The answer must appear exactly in the context. \"\n",
    "    \"If the context lists multiple conditions, actions, or branches, include them all as written. \"\n",
    "    \"Do not summarize or paraphrase — copy the exact text from the context, line by line.\"\n",
    ")\n",
    "\n",
    "def build_fusion_prompt(context_chunks, question):\n",
    "    context_lines = []\n",
    "    for chunk in context_chunks:\n",
    "        source = chunk.get(\"source\", \"unknown\").split(\"/\")[-1]\n",
    "        context_lines.append(f\"[Source: {source}]\\n-----\\n{chunk['content'].strip()}\")\n",
    "    fused_context = \"\\n\\n\".join(context_lines)\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Context:\\n{fused_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer from the context only:\"\n",
    "    )\n",
    "\n",
    "    return f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "# Output Cleaning\n",
    "def clean_prediction(raw_text):\n",
    "    answer = raw_text.split(\"[/INST]\")[-1].strip()\n",
    "    answer = re.sub(r\"[^\\w\\s\\-.,:/()]\", \"\", answer)\n",
    "    answer = re.sub(r'(\\b.+?:)(\\s*\\1)+', r'\\1', answer)\n",
    "\n",
    "    tokens = answer.split()\n",
    "    for i in range(1, len(tokens) // 2):\n",
    "        if tokens[:i] == tokens[i:2*i]:\n",
    "            answer = \" \".join(tokens[:i])\n",
    "            break\n",
    "\n",
    "    sentence_end = re.search(r'[.?!]', answer)\n",
    "    if sentence_end:\n",
    "        answer = answer[:sentence_end.end()]\n",
    "    return answer.strip()\n",
    "\n",
    "# Load Fine-Tuned LLaMA-2 + Pipeline\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output5/final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def answer_with_fusion_cross_rag_truncated(question, top_k=6, max_windows=5, verbose=False):\n",
    "    # Step 1: Retrieve + rerank with cross-encoder\n",
    "    initial_chunks = retrieve_with_rerank(question, top_k=top_k)\n",
    "\n",
    "    # Step 2: Dynamic truncation filtering\n",
    "    final_chunks = truncate_and_filter_chunks(initial_chunks, question, max_windows=max_windows)\n",
    "\n",
    "    # Step 3: Build fusion prompt\n",
    "    prompt = build_fusion_prompt(final_chunks, question)\n",
    "\n",
    "    # Step 4: Run model\n",
    "    output = qa_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=160,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    answer = clean_prediction(output)\n",
    "\n",
    "    # Step 5: Fuzzy validation\n",
    "    all_context = \" \".join([c[\"content\"] for c in final_chunks])\n",
    "    if not any(is_similar(answer.lower(), c[\"content\"].lower()) for c in final_chunks):\n",
    "        print(\"🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"📌 Prompt (truncated):\\n\", prompt[:500], \"...\\n\")\n",
    "        print(\"🧾 Raw Output:\\n\", output)\n",
    "        print(\"✅ Final Answer:\\n\", answer)\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            print(f\"\\n--- Context {i+1} ---\\n{chunk['content'][:300]}...\\n\")\n",
    "\n",
    "    return answer, final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e660c92b-6249-4e63-b3b7-7f0de3fa7a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc1ca2b-949d-4a49-9c2b-943e73d6c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  1%|▍                                          | 1/100 [00:07<12:25,  7.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                          | 2/100 [00:08<05:42,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                         | 3/100 [00:12<06:25,  3.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                         | 4/100 [00:19<08:20,  5.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                        | 5/100 [00:21<06:17,  3.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▌                                        | 6/100 [00:28<07:47,  4.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███                                        | 7/100 [00:29<05:31,  3.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                       | 8/100 [00:31<04:44,  3.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▊                                       | 9/100 [00:32<03:56,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▏                                     | 10/100 [00:39<05:47,  3.86s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▌                                     | 11/100 [00:46<07:08,  4.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████                                     | 12/100 [00:53<08:02,  5.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▍                                    | 13/100 [00:56<06:56,  4.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▉                                    | 14/100 [00:57<05:11,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▎                                   | 15/100 [00:58<04:09,  2.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▋                                   | 16/100 [01:06<05:53,  4.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▏                                  | 17/100 [01:12<06:54,  5.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▌                                  | 18/100 [01:15<05:51,  4.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▉                                  | 19/100 [01:22<06:58,  5.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▍                                 | 20/100 [01:23<05:15,  3.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▊                                 | 21/100 [01:29<05:57,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████▏                                | 22/100 [01:30<04:29,  3.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▋                                | 23/100 [01:31<03:35,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████                                | 24/100 [01:38<05:05,  4.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▌                               | 25/100 [01:45<06:10,  4.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▉                               | 26/100 [01:46<04:32,  3.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▎                              | 27/100 [01:53<05:40,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▊                              | 28/100 [02:00<06:30,  5.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▏                             | 29/100 [02:07<06:57,  5.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▌                             | 30/100 [02:08<05:05,  4.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████                             | 31/100 [02:15<06:00,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████▍                            | 32/100 [02:22<06:33,  5.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▊                            | 33/100 [02:29<06:50,  6.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████▎                           | 34/100 [02:31<05:22,  4.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▋                           | 35/100 [02:38<05:55,  5.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████                           | 36/100 [02:39<04:19,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████▌                          | 37/100 [02:46<05:09,  4.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▉                          | 38/100 [02:53<05:39,  5.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 39%|████████████████▍                         | 39/100 [02:58<05:37,  5.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▊                         | 40/100 [03:06<06:03,  6.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████▏                        | 41/100 [03:13<06:15,  6.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▋                        | 42/100 [03:20<06:23,  6.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████                        | 43/100 [03:27<06:25,  6.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████▍                       | 44/100 [03:28<04:36,  4.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████▉                       | 45/100 [03:35<05:07,  5.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████▎                      | 46/100 [03:42<05:22,  5.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████▋                      | 47/100 [03:48<05:27,  6.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████████████▏                     | 48/100 [03:49<03:55,  4.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████▌                     | 49/100 [03:54<03:54,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████                     | 50/100 [04:01<04:25,  5.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████▍                    | 51/100 [04:08<04:44,  5.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████▊                    | 52/100 [04:12<04:15,  5.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████▎                   | 53/100 [04:19<04:28,  5.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████▋                   | 54/100 [04:26<04:43,  6.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████                   | 55/100 [04:33<04:45,  6.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████▌                  | 56/100 [04:40<04:50,  6.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████▉                  | 57/100 [04:47<04:51,  6.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████▎                 | 58/100 [04:54<04:45,  6.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████▊                 | 59/100 [05:01<04:39,  6.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▏                | 60/100 [05:08<04:34,  6.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▌                | 61/100 [05:12<03:59,  6.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████                | 62/100 [05:16<03:34,  5.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████▍               | 63/100 [05:23<03:43,  6.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████▉               | 64/100 [05:30<03:45,  6.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████▎              | 65/100 [05:34<03:13,  5.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████▋              | 66/100 [05:36<02:33,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████▏             | 67/100 [05:43<02:52,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████▌             | 68/100 [05:50<03:03,  5.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████▉             | 69/100 [05:54<02:45,  5.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████▍            | 70/100 [05:55<02:00,  4.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████▊            | 71/100 [05:56<01:29,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████▏           | 72/100 [05:58<01:14,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████████▋           | 73/100 [06:05<01:44,  3.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████           | 74/100 [06:12<02:05,  4.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████▌          | 75/100 [06:12<01:29,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████▉          | 76/100 [06:19<01:48,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████▎         | 77/100 [06:26<02:00,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████▊         | 78/100 [06:32<02:03,  5.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▏        | 79/100 [06:40<02:09,  6.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████▌        | 80/100 [06:41<01:35,  4.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████        | 81/100 [06:49<01:44,  5.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████▍       | 82/100 [06:55<01:45,  5.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████▊       | 83/100 [07:01<01:38,  5.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████▎      | 84/100 [07:03<01:16,  4.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████▋      | 85/100 [07:10<01:21,  5.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 86%|████████████████████████████████████      | 86/100 [07:17<01:21,  5.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████▌     | 87/100 [07:24<01:21,  6.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████▉     | 88/100 [07:31<01:18,  6.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████▍    | 89/100 [07:39<01:13,  6.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████▊    | 90/100 [07:42<00:57,  5.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████▏   | 91/100 [07:49<00:55,  6.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████▋   | 92/100 [07:56<00:51,  6.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████   | 93/100 [07:58<00:34,  4.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████▍  | 94/100 [08:05<00:32,  5.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████▉  | 95/100 [08:11<00:29,  5.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████▎ | 96/100 [08:14<00:19,  4.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████▋ | 97/100 [08:21<00:16,  5.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [08:22<00:08,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████▌| 99/100 [08:29<00:05,  5.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [08:33<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\n",
      "\n",
      "📊 Final Evaluation Results (Setup 8 — Truncation + Validation + Cross-Encoder + Fusion):\n",
      "Exact Match (EM): 2.00\n",
      "F1 Score        : 21.19\n",
      "ROUGE-L         : 0.2249\n",
      "BLEU            : 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "\n",
    "# Load QA pairs\n",
    "def load_qa_pairs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "qa_pairs = load_qa_pairs(\"3gpp_qa_100_pairs.jsonl\")\n",
    "\n",
    "# Load metrics\n",
    "squad_metric = load(\"squad\")\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "bleu_predictions = []\n",
    "bleu_references = []\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(qa_pairs):\n",
    "    question = sample[\"question\"]\n",
    "    reference = sample[\"answer\"]\n",
    "\n",
    "    try:\n",
    "        prediction, _ = answer_with_fusion_cross_rag_truncated(question)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error on: {question}\\n{e}\")\n",
    "        prediction = \"\"\n",
    "\n",
    "    # Add to metrics\n",
    "    squad_metric.add(\n",
    "        prediction={\"id\": str(hash(question)), \"prediction_text\": prediction},\n",
    "        reference={\"id\": str(hash(question)), \"answers\": {\"text\": [reference], \"answer_start\": [0]}}\n",
    "    )\n",
    "    rouge.add(prediction=prediction, reference=reference)\n",
    "    bleu_predictions.append(prediction)\n",
    "    bleu_references.append([reference])\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "\n",
    "# Compute final scores\n",
    "squad_scores = squad_metric.compute()\n",
    "rouge_scores = rouge.compute()\n",
    "bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)[\"bleu\"]\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 Final Evaluation Results (Setup 8 — Truncation + Validation + Cross-Encoder + Fusion):\")\n",
    "print(f\"Exact Match (EM): {squad_scores['exact_match']:.2f}\")\n",
    "print(f\"F1 Score        : {squad_scores['f1']:.2f}\")\n",
    "print(f\"ROUGE-L         : {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"BLEU            : {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
