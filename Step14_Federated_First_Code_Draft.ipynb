{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bf1ca-1b78-49fd-a4bd-0116e1ae3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base = \"/mnt/data/federated_qa_jupyter\"\n",
    "folders = [\n",
    "    \"data/client1\", \"data/client2\", \"data/test\", \"data/central_rag_index\",\n",
    "    \"model/base_model\", \"model/client1\", \"model/client2\", \"model/federated_merged_model\",\n",
    "    \"checkpoints/client1\", \"checkpoints/client2\", \"results\", \"utils\"\n",
    "]\n",
    "\n",
    "for f in folders:\n",
    "    os.makedirs(os.path.join(base, f), exist_ok=True)\n",
    "\n",
    "print(\"✅ Folder structure created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e05371-b8ea-47c3-8615-9b166c37d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  answer_utils.py\n",
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Load FAISS index and chunk metadata\n",
    "index = faiss.read_index(\"/mnt/data/RAG/3gpp_index.faiss\")\n",
    "with open(\"/mnt/data/RAG/3gpp_chunks.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def is_similar(a, b, threshold=0.75):\n",
    "    return SequenceMatcher(None, a, b).ratio() >= threshold\n",
    "\n",
    "def truncate_and_filter_chunks(chunks, query, window_size=150, stride=75, max_windows=5):\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def normalize(text):\n",
    "        return re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    def lexical_overlap(query, span):\n",
    "        q_tokens = set(normalize(query).split()) - STOPWORDS\n",
    "        c_tokens = set(normalize(span).split()) - STOPWORDS\n",
    "        return len(q_tokens & c_tokens) / (len(q_tokens | c_tokens) + 1e-5)\n",
    "\n",
    "    def tfidf_score(query, span):\n",
    "        vec = TfidfVectorizer().fit([query, span])\n",
    "        X = vec.transform([query, span])\n",
    "        return (X[0] @ X[1].T).A[0][0]\n",
    "\n",
    "    scored_spans = []\n",
    "    for chunk in chunks:\n",
    "        words = chunk[\"content\"].split()\n",
    "        for i in range(0, len(words), stride):\n",
    "            span_words = words[i:i + window_size]\n",
    "            if len(span_words) < 30:\n",
    "                continue\n",
    "            span = \" \".join(span_words)\n",
    "            score = 0.6 * lexical_overlap(query, span) + 0.4 * tfidf_score(query, span)\n",
    "            scored_spans.append({\n",
    "                \"content\": span,\n",
    "                \"score\": score,\n",
    "                \"source\": chunk.get(\"source\", \"unknown\")\n",
    "            })\n",
    "\n",
    "    return sorted(scored_spans, key=lambda x: x[\"score\"], reverse=True)[:max_windows]\n",
    "\n",
    "def retrieve_with_rerank(query, top_k=5):\n",
    "    query_vec = embedding_model.encode(query, normalize_embeddings=True)\n",
    "    query_vec = np.array(query_vec).reshape(1, -1).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, top_k * 2)\n",
    "    initial_results = [documents[i] for i in I[0]]\n",
    "    pairs = [(query, doc[\"content\"]) for doc in initial_results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(scores, initial_results), key=lambda x: x[0], reverse=True)[:top_k]\n",
    "    return [doc for _, doc in reranked]\n",
    "\n",
    "def build_fusion_prompt(context_chunks, question):\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "        \"Do not paraphrase, summarize, or add extra information. \"\n",
    "        \"The answer must appear exactly in the context. \"\n",
    "        \"If the context lists multiple conditions, actions, or branches, include them all as written. \"\n",
    "        \"Do not summarize or paraphrase — copy the exact text from the context, line by line.\"\n",
    "    )\n",
    "    context_lines = []\n",
    "    for chunk in context_chunks:\n",
    "        source = chunk.get(\"source\", \"unknown\").split(\"/\")[-1]\n",
    "        context_lines.append(f\"[Source: {source}]\\n-----\\n{chunk['content'].strip()}\")\n",
    "    fused_context = \"\\n\\n\".join(context_lines)\n",
    "    user_prompt = (\n",
    "        f\"Context:\\n{fused_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer from the context only:\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "def clean_prediction(raw_text):\n",
    "    answer = raw_text.split(\"[/INST]\")[-1].strip()\n",
    "    answer = re.sub(r\"[^\\w\\s\\-.,:/()]\", \"\", answer)\n",
    "    answer = re.sub(r'(\\b.+?:)(\\s*\\1)+', r'\\1', answer)\n",
    "    tokens = answer.split()\n",
    "    for i in range(1, len(tokens) // 2):\n",
    "        if tokens[:i] == tokens[i:2*i]:\n",
    "            answer = \" \".join(tokens[:i])\n",
    "            break\n",
    "    sentence_end = re.search(r'[.?!]', answer)\n",
    "    if sentence_end:\n",
    "        answer = answer[:sentence_end.end()]\n",
    "    return answer.strip()\n",
    "\n",
    "def answer_with_fusion_cross_rag_truncated(question, top_k=6, max_windows=5, verbose=False):\n",
    "    initial_chunks = retrieve_with_rerank(question, top_k=top_k)\n",
    "    final_chunks = truncate_and_filter_chunks(initial_chunks, question, max_windows=max_windows)\n",
    "    prompt = build_fusion_prompt(final_chunks, question)\n",
    "\n",
    "    model_path = \"./model/federated_merged_model\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    output = qa_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=160,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    answer = clean_prediction(output)\n",
    "\n",
    "    all_context = \" \".join([c[\"content\"] for c in final_chunks])\n",
    "    if not any(is_similar(answer.lower(), c[\"content\"].lower()) for c in final_chunks):\n",
    "        print(\"🚨 WARNING: Approximate match for answer not found in final context. Review answer relevance.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"📌 Prompt (truncated):\\n\", prompt[:500], \"...\\n\")\n",
    "        print(\"🧾 Raw Output:\\n\", output)\n",
    "        print(\"✅ Final Answer:\\n\", answer)\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            print(f\"\\n--- Context {i+1} ---\\n{chunk['content'][:300]}...\\n\")\n",
    "\n",
    "    return answer, final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10b2a3-0eb0-4e95-8695-2bc6ac22de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "def train_model(dataset_path, base_model_path, output_path, epochs=3):\n",
    "    # Load data\n",
    "    data = load_jsonl(dataset_path)\n",
    "    dataset = Dataset.from_list(data).shuffle(seed=42)\n",
    "    split = dataset.train_test_split(test_size=0.10, seed=42)\n",
    "    val_test = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    train_dataset = split[\"train\"]\n",
    "    val_dataset = val_test[\"train\"]\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data/llama2-model\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        )\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True, num_proc=2, remove_columns=[\"text\"])\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True, num_proc=2, remove_columns=[\"text\"])\n",
    "\n",
    "    # Data Collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=64\n",
    "    )\n",
    "\n",
    "    # Load Model with LoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "    base_model.config.use_cache = False\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    # Training Args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_dir=f\"{output_path}/logs\",\n",
    "        logging_steps=25,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers=2,\n",
    "        group_by_length=True,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        max_grad_norm=1,\n",
    "        warmup_ratio=0.03\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"{output_path}/final\")\n",
    "    tokenizer.save_pretrained(f\"{output_path}/final\")\n",
    "\n",
    "    return trainer, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d66bf-a62a-4e7f-b42c-e1c067377bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "MAX_TOKEN_LENGTH = 2048\n",
    "SIM_THRESHOLD = 0.6\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "    \"Do not paraphrase, summarize, or add extra information. \"\n",
    "    \"The answer must appear exactly in the context.\"\n",
    ")\n",
    "\n",
    "def select_relevant_chunks(context: str, answer: str, window_size=150, stride=100):\n",
    "    words = context.split()\n",
    "    for start in range(0, len(words), stride):\n",
    "        end = start + window_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        if answer in chunk:\n",
    "            return chunk\n",
    "        if end >= len(words):\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def build_prompt(context: str, question: str, answer: str) -> str:\n",
    "    user_prompt = (\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n{user_prompt} [/INST] {answer}</s>\"\n",
    "\n",
    "def extract_context_and_answer(text):\n",
    "    try:\n",
    "        prompt_part, answer = text.split(\"[/INST]\", 1)\n",
    "        answer = answer.strip().replace(\"</s>\", \"\")\n",
    "        lines = prompt_part.splitlines()\n",
    "\n",
    "        context_lines = []\n",
    "        inside_context = False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"Context:\"):\n",
    "                inside_context = True\n",
    "                context_lines.append(line.replace(\"Context:\", \"\").strip())\n",
    "                continue\n",
    "            if line.strip().startswith(\"Question:\"):\n",
    "                break\n",
    "            if inside_context:\n",
    "                context_lines.append(line.strip())\n",
    "\n",
    "        context = \" \".join(context_lines)\n",
    "        return context, answer\n",
    "    except:\n",
    "        return \"\", \"\"\n",
    "\n",
    "def clean_answer(entry):\n",
    "    try:\n",
    "        text = entry.get(\"text\", \"\")\n",
    "        prompt_part, answer_part = text.split(\"[/INST]\", 1)\n",
    "        clean_answer = answer_part.strip().replace(\"</s>\", \"\")\n",
    "        return f\"{prompt_part}[/INST] {clean_answer}</s>\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def preprocess_client_dataset(input_path: Path, output_path: Path, model_path: str):\n",
    "    print(f\"🔄 Preprocessing: {input_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    reformatted_entries = []\n",
    "    total_count = 0\n",
    "    filtered_out_count = 0\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines, desc=\"Reformatting\"):\n",
    "            total_count += 1\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                original_text = entry[\"text\"]\n",
    "\n",
    "                prompt_part, answer = original_text.split(\"[/INST]\", 1)\n",
    "                answer = answer.strip().replace(\"</s>\", \"\")\n",
    "                if not answer:\n",
    "                    continue\n",
    "\n",
    "                lines = prompt_part.splitlines()\n",
    "                context_lines, question = [], \"\"\n",
    "                inside_context, inside_question = False, False\n",
    "\n",
    "                for l in lines:\n",
    "                    stripped = l.strip()\n",
    "                    if stripped.startswith(\"Context:\"):\n",
    "                        inside_context = True\n",
    "                        inside_question = False\n",
    "                        context_lines.append(stripped.replace(\"Context:\", \"\").strip())\n",
    "                        continue\n",
    "                    elif stripped.startswith(\"Question:\"):\n",
    "                        inside_question = True\n",
    "                        inside_context = False\n",
    "                        question = stripped.replace(\"Question:\", \"\").strip()\n",
    "                        continue\n",
    "                    if inside_context:\n",
    "                        context_lines.append(l)\n",
    "                    elif inside_question and not question:\n",
    "                        question = stripped\n",
    "\n",
    "                full_context = \"\\n\".join(context_lines).strip()\n",
    "                if not full_context or not question:\n",
    "                    continue\n",
    "\n",
    "                temp_prompt = build_prompt(full_context, question, answer)\n",
    "                input_ids = tokenizer(temp_prompt)[\"input_ids\"]\n",
    "\n",
    "                final_context = full_context\n",
    "                if len(input_ids) > MAX_TOKEN_LENGTH:\n",
    "                    short_context = select_relevant_chunks(full_context, answer)\n",
    "                    if short_context and answer in short_context:\n",
    "                        final_context = short_context\n",
    "                    else:\n",
    "                        filtered_out_count += 1\n",
    "                        continue\n",
    "\n",
    "                final_prompt = build_prompt(final_context, question, answer)\n",
    "                context, ans = extract_context_and_answer(final_prompt)\n",
    "\n",
    "                if not context or not ans:\n",
    "                    continue\n",
    "\n",
    "                context_emb = encoder.encode(context, convert_to_tensor=True)\n",
    "                answer_emb = encoder.encode(ans, convert_to_tensor=True)\n",
    "                similarity = util.cos_sim(context_emb, answer_emb).item()\n",
    "\n",
    "                if similarity >= SIM_THRESHOLD:\n",
    "                    cleaned = clean_answer({\"text\": final_prompt})\n",
    "                    if cleaned:\n",
    "                        reformatted_entries.append({\"text\": cleaned})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping malformed line {total_count}: {e}\")\n",
    "                continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for e in reformatted_entries:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Saved cleaned client data to {output_path}\")\n",
    "    print(f\"📊 Total processed: {total_count}, Kept: {len(reformatted_entries)}, Filtered: {filtered_out_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc45b6-f843-467b-9622-2c4df85466a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "import flwr as fl\n",
    "import torch\n",
    "from utils.trainer import get_trainer\n",
    "import os\n",
    "\n",
    "\n",
    "class LoraClient(fl.client.NumPyClient):\n",
    "    def __init__(self, client_id, dataset_path, model_path, output_dir):\n",
    "        self.client_id = client_id\n",
    "        self.dataset_path = dataset_path\n",
    "        self.model_path = model_path\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        self.trainer, self.model, self.tokenizer = get_trainer(\n",
    "            client_id, dataset_path, model_path, output_dir\n",
    "        )\n",
    "\n",
    "    def get_parameters(self, config=None):\n",
    "        return [val.cpu().numpy() for _, val in self.model.named_parameters() if val.requires_grad]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = dict(self.model.named_parameters())\n",
    "        for (name, param), new_val in zip(params_dict.items(), parameters):\n",
    "            if param.requires_grad:\n",
    "                param.data = torch.tensor(new_val).to(param.device)\n",
    "\n",
    "    def fit(self, parameters, config=None):\n",
    "        print(f\"🚀 [{self.client_id}] Starting training round...\")\n",
    "        self.set_parameters(parameters)\n",
    "        self.trainer.train()\n",
    "        return self.get_parameters(), len(self.trainer.train_dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config=None):\n",
    "        return 0.0, 0, {}  # Optional: skip evaluation during simulation\n",
    "\n",
    "\n",
    "def get_client_fn(client_id):\n",
    "    def client_fn(cid):\n",
    "        print(f\"🚀 Launching client {client_id} ...\")\n",
    "\n",
    "        dataset_path = f\"/mnt/data/federated_qa_jupyter/data/client{client_id}/client{client_id}_qa.jsonl\"\n",
    "        \n",
    "        # ✅ LoRA-finetuned model (read-only)\n",
    "        model_path = \"/mnt/data/llama2_qa_lora_output5\"\n",
    "\n",
    "        # ✅ Save new adapters and logs here\n",
    "        output_dir = f\"/mnt/data/federated_qa_jupyter/checkpoints/client{client_id}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        return LoraClient(\n",
    "            client_id=f\"client{client_id}\",\n",
    "            dataset_path=dataset_path,\n",
    "            model_path=model_path,\n",
    "        )\n",
    "    return client_fn\n",
    "\n",
    "get_client_fn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d8954-e23a-4185-9c2c-a7e0f83c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from data_utils import preprocess_client_dataset\n",
    "\n",
    "base_model_path = \"/mnt/data/llama2-model\"  \n",
    "input_client1 = Path(\"/mnt/data/client_raw/client1_raw.jsonl\")\n",
    "input_client2 = Path(\"/mnt/data/client_raw/client2_raw.jsonl\")\n",
    "\n",
    "output_client1 = Path(\"/mnt/data/federated_qa_jupyter/data/client1/client1_qa.jsonl\")\n",
    "output_client2 = Path(\"/mnt/data/federated_qa_jupyter/data/client2/client2_qa.jsonl\")\n",
    "\n",
    "# Format datasets for training\n",
    "preprocess_client_dataset(input_client1, output_client1, base_model_path)\n",
    "preprocess_client_dataset(input_client2, output_client2, base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c5a40-99f9-4b31-a50b-99dfd764ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "\n",
    "NUM_CLIENTS = 2\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    on_fit_config_fn=lambda rnd: {\"round\": rnd}\n",
    ")\n",
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=get_client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=strategy,\n",
    "    client_resources={\"num_cpus\": 2, \"num_gpus\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053d996-d93c-4205-a963-16e8e7349385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_eval.py\n",
    "from pathlib import Path\n",
    "import json, re, time, math\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Reuse your retrieval + prompt utilities (no model inside)\n",
    "from answer_utils import (\n",
    "    retrieve_with_rerank,\n",
    "    truncate_and_filter_chunks,\n",
    "    build_fusion_prompt,\n",
    "    clean_prediction,\n",
    ")\n",
    "\n",
    "# Models\n",
    "PRE_FL_MODEL_PATH = Path(\"/mnt/data/llama2_qa_lora_output5/final\")  #  pre-FL fine-tuned model dir\n",
    "POST_FL_MODEL_PATH = Path(\"/mnt/data/federated_qa_jupyter/model/federated_merged_model\")  # merged FL model\n",
    "\n",
    "# Datasets (expected format: {\"question\": \"...\", \"answer\": \"...\"} per line)\n",
    "RAG_TEST_PATH = Path(\"/mnt/data/federated_qa_jupyter/data/test/federated_test_set.jsonl\")\n",
    "CLIENT_A_HOLDOUT = Path(\"/mnt/data/federated_qa_jupyter/data/test/clientA_holdout.jsonl\")\n",
    "CLIENT_B_HOLDOUT = Path(\"/mnt/data/federated_qa_jupyter/data/test/clientB_holdout.jsonl\")\n",
    "\n",
    "# Outputs\n",
    "RESULTS_DIR = Path(\"/mnt/data/federated_qa_jupyter/results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUMMARY_CSV = RESULTS_DIR / \"federated_eval_summary.csv\"\n",
    "DETAILS_JSONL = RESULTS_DIR / \"federated_eval_details.jsonl\"\n",
    "\n",
    "# Retrieval windows used by your fusion pipeline\n",
    "TOP_K = 6\n",
    "MAX_WINDOWS = 5\n",
    "\n",
    "\n",
    "# METRICS\n",
    "def _normalize(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\-.,:/() ]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def exact_match(pred, ref):\n",
    "    return 1.0 if _normalize(pred) == _normalize(ref) else 0.0\n",
    "\n",
    "def f1_score(pred, ref):\n",
    "    p_tok = _normalize(pred).split()\n",
    "    r_tok = _normalize(ref).split()\n",
    "    if not p_tok and not r_tok:\n",
    "        return 1.0\n",
    "    if not p_tok or not r_tok:\n",
    "        return 0.0\n",
    "    common = 0\n",
    "    r_counts = {}\n",
    "    for t in r_tok: r_counts[t] = r_counts.get(t, 0) + 1\n",
    "    for t in p_tok:\n",
    "        if r_counts.get(t, 0) > 0:\n",
    "            common += 1\n",
    "            r_counts[t] -= 1\n",
    "    if common == 0:\n",
    "        return 0.0\n",
    "    precision = common / len(p_tok)\n",
    "    recall = common / len(r_tok)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def lcs_length(a, b):\n",
    "    a, b = _normalize(a), _normalize(b)\n",
    "    A, B = a.split(), b.split()\n",
    "    dp = [[0]*(len(B)+1) for _ in range(len(A)+1)]\n",
    "    for i in range(1, len(A)+1):\n",
    "        for j in range(1, len(B)+1):\n",
    "            if A[i-1] == B[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[-1][-1], len(A), len(B)\n",
    "\n",
    "def rouge_l(pred, ref):\n",
    "    lcs, m, n = lcs_length(pred, ref)\n",
    "    if m == 0 or n == 0 or lcs == 0:\n",
    "        return 0.0\n",
    "    prec = lcs / m\n",
    "    rec = lcs / n\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    beta2 = 1.2**2\n",
    "    return (1 + beta2) * prec * rec / (rec + beta2 * prec)\n",
    "\n",
    "def bleu1(pred, ref):\n",
    "    # simple BLEU-1 with brevity penalty\n",
    "    p_tokens = _normalize(pred).split()\n",
    "    r_tokens = _normalize(ref).split()\n",
    "    if not p_tokens or not r_tokens:\n",
    "        return 0.0\n",
    "    ref_counts = {}\n",
    "    for t in r_tokens: ref_counts[t] = ref_counts.get(t, 0) + 1\n",
    "    match = 0\n",
    "    used = {}\n",
    "    for t in p_tokens:\n",
    "        c = used.get(t, 0)\n",
    "        if c < ref_counts.get(t, 0):\n",
    "            match += 1\n",
    "            used[t] = c + 1\n",
    "    precision = match / len(p_tokens)\n",
    "    # brevity penalty\n",
    "    bp = 1.0 if len(p_tokens) > len(r_tokens) else math.exp(1 - len(r_tokens)/max(1, len(p_tokens)))\n",
    "    return bp * precision\n",
    "\n",
    "def compute_metrics(pred, ref):\n",
    "    return {\n",
    "        \"EM\": exact_match(pred, ref),\n",
    "        \"F1\": f1_score(pred, ref),\n",
    "        \"ROUGE_L\": rouge_l(pred, ref),\n",
    "        \"BLEU1\": bleu1(pred, ref),\n",
    "    }\n",
    "\n",
    "\n",
    "# DATA HELPERS\n",
    "def load_jsonl(path: Path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "# INFERENCE HELPERS\n",
    "def load_pipeline(model_path: Path):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path)\n",
    "    # decoder-only best practice\n",
    "    tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "def answer_with_rag(qa_pipe, question, top_k=TOP_K, max_windows=MAX_WINDOWS):\n",
    "    # Retrieve & fuse context with your existing utilities\n",
    "    initial_chunks = retrieve_with_rerank(question, top_k=top_k)\n",
    "    final_chunks = truncate_and_filter_chunks(initial_chunks, question, max_windows=max_windows)\n",
    "    prompt = build_fusion_prompt(final_chunks, question)\n",
    "\n",
    "    out = qa_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=160,\n",
    "        do_sample=False,\n",
    "        eos_token_id=qa_pipe.tokenizer.eos_token_id,\n",
    "        pad_token_id=qa_pipe.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.05,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    pred = clean_prediction(out)\n",
    "    return pred, final_chunks\n",
    "\n",
    "def answer_without_rag(qa_pipe, question):\n",
    "    # Short, model-knowledge-only prompt\n",
    "    sys = (\n",
    "        \"You are a precise telecom expert. Answer concisely and factually. \"\n",
    "        \"If unsure, say 'I don't know'.\"\n",
    "    )\n",
    "    user = f\"Question: {question}\\nAnswer:\"\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{user} [/INST]\"\n",
    "    out = qa_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=160,\n",
    "        do_sample=False,\n",
    "        eos_token_id=qa_pipe.tokenizer.eos_token_id,\n",
    "        pad_token_id=qa_pipe.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.05,\n",
    "    )[0][\"generated_text\"]\n",
    "    # Extract everything after [/INST]\n",
    "    pred = out.split(\"[/INST]\")[-1].strip()\n",
    "    # Trim at first sentence end to reduce rambling\n",
    "    m = re.search(r\"[.?!]\", pred)\n",
    "    if m:\n",
    "        pred = pred[: m.end()]\n",
    "    return pred\n",
    "\n",
    "\n",
    "# RUN EVAL\n",
    "def eval_one_dataset(qa_pipe, dataset, rag_mode, model_tag, dataset_name, details_fp):\n",
    "    # rag_mode ∈ {\"with_rag\",\"no_rag\"}\n",
    "    agg = {\"EM\": [], \"F1\": [], \"ROUGE_L\": [], \"BLEU1\": []}\n",
    "    for ex in dataset:\n",
    "        q = ex[\"question\"].strip()\n",
    "        ref = ex[\"answer\"].strip()\n",
    "        try:\n",
    "            if rag_mode == \"with_rag\":\n",
    "                pred, ctx = answer_with_rag(qa_pipe, q)\n",
    "            else:\n",
    "                pred = answer_without_rag(qa_pipe, q)\n",
    "                ctx = None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Inference error for Q: {q[:80]}... -> {e}\")\n",
    "            pred, ctx = \"\", None\n",
    "\n",
    "        m = compute_metrics(pred, ref)\n",
    "        for k,v in m.items(): agg[k].append(v)\n",
    "\n",
    "        # write details row\n",
    "        record = {\n",
    "            \"model\": model_tag,\n",
    "            \"rag_mode\": rag_mode,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"question\": q,\n",
    "            \"reference\": ref,\n",
    "            \"prediction\": pred,\n",
    "            \"metrics\": m,\n",
    "        }\n",
    "        if ctx:\n",
    "            record[\"contexts\"] = [{\"source\": c.get(\"source\",\"\"), \"snippet\": c[\"content\"][:300]} for c in ctx]\n",
    "        details_fp.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    # summarize\n",
    "    return {\n",
    "        \"model\": model_tag,\n",
    "        \"rag_mode\": rag_mode,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"EM\": np.mean(agg[\"EM\"]) if agg[\"EM\"] else 0.0,\n",
    "        \"F1\": np.mean(agg[\"F1\"]) if agg[\"F1\"] else 0.0,\n",
    "        \"ROUGE_L\": np.mean(agg[\"ROUGE_L\"]) if agg[\"ROUGE_L\"] else 0.0,\n",
    "        \"BLEU1\": np.mean(agg[\"BLEU1\"]) if agg[\"BLEU1\"] else 0.0,\n",
    "        \"N\": len(dataset),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    datasets = []\n",
    "    if RAG_TEST_PATH.exists():\n",
    "        datasets.append((\"R15_16_RAG100\", load_jsonl(RAG_TEST_PATH)))\n",
    "    if CLIENT_A_HOLDOUT.exists():\n",
    "        datasets.append((\"ClientA_holdout\", load_jsonl(CLIENT_A_HOLDOUT)))\n",
    "    if CLIENT_B_HOLDOUT.exists():\n",
    "        datasets.append((\"ClientB_holdout\", load_jsonl(CLIENT_B_HOLDOUT)))\n",
    "\n",
    "    if not datasets:\n",
    "        raise FileNotFoundError(\"No datasets found. Check the dataset paths.\")\n",
    "\n",
    "    # Load both model pipelines\n",
    "    print(\"🔹 Loading PRE-FL pipeline...\")\n",
    "    pre_pipe = load_pipeline(PRE_FL_MODEL_PATH)\n",
    "\n",
    "    print(\"🔹 Loading POST-FL pipeline...\")\n",
    "    post_pipe = load_pipeline(POST_FL_MODEL_PATH)\n",
    "\n",
    "    # Run all combinations\n",
    "    rows = []\n",
    "    with open(DETAILS_JSONL, \"w\", encoding=\"utf-8\") as fp:\n",
    "        for (ds_name, ds_data) in datasets:\n",
    "            for rag_mode, pipe, tag in [\n",
    "                (\"no_rag\", pre_pipe, \"CentralFT\"),\n",
    "                (\"with_rag\", pre_pipe, \"CentralFT\"),\n",
    "                (\"no_rag\", post_pipe, \"FederatedFT\"),\n",
    "                (\"with_rag\", post_pipe, \"FederatedFT\"),\n",
    "            ]:\n",
    "                print(f\"▶️  {tag} | {rag_mode} | {ds_name}  (N={len(ds_data)})\")\n",
    "                t0 = time.time()\n",
    "                summary = eval_one_dataset(pipe, ds_data, rag_mode, tag, ds_name, fp)\n",
    "                summary[\"secs\"] = round(time.time() - t0, 2)\n",
    "                rows.append(summary)\n",
    "\n",
    "    # Write summary CSV\n",
    "    with open(SUMMARY_CSV, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"model,rag_mode,dataset,N,EM,F1,ROUGE_L,BLEU1,secs\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\"{model},{rag_mode},{dataset},{N},{EM:.4f},{F1:.4f},{ROUGE_L:.4f},{BLEU1:.4f},{secs}\\n\".format(**r))\n",
    "\n",
    "    print(f\"✅ Done.\\nSummary → {SUMMARY_CSV}\\nDetails → {DETAILS_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
