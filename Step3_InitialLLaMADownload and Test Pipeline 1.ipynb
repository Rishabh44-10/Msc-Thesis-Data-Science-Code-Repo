{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4023b70-5f0c-4c79-af17-424db71e70f0",
   "metadata": {},
   "source": [
    "# Downloading Llama-2-7b-hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb42e4-0da9-492d-b0e6-1eef6cda25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02912182-ed9f-44cd-8532-3aed0c1a7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-hf\",\n",
    "    local_dir=\"/mnt/data/llama2-model\",\n",
    "    token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d3836-1591-4a0d-a849-5b16ac4ed0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the downloads in EC2\n",
    "ls -lh /mnt/data/llama2-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b335e63-5fdc-49f1-b356-f7cb477be5e3",
   "metadata": {},
   "source": [
    "# Test Pipeline 1 for Question Answer Generation Pipeline\n",
    "Notebook Overview\n",
    "\n",
    "This notebook builds a pipeline for generating and filtering question–answer (QA) pairs from technical telecom documents (DOCX format). The main steps are:\n",
    "\n",
    "Model Loading\n",
    "\n",
    "A T5-based Question Generation (QG) model generates candidate questions from text chunks.\n",
    "\n",
    "A fine-tuned RoBERTa QA model (trained on telecom data) extracts answer spans from the same context.\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "Raw text is cleaned to remove boilerplate (tables, figures, metadata, headers, etc.).\n",
    "\n",
    "The text is split into overlapping chunks to preserve context for both QG and QA models.\n",
    "\n",
    "QA Pair Generation\n",
    "\n",
    "For each chunk, the longest sentences are highlighted to guide the QG model.\n",
    "\n",
    "Generated questions are passed to the QA model to extract precise answers.\n",
    "\n",
    "Low-confidence, trivial, or invalid answers are discarded.\n",
    "\n",
    "Valid pairs are saved with metadata (context, question, answer, confidence).\n",
    "\n",
    "Filtering for Quality\n",
    "\n",
    "A post-processing step applies heuristics (confidence threshold, answer-in-context checks, vague question removal).\n",
    "\n",
    "Only “good” QA pairs are retained and saved in JSONL format.\n",
    "\n",
    "Output\n",
    "\n",
    "Two JSONL files are produced:\n",
    "\n",
    "Raw QA pairs (qa_output.jsonl)\n",
    "\n",
    "Filtered high-quality QA pairs (qa_output_filtered_good.jsonl)\n",
    "\n",
    "This ensures that only reliable, domain-grounded QA pairs are preserved for downstream use, such as fine-tuning telecom-specific extractive QA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27378622-ad1c-4ce9-aa6d-3db781d45d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from docx import Document\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c426aa-fcb0-417e-a094-8d87732aa843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "qg_model = pipeline(\"text2text-generation\", model=\"mrm8488/t5-base-finetuned-question-generation-ap\", device=0, batch_size=8)\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load fine-tuned telecom QA model\n",
    "model_path = \"/home/ec2-user/qa_roberta_telecom\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aabf015-1bdf-4c84-b442-d8958c350a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean DOCX text\n",
    "def clean_technical_text(raw_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line or len(line.split()) < 5:\n",
    "            continue\n",
    "        if re.match(r\"^(figure|fig\\.|table)\\s*\\d+\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.match(r\"^\\d+(\\.\\d+){0,4}\\s+[A-Z]\", line):\n",
    "            continue\n",
    "        if any(kw in line.lower() for kw in [\"3gpp\", \"etsi\", \"confidential\", \"page\", \"table of contents\", \"index\", \"appendix\"]):\n",
    "            continue\n",
    "        if line.isupper() and len(line.split()) < 10:\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    return \" \".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e8984d-2ea0-4692-9552-385a4590d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking with overlap\n",
    "def split_with_overlap(text, max_words=150, overlap=30):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_words]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_words - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05ccd96-027b-477a-8335-74cd0ab4e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def select_highlight_sentence(chunk):\n",
    "    # Split by punctuation followed by a space and uppercase letter\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip())\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "    if not sentences:\n",
    "        return None\n",
    "    if len(sentences) == 1:\n",
    "        return sentences[0]\n",
    "\n",
    "    # Fallback scoring: use longest sentence\n",
    "    highlight = max(sentences, key=lambda s: len(s))\n",
    "    return highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04d6d7-2464-4400-86eb-cd3f0b7f32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs_from_text(text: str, source: str):\n",
    "    chunks = split_with_overlap(text, max_words=250, overlap=50)\n",
    "    qa_pairs = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Generating QA pairs\")):\n",
    "        # Skip likely TOC or meta sections\n",
    "        if re.search(r'\\d+\\.\\d+.*?:?', chunk) and len(chunk) < 400:\n",
    "            continue\n",
    "        if sum(c.isdigit() for c in chunk) / max(len(chunk), 1) > 0.3:\n",
    "            continue\n",
    "\n",
    "        # Try top 2 longest sentences as highlight options\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip())\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "        top_sentences = sorted(sentences, key=len, reverse=True)[:2]\n",
    "\n",
    "        for highlight_sent in top_sentences:\n",
    "            q_input = f\"highlight: {chunk.replace(highlight_sent, f'<hl> {highlight_sent} <hl>')}\"\n",
    "\n",
    "            try:\n",
    "                question = qg_model(q_input, max_new_tokens=64)[0]['generated_text']\n",
    "\n",
    "                # Skip vague prompts\n",
    "                if question.lower().startswith(\"what is the highlight\"):\n",
    "                    continue\n",
    "\n",
    "                result = qa_model({'question': question, 'context': chunk})\n",
    "\n",
    "                if result['score'] < 0.2:\n",
    "                    continue\n",
    "                answer = result['answer'].strip()\n",
    "\n",
    "                # Answer must be extractive (in context) and non-trivial\n",
    "                if len(answer) < 3 or answer.lower() not in chunk.lower():\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Chunk {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "            qa_pairs.append({\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{chunk}\\n\\n### Question:\\n{question}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"chunk_id\": i,\n",
    "                \"confidence\": round(result['score'], 3)\n",
    "            })\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c1490-5b7b-4398-9230-5b1773e0879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DOCX file\n",
    "def process_docx_file(input_docx_path, output_jsonl_path):\n",
    "    input_path = Path(input_docx_path)\n",
    "    doc = Document(input_path)\n",
    "    full_text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    cleaned = clean_technical_text(full_text)\n",
    "    qa_pairs = generate_qa_pairs_from_text(cleaned, source=input_path.name)\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\" Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e267475-eb92-436c-beee-af5156dce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA pairs:  15%|███▏                 | 34/228 [00:10<01:11,  2.72it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating QA pairs: 100%|████████████████████| 228/228 [04:09<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Total Paragraphs: 3634\n",
      "✅ QA Pairs: 275\n",
      "🚫 Skipped: 1486 | ❌ Errors: 0\n",
      "✅ Saved 275 QA pairs to /mnt/data/qa_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_docx_file(\"/mnt/data/Datasets/38104-j00.docx\", \"/mnt/data/qa_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132d44c6-6657-4a2b-b796-14b1d45d0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 154 good QA pairs to /mnt/data/qa_output_filtered_good.jsonl\n"
     ]
    }
   ],
   "source": [
    "# STEP: Filter high-quality QA pairs and save to new JSONL file\n",
    "import pandas as pd\n",
    "\n",
    "def filter_good_qa_pairs(input_jsonl_path, output_jsonl_path):\n",
    "    # Load QA pairs\n",
    "    qa_pairs = []\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "    # Extract question and context for filtering\n",
    "    qa_df[\"question\"] = qa_df[\"input\"].str.extract(r\"### Question:\\n(.+?)\\n\\n### Answer:\")\n",
    "    qa_df[\"context\"] = qa_df[\"input\"].str.extract(r\"### Context:\\n(.+?)\\n\\n### Question:\", flags=re.DOTALL)\n",
    "\n",
    "    # Heuristic filters\n",
    "    qa_df[\"low_confidence_flag\"] = qa_df[\"confidence\"].astype(float) < 0.4\n",
    "    qa_df[\"mismatch_flag\"] = ~qa_df.apply(lambda row: row[\"output\"].lower() in row[\"context\"].lower(), axis=1)\n",
    "    qa_df[\"vague_question_flag\"] = qa_df[\"question\"].str.lower().str.startswith(\"what is the highlight\")\n",
    "\n",
    "    # Final label\n",
    "    qa_df[\"quality\"] = \"good\"\n",
    "    qa_df.loc[\n",
    "        qa_df[\"low_confidence_flag\"] | qa_df[\"mismatch_flag\"] | qa_df[\"vague_question_flag\"],\n",
    "        \"quality\"\n",
    "    ] = \"poor\"\n",
    "\n",
    "    # Filter and save\n",
    "    good_qa_df = qa_df[qa_df[\"quality\"] == \"good\"]\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in good_qa_df.to_dict(orient=\"records\"):\n",
    "            json.dump(record, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    print(f\" Saved {len(good_qa_df)} good QA pairs to {output_jsonl_path}\")\n",
    "\n",
    "# Run it on the generated file\n",
    "filter_good_qa_pairs(\"/mnt/data/qa_output.jsonl\", \"/mnt/data/qa_output_filtered_good.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7f93e-f58f-4cd6-9b5e-70fbeb0e3012",
   "metadata": {},
   "source": [
    "# Test Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc560826-74da-468b-87e2-c55fb64f3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs_from_paragraphs(paragraphs: list[str], source: str, batch_size=16):\n",
    "    qa_pairs = []\n",
    "    counters = {\n",
    "        \"total_paragraphs\": len(paragraphs),\n",
    "        \"qa_generated\": 0,\n",
    "        \"qa_fallback\": 0,\n",
    "        \"skipped_filters\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(0, len(paragraphs), batch_size), desc=\"Generating QA pairs\"):\n",
    "        batch_paras = paragraphs[i:i+batch_size]\n",
    "        q_inputs = []\n",
    "        batch_indices = []\n",
    "\n",
    "        for j, para in enumerate(batch_paras):\n",
    "            idx = i + j\n",
    "            if len(para.split()) < 5:\n",
    "                continue\n",
    "            if re.search(r'\\d+\\.\\d+.*?:?', para) and len(para) < 400:\n",
    "                continue\n",
    "            if sum(c.isdigit() for c in para) / max(len(para), 1) > 0.3:\n",
    "                continue\n",
    "\n",
    "            sentences = re.split(r'(?<=[.?!])\\s+(?=[A-Z])', para)\n",
    "            if len(sentences) < 2:\n",
    "                sentences = para.split(\".\")\n",
    "\n",
    "            sentences = [s.strip() for s in sentences if len(s.strip().split()) >= 5]\n",
    "            top_sentences = sorted(sentences, key=len, reverse=True)[:2]\n",
    "\n",
    "            for sent in top_sentences:\n",
    "                highlight = para.replace(sent, f\"<hl> {sent} <hl>\")\n",
    "                q_inputs.append(f\"highlight: {highlight}\")\n",
    "                batch_indices.append((idx, para, sent, sentences))\n",
    "\n",
    "        # Generate questions\n",
    "        if not q_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions = qg_model(q_inputs, max_new_tokens=64)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QG batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(q_inputs)\n",
    "            continue\n",
    "\n",
    "        # Format QA batch inputs\n",
    "        qa_inputs = [\n",
    "            {\"question\": q[\"generated_text\"], \"context\": para}\n",
    "            for (_, para, _, _), q in zip(batch_indices, questions)\n",
    "            if not q[\"generated_text\"].lower().startswith(\"what is the highlight\")\n",
    "        ]\n",
    "\n",
    "        if not qa_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions_list = [qa[\"question\"] for qa in qa_inputs]\n",
    "            contexts_list = [qa[\"context\"] for qa in qa_inputs]\n",
    "\n",
    "            answers = qa_model(question=questions_list, context=contexts_list)\n",
    "\n",
    "            if isinstance(answers, dict):\n",
    "                answers = [answers]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QA batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(qa_inputs)\n",
    "            continue\n",
    "\n",
    "        for (idx, para, sent, sentences), question, result in zip(batch_indices, questions, answers):\n",
    "            question_text = question[\"generated_text\"]\n",
    "            if question_text.lower().startswith(\"what is the highlight\"):\n",
    "                continue\n",
    "\n",
    "            answer = result[\"answer\"].strip()\n",
    "            score = result[\"score\"]\n",
    "\n",
    "            # Apply filters\n",
    "            if score < 0.15 or len(answer) < 4 or not any(c.isalnum() for c in answer):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in [\"yes\", \"no\", \"maybe\"]:\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if not any(answer.lower() in s.lower() for s in sentences):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "\n",
    "            qa = {\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{para}\\n\\n### Question:\\n{question_text}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"chunk_id\": idx,\n",
    "                \"confidence\": round(score, 3)\n",
    "            }\n",
    "            qa_pairs.append(qa)\n",
    "            counters[\"qa_generated\"] += 1\n",
    "\n",
    "    print(f\" Total Paragraphs: {counters['total_paragraphs']}\")\n",
    "    print(f\" QA Pairs: {counters['qa_generated']}\")\n",
    "    print(f\" Skipped: {counters['skipped_filters']} |  Errors: {counters['errors']}\")\n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad60fabb-7768-4d5b-8f9c-25aa3f7635bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX Processing Wrapper\n",
    "def process_docx_file(input_docx_path, output_jsonl_path):\n",
    "    input_path = Path(input_docx_path)\n",
    "    doc = Document(input_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    qa_pairs = generate_qa_pairs_from_paragraphs(paragraphs, source=input_path.name)\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\" Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
