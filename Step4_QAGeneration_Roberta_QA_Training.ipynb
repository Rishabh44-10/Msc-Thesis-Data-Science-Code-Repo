{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c215ed98-cac2-4a83-b531-cac7da256376",
   "metadata": {},
   "source": [
    "# Notebook Summary\n",
    "\n",
    "This notebook builds a unified telecom QA dataset by merging multiple sources and fine-tunes a RoBERTa question answering model (pretrained on SQuAD2) for the domain.\n",
    "\n",
    "Workflow\n",
    "\n",
    "Load Input Datasets\n",
    "\n",
    "TeleQuAD-v1 (tabular format), TeleQuAD-v4 (SQuAD-like format), and TeleQnA (MCQ-style).\n",
    "\n",
    "Convert to SQuAD Format\n",
    "\n",
    "TeleQuAD-v1: extract question, context, and answer spans, ensuring valid character offsets.\n",
    "\n",
    "TeleQuAD-v4: directly reuse SQuAD-style qas structure.\n",
    "\n",
    "TeleQnA: convert multiple-choice questions into SQuAD-style by embedding options + explanation into context, aligning the correct answer span.\n",
    "\n",
    "Merge & Save Combined Dataset\n",
    "\n",
    "Merge all QA pairs into a single SQuAD-compliant JSON file.\n",
    "\n",
    "Group by context with multiple QAs per passage.\n",
    "\n",
    "Save as combined_telecom_qa.json.\n",
    "\n",
    "Dataset Preparation\n",
    "\n",
    "Flatten into a HuggingFace Dataset object.\n",
    "\n",
    "Split into train (90%) and validation (10%).\n",
    "\n",
    "Apply RoBERTa tokenizer with stride (128) to handle long contexts.\n",
    "\n",
    "Align character-based answer spans to token positions for supervised training.\n",
    "\n",
    "Model Fine-Tuning\n",
    "\n",
    "Load deepset/roberta-base-squad2 as the base QA model.\n",
    "\n",
    "Fine-tune for 3 epochs with AdamW optimizer, learning rate 2e-5, batch size 8, and evaluation every epoch.\n",
    "\n",
    "Use HuggingFace Trainer with DefaultDataCollator.\n",
    "\n",
    "Evaluation\n",
    "\n",
    "Use SQuAD metrics (Exact Match, F1) for validation.\n",
    "\n",
    "Post-process overlapping windows to keep the longest prediction per example.\n",
    "\n",
    "Track training and validation performance, saving the best model.\n",
    "\n",
    "Model Export\n",
    "\n",
    "Save fine-tuned model and tokenizer as ./qa_roberta_telecom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4199197-f15b-4f46-b5d0-b52c042958be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7790b399-0c5c-497c-82bb-bbe1ac239293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load input files\n",
    "file_v1 = \"/mnt/data/Datasets/TeleQuAD-v1-full-Tabular.json\"\n",
    "file_v4 = \"/mnt/data/Datasets/TeleQuAD-v4-full.json\"\n",
    "file_qna = \"/mnt/data/Datasets/TeleQnA.json\"\n",
    "\n",
    "with open(file_v1, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_v1 = json.load(f)\n",
    "\n",
    "with open(file_v4, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_v4 = json.load(f)\n",
    "\n",
    "with open(file_qna, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_qna = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a76592e-4bd5-48bb-b731-9d0ca3e57918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Convert TeleQuAD-v1 to SQuAD format\n",
    "v1_qas = []\n",
    "for entry in data_v1[\"data\"]:\n",
    "    for qa in entry.get(\"questions\", []):\n",
    "        if all(k in qa for k in (\"question\", \"answer\", \"context\")):\n",
    "            context = qa[\"context\"]\n",
    "            answer_text = qa[\"answer\"]\n",
    "            start = context.lower().find(answer_text.lower())\n",
    "            if start == -1:\n",
    "                continue\n",
    "            v1_qas.append({\n",
    "                \"id\": f'v1_{qa.get(\"id\", str(uuid.uuid4()))}',\n",
    "                \"context\": context,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": {\n",
    "                    \"text\": [answer_text],\n",
    "                    \"answer_start\": [start]\n",
    "                }\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e761a8b8-4a36-4ec3-8bd7-6f84c38df58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Convert TeleQuAD-v4 to SQuAD format\n",
    "v4_qas = []\n",
    "for doc in data_v4[\"data\"]:\n",
    "    for para in doc[\"paragraphs\"]:\n",
    "        context = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            v4_qas.append({\n",
    "                \"id\": f'v4_{qa[\"id\"]}',\n",
    "                \"context\": context,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": {\n",
    "                    \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                    \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]],\n",
    "                }\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3b00ed-7432-41ad-829a-e31d73307019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Convert TeleQnA (MCQ-style) to SQuAD format\n",
    "qna_qas = []\n",
    "for key, entry in data_qna.items():\n",
    "    if \"question\" not in entry or \"answer\" not in entry:\n",
    "        continue\n",
    "    question = entry[\"question\"]\n",
    "    answer_text = entry[\"answer\"].split(\":\", 1)[-1].strip()\n",
    "    explanation = entry.get(\"explanation\", \"\")\n",
    "    options = \" \".join([entry[k] for k in entry if k.startswith(\"option\")])\n",
    "    context = f\"{question} {options} {explanation}\"\n",
    "    start = context.lower().find(answer_text.lower())\n",
    "    if start == -1:\n",
    "        continue\n",
    "    qna_qas.append({\n",
    "        \"id\": f'qna_{key}',\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"answers\": {\n",
    "            \"text\": [answer_text],\n",
    "            \"answer_start\": [start]\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73967b22-6069-4e49-a384-614623bb0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Merge and format into SQuAD-compliant structure\n",
    "all_qas = v1_qas + v4_qas + qna_qas\n",
    "grouped = defaultdict(list)\n",
    "for qa in all_qas:\n",
    "    grouped[qa[\"context\"]].append({\n",
    "        \"id\": qa[\"id\"],\n",
    "        \"question\": qa[\"question\"],\n",
    "        \"answers\": qa[\"answers\"],\n",
    "        \"is_impossible\": False\n",
    "    })\n",
    "\n",
    "squad_data = {\n",
    "    \"version\": \"telecom-combined-v1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"telecom-doc\",\n",
    "            \"paragraphs\": [{\"context\": ctx, \"qas\": qas}]\n",
    "        }\n",
    "        for ctx, qas in grouped.items()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23125cba-d627-45e6-912f-7410deebb58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 14623 QA pairs to /mnt/data/Datasets/combined_telecom_qa.json\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Save to JSON file\n",
    "output_path = \"/mnt/data/Datasets/combined_telecom_qa.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(squad_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(all_qas)} QA pairs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608b95c2-208b-4596-aa4e-68e0ee4b77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fddf5ef-b372-49e6-a108-aa406c9ca51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['id', 'question', 'context', 'answers'], 'test': ['id', 'question', 'context', 'answers']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load combined QA JSON file\n",
    "with open(\"/mnt/data/Datasets/combined_telecom_qa.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Flatten from SQuAD format\n",
    "flat_data = []\n",
    "for article in raw_data[\"data\"]:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        context = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            flat_data.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"context\": context,\n",
    "                \"answers\": qa[\"answers\"]\n",
    "            })\n",
    "\n",
    "# Convert to HuggingFace Dataset and split\n",
    "dataset = Dataset.from_list(flat_data).train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff760aa-cedf-4bba-97c5-719a232600c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def preprocess(example):\n",
    "    # Tokenize with stride to handle long contexts\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized[\"start_positions\"] = []\n",
    "    tokenized[\"end_positions\"] = []\n",
    "    tokenized[\"example_id\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"][sample_index]\n",
    "        tokenized[\"example_id\"].append(example[\"id\"][sample_index])\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized[\"start_positions\"].append(cls_index)\n",
    "            tokenized[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if offsets[token_start_index][0] > end_char or offsets[token_end_index][1] < start_char:\n",
    "                tokenized[\"start_positions\"].append(cls_index)\n",
    "                tokenized[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_pos = token_start_index - 1\n",
    "\n",
    "                while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_pos = token_end_index + 1\n",
    "\n",
    "                tokenized[\"start_positions\"].append(start_pos)\n",
    "                tokenized[\"end_positions\"].append(end_pos)\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac199fe-7587-4241-8326-7affbcc4b733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8622da64aa1544ed84b0f1425142b5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08cea867a294916b717d47df3429b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf91822-b5ae-467e-b61c-fb32a32ae068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0a1408-352e-447c-b67f-7011be291846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForQuestionAnswering, DefaultDataCollator\n",
    "\n",
    "# Load SQuAD-style metric\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    start_logits, end_logits = pred.predictions\n",
    "    start_preds = start_logits.argmax(-1)\n",
    "    end_preds = end_logits.argmax(-1)\n",
    "\n",
    "    # Group by example_id\n",
    "    formatted_preds = {}\n",
    "    for i, (start, end) in enumerate(zip(start_preds, end_preds)):\n",
    "        input_ids = tokenized_val[i][\"input_ids\"]\n",
    "        example_id = tokenized_val[i][\"example_id\"]\n",
    "        pred_text = tokenizer.decode(input_ids[start:end+1], skip_special_tokens=True)\n",
    "\n",
    "        # Use the longest prediction for overlapping windows\n",
    "        if example_id not in formatted_preds or len(pred_text) > len(formatted_preds[example_id][\"prediction_text\"]):\n",
    "            formatted_preds[example_id] = {\n",
    "                \"id\": str(example_id),\n",
    "                \"prediction_text\": pred_text\n",
    "            }\n",
    "\n",
    "    # Format references\n",
    "    formatted_references = [{\n",
    "        \"id\": str(example[\"id\"]),\n",
    "        \"answers\": example[\"answers\"]\n",
    "    } for example in val_dataset]\n",
    "\n",
    "    predictions = list(formatted_preds.values())\n",
    "\n",
    "    return squad_metric.compute(predictions=predictions, references=formatted_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d91e0d-ee6a-411d-bc43-cae98fb51deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_6131/3200547654.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_telecom_roberta\",\n",
    "    eval_strategy =\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b76041c-1b95-4d1f-8828-0dc5177faea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8886' max='8886' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8886/8886 25:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.533057</td>\n",
       "      <td>82.570062</td>\n",
       "      <td>89.812978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.542357</td>\n",
       "      <td>85.509228</td>\n",
       "      <td>91.775944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.651309</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>92.123177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8886, training_loss=0.4161456815568064, metrics={'train_runtime': 1531.5211, 'train_samples_per_second': 46.403, 'train_steps_per_second': 5.802, 'total_flos': 1.3927182458217984e+16, 'train_loss': 0.4161456815568064, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7dc275d-9608-47f8-a491-75d24036e7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qa_roberta_telecom/tokenizer_config.json',\n",
       " './qa_roberta_telecom/special_tokens_map.json',\n",
       " './qa_roberta_telecom/vocab.json',\n",
       " './qa_roberta_telecom/merges.txt',\n",
       " './qa_roberta_telecom/added_tokens.json',\n",
       " './qa_roberta_telecom/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./qa_roberta_telecom\")\n",
    "tokenizer.save_pretrained(\"./qa_roberta_telecom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5adebba0-954c-44d8-8221-91d24c8a7e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /home/ec2-user/qa_telecom_roberta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
