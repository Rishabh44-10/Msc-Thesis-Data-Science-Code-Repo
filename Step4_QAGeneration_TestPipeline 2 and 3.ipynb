{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d1d872-f47a-4259-8b5a-01762f5c5edb",
   "metadata": {},
   "source": [
    "# Test Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627f71ae-f9be-442e-a526-0e99c3fedea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load Models\n",
    "qg_tokenizer = T5Tokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "qg_model = T5ForConditionalGeneration.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\").to(\"cuda\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"/home/ec2-user/qa_roberta_telecom\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/ec2-user/qa_roberta_telecom\").to(\"cuda\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0)\n",
    "\n",
    "# Text Cleaning\n",
    "def clean_technical_text(raw_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    last_heading = None\n",
    "    boilerplate_keywords = [\"confidential\", \"etsi\", \"3gpp\", \"table of contents\", \"appendix\", \"index\", \"page\", \"internal use only\"]\n",
    "\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line or len(line.split()) < 5:\n",
    "            continue\n",
    "        if any(kw in line.lower() for kw in boilerplate_keywords):\n",
    "            continue\n",
    "        if re.match(r\"^(figure|fig\\.|table)\\s*\\d+\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.match(r\"^\\d+(\\.\\d+){0,4}\\s+[A-Z]\", line):\n",
    "            if len(line.split()) < 3:\n",
    "                continue\n",
    "            if any(kw in line.lower() for kw in boilerplate_keywords):\n",
    "                continue\n",
    "            last_heading = line\n",
    "            continue\n",
    "        if line.isupper() and len(line.split()) < 10:\n",
    "            continue\n",
    "        if last_heading:\n",
    "            line = f\"{last_heading}. {line}\"\n",
    "            last_heading = None\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \" \".join(cleaned_lines)\n",
    "\n",
    "# Chunking\n",
    "def split_with_overlap(text, max_words=200, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_words]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_words - overlap\n",
    "    return chunks\n",
    "\n",
    "# Sentence Selector\n",
    "def select_highlight_sentence(chunk, strategy=\"length\"):\n",
    "    sentences = [s.strip() for s in re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip()) if len(s.strip().split()) > 3]\n",
    "    if not sentences:\n",
    "        return None\n",
    "    if strategy == \"length\":\n",
    "        return max(sentences, key=len)\n",
    "    elif strategy == \"tfidf\":\n",
    "        try:\n",
    "            tfidf = TfidfVectorizer().fit(sentences)\n",
    "            X = tfidf.transform(sentences)\n",
    "            scores = X.mean(axis=1).A1\n",
    "            return sentences[scores.argmax()]\n",
    "        except Exception:\n",
    "            return max(sentences, key=len)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy for sentence selection\")\n",
    "\n",
    "# Question Generation\n",
    "def batch_generate_questions(inputs, max_length=64):\n",
    "    tokenized = qg_tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = qg_model.generate(**tokenized, max_length=max_length)\n",
    "    return [qg_tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "\n",
    "# QA Generation\n",
    "def generate_qa_pairs_from_paragraphs(paragraphs: list[str], source: str, batch_size=16, strategy=\"length\"):\n",
    "    qa_pairs = []\n",
    "    counters = {\"total_paragraphs\": len(paragraphs), \"qa_generated\": 0, \"qa_fallback\": 0, \"skipped_filters\": 0, \"errors\": 0}\n",
    "\n",
    "    for i in tqdm(range(0, len(paragraphs), batch_size), desc=\"Generating QA pairs\"):\n",
    "        batch_paras = paragraphs[i:i+batch_size]\n",
    "        q_inputs, batch_indices = [], []\n",
    "\n",
    "        for j, para in enumerate(batch_paras):\n",
    "            idx = i + j\n",
    "            if len(para.split()) < 5 or (re.search(r'\\d+\\.\\d+.*?:?', para) and len(para) < 400) or (sum(c.isdigit() for c in para) / max(len(para), 1) > 0.3):\n",
    "                continue\n",
    "\n",
    "            sent = select_highlight_sentence(para, strategy=strategy)\n",
    "            if not sent:\n",
    "                continue\n",
    "            highlight = para.replace(sent, f\"<hl> {sent} <hl>\")\n",
    "            q_inputs.append(f\"highlight: {highlight}\")\n",
    "            batch_indices.append((idx, para, sent, para.split(\".\")))\n",
    "\n",
    "        if not q_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions = batch_generate_questions(q_inputs)\n",
    "            questions = [{\"generated_text\": q} for q in questions]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QG batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(q_inputs)\n",
    "            continue\n",
    "\n",
    "        qa_inputs = [\n",
    "            {\"question\": q[\"generated_text\"], \"context\": para}\n",
    "            for (_, para, _, _), q in zip(batch_indices, questions)\n",
    "            if not q[\"generated_text\"].lower().startswith(\"what is the highlight\")\n",
    "        ]\n",
    "\n",
    "        if not qa_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions_list = [qa[\"question\"] for qa in qa_inputs]\n",
    "            contexts_list = [qa[\"context\"] for qa in qa_inputs]\n",
    "            answers = qa_pipeline(question=questions_list, context=contexts_list)\n",
    "            if isinstance(answers, dict):\n",
    "                answers = [answers]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QA batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(qa_inputs)\n",
    "            continue\n",
    "\n",
    "        for (idx, para, sent, sentences), question, result in zip(batch_indices, questions, answers):\n",
    "            question_text = question[\"generated_text\"]\n",
    "            if question_text.lower().startswith(\"what is the highlight\"):\n",
    "                continue\n",
    "            answer, score = result[\"answer\"].strip(), result[\"score\"]\n",
    "            if score < 0.15 or len(answer) < 4 or not any(c.isalnum() for c in answer):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in [\"yes\", \"no\", \"maybe\"]:\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if not any(answer.lower() in s.lower() for s in sentences):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "\n",
    "            qa_pairs.append({\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{para}\\n\\n### Question:\\n{question_text}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"chunk_id\": idx,\n",
    "                \"confidence\": round(score, 3)\n",
    "            })\n",
    "            counters[\"qa_generated\"] += 1\n",
    "\n",
    "    print(f\"\\U0001F50D Total Paragraphs: {counters['total_paragraphs']}\")\n",
    "    print(f\" QA Pairs: {counters['qa_generated']}\")\n",
    "    print(f\" Skipped: {counters['skipped_filters']} |  Errors: {counters['errors']}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# DOCX Processing\n",
    "def process_docx_file(input_docx_path, output_jsonl_path, strategy=\"length\"):\n",
    "    input_path = Path(input_docx_path)\n",
    "    doc = Document(input_path)\n",
    "    raw_text = \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
    "    cleaned = clean_technical_text(raw_text)\n",
    "    paragraphs = split_with_overlap(cleaned, max_words=200)\n",
    "    qa_pairs = generate_qa_pairs_from_paragraphs(paragraphs, source=input_path.name, strategy=strategy)\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"✅ Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")\n",
    "\n",
    "# Filtering Good QA Pairs\n",
    "def filter_good_qa_pairs(input_jsonl_path, output_jsonl_path):\n",
    "    qa_pairs = []\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "    qa_df[\"question\"] = qa_df[\"input\"].str.extract(r\"### Question:\\n(.+?)\\n\\n### Answer:\")\n",
    "    qa_df[\"context\"] = qa_df[\"input\"].str.extract(r\"### Context:\\n(.+?)\\n\\n### Question:\", flags=re.DOTALL)\n",
    "\n",
    "    qa_df[\"low_confidence_flag\"] = qa_df[\"confidence\"].astype(float) < 0.4\n",
    "    qa_df[\"mismatch_flag\"] = ~qa_df.apply(lambda row: row[\"output\"].lower() in row[\"context\"].lower(), axis=1)\n",
    "    qa_df[\"vague_question_flag\"] = qa_df[\"question\"].str.lower().str.startswith(\"what is the highlight\")\n",
    "\n",
    "    qa_df[\"quality\"] = \"good\"\n",
    "    qa_df.loc[qa_df[\"low_confidence_flag\"] | qa_df[\"mismatch_flag\"] | qa_df[\"vague_question_flag\"], \"quality\"] = \"poor\"\n",
    "\n",
    "    good_qa_df = qa_df[qa_df[\"quality\"] == \"good\"]\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in good_qa_df.to_dict(orient=\"records\"):\n",
    "            json.dump(record, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    print(f\" Saved {len(good_qa_df)} good QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfdea05-8fef-44d8-9186-7b12f5817bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA pairs:   0%|                               | 0/18 [00:00<?, ?it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115fa8b0> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115fa8b0> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  11%|██▌                    | 2/18 [00:02<00:15,  1.05it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  28%|██████▍                | 5/18 [00:04<00:11,  1.10it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  56%|████████████▏         | 10/18 [00:08<00:06,  1.26it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606668220> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  72%|███████████████▉      | 13/18 [00:10<00:03,  1.46it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5712044dc0> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5712044dc0> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  78%|█████████████████     | 14/18 [00:11<00:02,  1.50it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 21 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 22 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  83%|██████████████████▎   | 15/18 [00:12<00:02,  1.18it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 21 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 22 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 23 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 24 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 25 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f5606674eb0> was reported to be 16(when accessing len(dataloader)), but 26 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  94%|████████████████████▊ | 17/18 [00:13<00:00,  1.31it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 15(when accessing len(dataloader)), but 16 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 15(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 15(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 15(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f571204cd30> was reported to be 15(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs: 100%|██████████████████████| 18/18 [00:14<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Total Paragraphs: 287\n",
      "✅ QA Pairs: 83\n",
      "🚫 Skipped: 204 | ❌ Errors: 0\n",
      "✅ Saved 83 QA pairs to /mnt/data/qa_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_docx_file(\n",
    "    input_docx_path=\"/mnt/data/Datasets/38104-j00.docx\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a200f9-1f98-40bb-9dbc-67b9a5c765b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 48 good QA pairs to /mnt/data/qa_output_filtered_good.jsonl\n"
     ]
    }
   ],
   "source": [
    "filter_good_qa_pairs(\n",
    "    input_jsonl_path=\"/mnt/data/qa_output.jsonl\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output_filtered_good.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c8b930-a8ce-4809-a4f8-008f6b251f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA pairs:   0%|                               | 0/18 [00:00<?, ?it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56149541f0> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56149541f0> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  11%|██▌                    | 2/18 [00:01<00:11,  1.43it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115d7a00> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115d7a00> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  28%|██████▍                | 5/18 [00:04<00:11,  1.13it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56149541f0> was reported to be 16(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56149541f0> was reported to be 16(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  61%|█████████████▍        | 11/18 [00:08<00:05,  1.36it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  72%|███████████████▉      | 13/18 [00:10<00:03,  1.45it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115e1910> was reported to be 16(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f57115e1910> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  78%|█████████████████     | 14/18 [00:10<00:02,  1.47it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 21 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 22 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  83%|██████████████████▎   | 15/18 [00:11<00:02,  1.18it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 23 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 24 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 25 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 16(when accessing len(dataloader)), but 26 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs:  94%|████████████████████▊ | 17/18 [00:13<00:00,  1.29it/s]/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 15(when accessing len(dataloader)), but 16 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 15(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 15(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 15(when accessing len(dataloader)), but 19 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:718: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x7f56071dc040> was reported to be 15(when accessing len(dataloader)), but 20 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Generating QA pairs: 100%|██████████████████████| 18/18 [00:13<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Total Paragraphs: 287\n",
      "✅ QA Pairs: 81\n",
      "🚫 Skipped: 206 | ❌ Errors: 0\n",
      "✅ Saved 81 QA pairs to /mnt/data/qa_output_tfidf.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_docx_file(\n",
    "    input_docx_path=\"/mnt/data/Datasets/38104-j00.docx\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output_tfidf.jsonl\",\n",
    "    strategy=\"tfidf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d40a93a-b7d3-44c5-b12d-24126113d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 45 good QA pairs to /mnt/data/tfidf_qa_output_filtered_good.jsonl\n"
     ]
    }
   ],
   "source": [
    "filter_good_qa_pairs(\n",
    "    input_jsonl_path=\"/mnt/data/qa_output_tfidf.jsonl\",\n",
    "    output_jsonl_path=\"/mnt/data/tfidf_qa_output_filtered_good.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818ae49-46f1-49c8-9b6f-98748f481f58",
   "metadata": {},
   "source": [
    "# Test Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100c32d-932f-4b0e-8da9-4825c52da57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8b109-e220-48e7-ae6b-8e40ce5bba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "qg_tokenizer = T5Tokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "qg_model = T5ForConditionalGeneration.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\").to(\"cuda\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"/home/ec2-user/qa_roberta_telecom\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/ec2-user/qa_roberta_telecom\").to(\"cuda\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21aed2-64f2-4cc8-b378-4e6206ea78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "def clean_technical_text(raw_text: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    last_heading = None\n",
    "    boilerplate_keywords = [\n",
    "        \"confidential\", \"etsi\", \"3gpp\", \"table of contents\",\n",
    "        \"appendix\", \"index\", \"page\", \"internal use only\"\n",
    "    ]\n",
    "\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line or (len(line.split()) < 4 and \":\" not in line):\n",
    "            continue\n",
    "        if any(kw in line.lower() for kw in boilerplate_keywords):\n",
    "            continue\n",
    "        if re.match(r\"^(figure|fig\\.|table)\\s*\\d+\", line, re.IGNORECASE):\n",
    "            continue\n",
    "        if re.match(r\"^\\d+(\\.\\d+){0,4}\\s+[A-Z]\", line):\n",
    "            if len(line.split()) < 3:\n",
    "                continue\n",
    "            if any(kw in line.lower() for kw in boilerplate_keywords):\n",
    "                continue\n",
    "            last_heading = line\n",
    "            continue\n",
    "        if line.isupper() and len(line.split()) < 10:\n",
    "            continue\n",
    "\n",
    "        # Remove reference brackets and long citations\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\(ETSI.*?\\)\", \"\", line)\n",
    "\n",
    "        if last_heading:\n",
    "            line = f\"{last_heading}. {line}\"\n",
    "            last_heading = None\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \" \".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dcfa2-e22a-4d25-988d-0cd86e6b9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "def split_with_overlap(text, max_words=200, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_words]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_words - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df362d-3852-4af4-92bd-1f91cdd084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Selector\n",
    "\n",
    "def select_highlight_sentence(chunk, strategy=\"length\"):\n",
    "    sentences = [s.strip() for s in re.split(r'(?<=[.?!])\\s+(?=[A-Z])', chunk.strip()) if len(s.strip().split()) > 3]\n",
    "    if not sentences:\n",
    "        return None\n",
    "    if strategy == \"length\":\n",
    "        return max(sentences, key=len)\n",
    "    elif strategy == \"tfidf\":\n",
    "        try:\n",
    "            tfidf = TfidfVectorizer().fit(sentences)\n",
    "            X = tfidf.transform(sentences)\n",
    "            scores = X.mean(axis=1).A1\n",
    "            return sentences[scores.argmax()]\n",
    "        except Exception:\n",
    "            return max(sentences, key=len)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy for sentence selection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb83c3a-b01b-4564-92c3-921f035c4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Generation\n",
    "def batch_generate_questions(inputs, max_length=64):\n",
    "    tokenized = qg_tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = qg_model.generate(**tokenized, max_length=max_length)\n",
    "    return [qg_tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9faf5-66fd-481a-b6f5-303f51f5f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Generation\n",
    "def generate_qa_pairs_from_paragraphs(paragraphs: list[str], source: str, batch_size=16, strategy=\"length\"):\n",
    "    qa_pairs = []\n",
    "    counters = {\"total_paragraphs\": len(paragraphs), \"qa_generated\": 0, \"qa_fallback\": 0, \"skipped_filters\": 0, \"errors\": 0}\n",
    "\n",
    "    for i in tqdm(range(0, len(paragraphs), batch_size), desc=\"Generating QA pairs\"):\n",
    "        batch_paras = paragraphs[i:i+batch_size]\n",
    "        q_inputs, batch_indices = [], []\n",
    "\n",
    "        for j, para in enumerate(batch_paras):\n",
    "            idx = i + j\n",
    "            if len(para.split()) < 3:\n",
    "                continue\n",
    "            if re.search(r'\\d+\\.\\d+.*?:?', para) and len(para) < 400:\n",
    "                continue\n",
    "            if (sum(c.isdigit() for c in para) / max(len(para), 1)) > 0.6:\n",
    "                continue\n",
    "\n",
    "            sent = select_highlight_sentence(para, strategy=strategy)\n",
    "            if not sent:\n",
    "                continue\n",
    "            highlight = para.replace(sent, f\"<hl> {sent} <hl>\")\n",
    "            q_inputs.append(f\"highlight: {highlight}\")\n",
    "            batch_indices.append((idx, para, sent, para.split(\".\")))\n",
    "\n",
    "        if not q_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tokenized = qg_tokenizer(q_inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                output_ids = qg_model.generate(**tokenized, max_length=64, num_beams=2)\n",
    "            questions = [{\"generated_text\": qg_tokenizer.decode(ids, skip_special_tokens=True)} for ids in output_ids]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QG batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(q_inputs)\n",
    "            continue\n",
    "\n",
    "        qa_inputs = [\n",
    "            {\"question\": q[\"generated_text\"], \"context\": para}\n",
    "            for (_, para, _, _), q in zip(batch_indices, questions)\n",
    "            if not q[\"generated_text\"].lower().startswith(\"what is the highlight\")\n",
    "        ]\n",
    "\n",
    "        if not qa_inputs:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            questions_list = [qa[\"question\"] for qa in qa_inputs]\n",
    "            contexts_list = [qa[\"context\"] for qa in qa_inputs]\n",
    "            answers = qa_pipeline(question=questions_list, context=contexts_list)\n",
    "            if isinstance(answers, dict):\n",
    "                answers = [answers]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] QA batch failed: {e}\")\n",
    "            counters[\"errors\"] += len(qa_inputs)\n",
    "            continue\n",
    "\n",
    "        for (idx, para, sent, sentences), question, result in zip(batch_indices, questions, answers):\n",
    "            question_text = question[\"generated_text\"]\n",
    "            if question_text.lower().startswith(\"what is the highlight\"):\n",
    "                continue\n",
    "            answer, score = result[\"answer\"].strip(), result[\"score\"]\n",
    "            if score < 0.15 or len(answer) < 4 or not any(c.isalnum() for c in answer):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if answer.lower() in [\"yes\", \"no\", \"maybe\"]:\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "            if not any(answer.lower() in s.lower() for s in sentences):\n",
    "                counters[\"skipped_filters\"] += 1\n",
    "                continue\n",
    "\n",
    "            qa_pairs.append({\n",
    "                \"instruction\": \"Extract the correct answer span from the telecom document context.\",\n",
    "                \"input\": f\"### Task: extractive_qa\\n### Context:\\n{para}\\n\\n### Question:\\n{question_text}\\n\\n### Answer:\",\n",
    "                \"output\": answer,\n",
    "                \"source_doc\": source,\n",
    "                \"chunk_id\": idx,\n",
    "                \"confidence\": round(score, 3)\n",
    "            })\n",
    "            counters[\"qa_generated\"] += 1\n",
    "\n",
    "    print(f\"\\U0001F50D Total Paragraphs: {counters['total_paragraphs']}\")\n",
    "    print(f\" QA Pairs: {counters['qa_generated']}\")\n",
    "    print(f\" Skipped: {counters['skipped_filters']} |  Errors: {counters['errors']}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df7d41-b514-4568-8824-15330c22254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX Processing\n",
    "def process_docx_file(input_docx_path, output_jsonl_path, strategy=\"length\"):\n",
    "    \"\"\"\n",
    "    Load DOCX file, clean text, chunk with overlap, generate QA pairs, and save output.\n",
    "    Uses tighter chunking: 150 words with 50-word overlap.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_docx_path)\n",
    "    doc = Document(input_path)\n",
    "\n",
    "    # Combine and clean text\n",
    "    raw_text = \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
    "    cleaned = clean_technical_text(raw_text)\n",
    "\n",
    "    # Apply tighter chunking for better QA coverage\n",
    "    paragraphs = split_with_overlap(cleaned, max_words=150, overlap=50)\n",
    "\n",
    "    # Generate QA pairs from processed paragraphs\n",
    "    qa_pairs = generate_qa_pairs_from_paragraphs(paragraphs, source=input_path.name, strategy=strategy)\n",
    "\n",
    "    # Save as JSONL\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in qa_pairs:\n",
    "            f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\" Saved {len(qa_pairs)} QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a7cea-c5b8-4cd5-a7c7-d9327f2ae593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Good QA Pairs\n",
    "def filter_good_qa_pairs(input_jsonl_path, output_jsonl_path):\n",
    "    qa_pairs = []\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qa_pairs.append(json.loads(line))\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "    qa_df[\"question\"] = qa_df[\"input\"].str.extract(r\"### Question:\\n(.+?)\\n\\n### Answer:\")\n",
    "    qa_df[\"context\"] = qa_df[\"input\"].str.extract(r\"### Context:\\n(.+?)\\n\\n### Question:\", flags=re.DOTALL)\n",
    "\n",
    "    qa_df[\"low_confidence_flag\"] = qa_df[\"confidence\"].astype(float) < 0.4\n",
    "    qa_df[\"mismatch_flag\"] = ~qa_df.apply(lambda row: row[\"output\"].lower() in row[\"context\"].lower(), axis=1)\n",
    "    qa_df[\"vague_question_flag\"] = qa_df[\"question\"].str.lower().str.startswith(\"what is the highlight\")\n",
    "\n",
    "    qa_df[\"quality\"] = \"good\"\n",
    "    qa_df.loc[qa_df[\"low_confidence_flag\"] | qa_df[\"mismatch_flag\"] | qa_df[\"vague_question_flag\"], \"quality\"] = \"poor\"\n",
    "\n",
    "    good_qa_df = qa_df[qa_df[\"quality\"] == \"good\"]\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for record in good_qa_df.to_dict(orient=\"records\"):\n",
    "            json.dump(record, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    print(f\" Saved {len(good_qa_df)} good QA pairs to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2a160-8c4d-4559-b0c5-2da26458ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_docx_file(\n",
    "    input_docx_path=\"/mnt/data/Datasets/38104-j00.docx\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output.jsonl\",\n",
    "    strategy=\"tfidf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c81fb-5907-4c45-a532-784517a22ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_good_qa_pairs(\n",
    "    input_jsonl_path=\"/mnt/data/qa_output.jsonl\",\n",
    "    output_jsonl_path=\"/mnt/data/qa_output_filtered_good.jsonl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
