{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cad01b-d589-4d00-b374-6f323feb3ba6",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook fine-tunes a LLaMA-2 causal language model with LoRA on a JSONL corpus using 4-bit quantization for memory efficiency. The end result is a lightweight adapter-augmented model saved for downstream inference.\n",
    "\n",
    "Config & Data\n",
    "\n",
    "Paths set for base model, dataset, and output directory.\n",
    "\n",
    "Dataset loaded via datasets from testing_85.jsonl, shuffled, and split 90/5/5 (train/val/test).\n",
    "\n",
    "Tokenization\n",
    "\n",
    "LLaMA-2 tokenizer loaded; pad_token set to eos_token (required for LLaMA).\n",
    "\n",
    "Examples tokenized with truncation at max_length=384 (no padding at encode time).\n",
    "\n",
    "Columns mapped to model inputs; original text removed.\n",
    "\n",
    "Batching / Collation\n",
    "\n",
    "DataCollatorForLanguageModeling with mlm=False (causal LM objective).\n",
    "\n",
    "Dynamic padding to multiples of 64 for tensor efficiency; group_by_length=True minimizes padding.\n",
    "\n",
    "Model Loading with 4-bit + LoRA\n",
    "\n",
    "Base model loaded with BitsAndBytes: 4-bit NF4 quantization, bfloat16 compute, device_map=\"auto\".\n",
    "\n",
    "Prepared for k-bit training, gradient checkpointing enabled, use_cache=False.\n",
    "\n",
    "LoRA config: r=16, alpha=8, dropout=0.05, targeting q_proj, k_proj, v_proj, o_proj on a CAUSAL_LM task.\n",
    "\n",
    "PEFT applied; prints trainable parameter count.\n",
    "\n",
    "Training Setup\n",
    "\n",
    "TrainingArguments: 2 epochs; batch size 16 (train/eval); grad accumulation=2; cosine LR schedule; lr=2e-4; bf16=True.\n",
    "\n",
    "Eval every 1000 steps; save every 2000 (keep 3 checkpoints); optim=\"paged_adamw_8bit\".\n",
    "\n",
    "Logging to output_dir/logs; dataloader_num_workers=8; max_grad_norm=0.3; warmup_ratio=0.03.\n",
    "\n",
    "Trainer & Callback\n",
    "\n",
    "Trainer constructed with model, data, tokenizer, and collator.\n",
    "\n",
    "Custom SpeedCallback prints throughput every 20 steps.\n",
    "\n",
    "Run & Save\n",
    "\n",
    "Clears CUDA cache, runs trainer.train().\n",
    "\n",
    "Saves adapter-augmented model and tokenizer to .../llama2_qa_lora_output/final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8af8b7-5c8a-4a53-8b89-5e255b5d9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration\n",
    "model_path = \"/mnt/data/llama2-model\"  \n",
    "data_path = \"/mnt/data/testing_85.jsonl\" \n",
    "output_dir = \"/mnt/data/llama2_qa_lora_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b315b00a-5494-4e73-8242-96be8be9b294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "print(\" Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04407b4f-8739-44ad-a679-8d56ebc555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.select(range(2000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60152462-fac0-41a2-983a-82f3183990e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-Way Split\n",
    "split = dataset.train_test_split(test_size=0.10, seed=42)\n",
    "val_test = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1b5383-f2d7-4e63-945e-9065e195f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(\" Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# New version: no padding\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=384  # or 384 r\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d270bf2-6ec3-47a1-a28a-40b7812a877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=64  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fb1c65-462d-4ccc-b94c-9d36bc2c6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading LLaMA-2 with LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0219921470e84b62b9dd2b6fe8e464a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.24836028248556738\n"
     ]
    }
   ],
   "source": [
    "# Load Model with LoRA \n",
    "print(\" Loading LLaMA-2 with LoRA...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d948789a-c518-421b-aaa0-eb590e42a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Setting up training...\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments\n",
    "print(\" Setting up training...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=8,\n",
    "    group_by_length=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fd8767-27ea-48d1-a138-046c08c23471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3894/2913064862.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "from transformers import default_data_collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaab353-3961-4a8a-91d5-e1bc8304d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SpeedCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.last_time = time.time()\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 20 == 0:\n",
    "            now = time.time()\n",
    "            duration = now - self.last_time\n",
    "            print(f\"⚡ Step {state.global_step} — {20/duration:.3f} it/s\")\n",
    "            self.last_time = now\n",
    "\n",
    "trainer.add_callback(SpeedCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c184bc6e-3db1-456b-908d-3fee6b549734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3194' max='3194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3194/3194 12:57:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.228100</td>\n",
       "      <td>1.231933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.141400</td>\n",
       "      <td>1.161807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.125100</td>\n",
       "      <td>1.140821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 20 — 0.064 it/s\n",
      "⚡ Step 40 — 0.068 it/s\n",
      "⚡ Step 60 — 0.075 it/s\n",
      "⚡ Step 80 — 0.067 it/s\n",
      "⚡ Step 100 — 0.074 it/s\n",
      "⚡ Step 120 — 0.068 it/s\n",
      "⚡ Step 140 — 0.068 it/s\n",
      "⚡ Step 160 — 0.075 it/s\n",
      "⚡ Step 180 — 0.067 it/s\n",
      "⚡ Step 200 — 0.075 it/s\n",
      "⚡ Step 220 — 0.068 it/s\n",
      "⚡ Step 240 — 0.067 it/s\n",
      "⚡ Step 260 — 0.075 it/s\n",
      "⚡ Step 280 — 0.067 it/s\n",
      "⚡ Step 300 — 0.073 it/s\n",
      "⚡ Step 320 — 0.068 it/s\n",
      "⚡ Step 340 — 0.068 it/s\n",
      "⚡ Step 360 — 0.075 it/s\n",
      "⚡ Step 380 — 0.067 it/s\n",
      "⚡ Step 400 — 0.075 it/s\n",
      "⚡ Step 420 — 0.068 it/s\n",
      "⚡ Step 440 — 0.068 it/s\n",
      "⚡ Step 460 — 0.075 it/s\n",
      "⚡ Step 480 — 0.067 it/s\n",
      "⚡ Step 500 — 0.074 it/s\n",
      "⚡ Step 520 — 0.068 it/s\n",
      "⚡ Step 540 — 0.068 it/s\n",
      "⚡ Step 560 — 0.075 it/s\n",
      "⚡ Step 580 — 0.067 it/s\n",
      "⚡ Step 600 — 0.074 it/s\n",
      "⚡ Step 620 — 0.068 it/s\n",
      "⚡ Step 640 — 0.067 it/s\n",
      "⚡ Step 660 — 0.075 it/s\n",
      "⚡ Step 680 — 0.067 it/s\n",
      "⚡ Step 700 — 0.075 it/s\n",
      "⚡ Step 720 — 0.068 it/s\n",
      "⚡ Step 740 — 0.067 it/s\n",
      "⚡ Step 760 — 0.075 it/s\n",
      "⚡ Step 780 — 0.067 it/s\n",
      "⚡ Step 800 — 0.073 it/s\n",
      "⚡ Step 820 — 0.068 it/s\n",
      "⚡ Step 840 — 0.068 it/s\n",
      "⚡ Step 860 — 0.075 it/s\n",
      "⚡ Step 880 — 0.067 it/s\n",
      "⚡ Step 900 — 0.075 it/s\n",
      "⚡ Step 920 — 0.068 it/s\n",
      "⚡ Step 940 — 0.068 it/s\n",
      "⚡ Step 960 — 0.075 it/s\n",
      "⚡ Step 980 — 0.067 it/s\n",
      "⚡ Step 1000 — 0.074 it/s\n",
      "⚡ Step 1020 — 0.029 it/s\n",
      "⚡ Step 1040 — 0.068 it/s\n",
      "⚡ Step 1060 — 0.075 it/s\n",
      "⚡ Step 1080 — 0.067 it/s\n",
      "⚡ Step 1100 — 0.075 it/s\n",
      "⚡ Step 1120 — 0.068 it/s\n",
      "⚡ Step 1140 — 0.068 it/s\n",
      "⚡ Step 1160 — 0.075 it/s\n",
      "⚡ Step 1180 — 0.067 it/s\n",
      "⚡ Step 1200 — 0.074 it/s\n",
      "⚡ Step 1220 — 0.068 it/s\n",
      "⚡ Step 1240 — 0.067 it/s\n",
      "⚡ Step 1260 — 0.075 it/s\n",
      "⚡ Step 1280 — 0.067 it/s\n",
      "⚡ Step 1300 — 0.074 it/s\n",
      "⚡ Step 1320 — 0.068 it/s\n",
      "⚡ Step 1340 — 0.067 it/s\n",
      "⚡ Step 1360 — 0.075 it/s\n",
      "⚡ Step 1380 — 0.067 it/s\n",
      "⚡ Step 1400 — 0.074 it/s\n",
      "⚡ Step 1420 — 0.068 it/s\n",
      "⚡ Step 1440 — 0.068 it/s\n",
      "⚡ Step 1460 — 0.075 it/s\n",
      "⚡ Step 1480 — 0.067 it/s\n",
      "⚡ Step 1500 — 0.075 it/s\n",
      "⚡ Step 1520 — 0.068 it/s\n",
      "⚡ Step 1540 — 0.068 it/s\n",
      "⚡ Step 1560 — 0.075 it/s\n",
      "⚡ Step 1580 — 0.067 it/s\n",
      "⚡ Step 1600 — 0.076 it/s\n",
      "⚡ Step 1620 — 0.068 it/s\n",
      "⚡ Step 1640 — 0.069 it/s\n",
      "⚡ Step 1660 — 0.073 it/s\n",
      "⚡ Step 1680 — 0.067 it/s\n",
      "⚡ Step 1700 — 0.074 it/s\n",
      "⚡ Step 1720 — 0.068 it/s\n",
      "⚡ Step 1740 — 0.070 it/s\n",
      "⚡ Step 1760 — 0.073 it/s\n",
      "⚡ Step 1780 — 0.067 it/s\n",
      "⚡ Step 1800 — 0.074 it/s\n",
      "⚡ Step 1820 — 0.068 it/s\n",
      "⚡ Step 1840 — 0.070 it/s\n",
      "⚡ Step 1860 — 0.073 it/s\n",
      "⚡ Step 1880 — 0.067 it/s\n",
      "⚡ Step 1900 — 0.075 it/s\n",
      "⚡ Step 1920 — 0.068 it/s\n",
      "⚡ Step 1940 — 0.069 it/s\n",
      "⚡ Step 1960 — 0.073 it/s\n",
      "⚡ Step 1980 — 0.067 it/s\n",
      "⚡ Step 2000 — 0.074 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 2020 — 0.029 it/s\n",
      "⚡ Step 2040 — 0.069 it/s\n",
      "⚡ Step 2060 — 0.073 it/s\n",
      "⚡ Step 2080 — 0.067 it/s\n",
      "⚡ Step 2100 — 0.076 it/s\n",
      "⚡ Step 2120 — 0.068 it/s\n",
      "⚡ Step 2140 — 0.070 it/s\n",
      "⚡ Step 2160 — 0.073 it/s\n",
      "⚡ Step 2180 — 0.067 it/s\n",
      "⚡ Step 2200 — 0.075 it/s\n",
      "⚡ Step 2220 — 0.068 it/s\n",
      "⚡ Step 2240 — 0.070 it/s\n",
      "⚡ Step 2260 — 0.072 it/s\n",
      "⚡ Step 2280 — 0.067 it/s\n",
      "⚡ Step 2300 — 0.075 it/s\n",
      "⚡ Step 2320 — 0.068 it/s\n",
      "⚡ Step 2340 — 0.070 it/s\n",
      "⚡ Step 2360 — 0.073 it/s\n",
      "⚡ Step 2380 — 0.067 it/s\n",
      "⚡ Step 2400 — 0.075 it/s\n",
      "⚡ Step 2420 — 0.068 it/s\n",
      "⚡ Step 2440 — 0.069 it/s\n",
      "⚡ Step 2460 — 0.073 it/s\n",
      "⚡ Step 2480 — 0.067 it/s\n",
      "⚡ Step 2500 — 0.075 it/s\n",
      "⚡ Step 2520 — 0.068 it/s\n",
      "⚡ Step 2540 — 0.069 it/s\n",
      "⚡ Step 2560 — 0.072 it/s\n",
      "⚡ Step 2580 — 0.067 it/s\n",
      "⚡ Step 2600 — 0.075 it/s\n",
      "⚡ Step 2620 — 0.068 it/s\n",
      "⚡ Step 2640 — 0.069 it/s\n",
      "⚡ Step 2660 — 0.073 it/s\n",
      "⚡ Step 2680 — 0.067 it/s\n",
      "⚡ Step 2700 — 0.075 it/s\n",
      "⚡ Step 2720 — 0.068 it/s\n",
      "⚡ Step 2740 — 0.069 it/s\n",
      "⚡ Step 2760 — 0.073 it/s\n",
      "⚡ Step 2780 — 0.067 it/s\n",
      "⚡ Step 2800 — 0.075 it/s\n",
      "⚡ Step 2820 — 0.068 it/s\n",
      "⚡ Step 2840 — 0.070 it/s\n",
      "⚡ Step 2860 — 0.073 it/s\n",
      "⚡ Step 2880 — 0.067 it/s\n",
      "⚡ Step 2900 — 0.075 it/s\n",
      "⚡ Step 2920 — 0.068 it/s\n",
      "⚡ Step 2940 — 0.069 it/s\n",
      "⚡ Step 2960 — 0.073 it/s\n",
      "⚡ Step 2980 — 0.067 it/s\n",
      "⚡ Step 3000 — 0.075 it/s\n",
      "⚡ Step 3020 — 0.029 it/s\n",
      "⚡ Step 3040 — 0.070 it/s\n",
      "⚡ Step 3060 — 0.073 it/s\n",
      "⚡ Step 3080 — 0.067 it/s\n",
      "⚡ Step 3100 — 0.075 it/s\n",
      "⚡ Step 3120 — 0.068 it/s\n",
      "⚡ Step 3140 — 0.070 it/s\n",
      "⚡ Step 3160 — 0.073 it/s\n",
      "⚡ Step 3180 — 0.067 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3194, training_loss=1.215690178056023, metrics={'train_runtime': 46689.8955, 'train_samples_per_second': 2.189, 'train_steps_per_second': 0.068, 'total_flos': 1.4987055364791337e+18, 'train_loss': 1.215690178056023, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tart Training\n",
    "print(\" Starting fine-tuning...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04a4042-2302-4382-8083-ea2300293d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/data/llama2_qa_lora_output/final/tokenizer_config.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/special_tokens_map.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/tokenizer.model',\n",
       " '/mnt/data/llama2_qa_lora_output/final/added_tokens.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model + Tokenizer\n",
    "print(\" Saving model...\")\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
