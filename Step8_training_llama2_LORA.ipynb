{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cad01b-d589-4d00-b374-6f323feb3ba6",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook fine-tunes a LLaMA-2 causal language model with LoRA on a JSONL corpus using 4-bit quantization for memory efficiency. The end result is a lightweight adapter-augmented model saved for downstream inference.\n",
    "\n",
    "Config & Data\n",
    "\n",
    "Paths set for base model, dataset, and output directory.\n",
    "\n",
    "Dataset loaded via datasets from testing_85.jsonl, shuffled, and split 90/5/5 (train/val/test).\n",
    "\n",
    "Tokenization\n",
    "\n",
    "LLaMA-2 tokenizer loaded; pad_token set to eos_token (required for LLaMA).\n",
    "\n",
    "Examples tokenized with truncation at max_length=384 (no padding at encode time).\n",
    "\n",
    "Columns mapped to model inputs; original text removed.\n",
    "\n",
    "Batching / Collation\n",
    "\n",
    "DataCollatorForLanguageModeling with mlm=False (causal LM objective).\n",
    "\n",
    "Dynamic padding to multiples of 64 for tensor efficiency; group_by_length=True minimizes padding.\n",
    "\n",
    "Model Loading with 4-bit + LoRA\n",
    "\n",
    "Base model loaded with BitsAndBytes: 4-bit NF4 quantization, bfloat16 compute, device_map=\"auto\".\n",
    "\n",
    "Prepared for k-bit training, gradient checkpointing enabled, use_cache=False.\n",
    "\n",
    "LoRA config: r=16, alpha=8, dropout=0.05, targeting q_proj, k_proj, v_proj, o_proj on a CAUSAL_LM task.\n",
    "\n",
    "PEFT applied; prints trainable parameter count.\n",
    "\n",
    "Training Setup\n",
    "\n",
    "TrainingArguments: 2 epochs; batch size 16 (train/eval); grad accumulation=2; cosine LR schedule; lr=2e-4; bf16=True.\n",
    "\n",
    "Eval every 1000 steps; save every 2000 (keep 3 checkpoints); optim=\"paged_adamw_8bit\".\n",
    "\n",
    "Logging to output_dir/logs; dataloader_num_workers=8; max_grad_norm=0.3; warmup_ratio=0.03.\n",
    "\n",
    "Trainer & Callback\n",
    "\n",
    "Trainer constructed with model, data, tokenizer, and collator.\n",
    "\n",
    "Custom SpeedCallback prints throughput every 20 steps.\n",
    "\n",
    "Run & Save\n",
    "\n",
    "Clears CUDA cache, runs trainer.train().\n",
    "\n",
    "Saves adapter-augmented model and tokenizer to .../llama2_qa_lora_output/final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8af8b7-5c8a-4a53-8b89-5e255b5d9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration\n",
    "model_path = \"/mnt/data/llama2-model\"  \n",
    "data_path = \"/mnt/data/testing_85.jsonl\" \n",
    "output_dir = \"/mnt/data/llama2_qa_lora_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b315b00a-5494-4e73-8242-96be8be9b294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "print(\" Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04407b4f-8739-44ad-a679-8d56ebc555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.select(range(2000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60152462-fac0-41a2-983a-82f3183990e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-Way Split\n",
    "split = dataset.train_test_split(test_size=0.10, seed=42)\n",
    "val_test = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1b5383-f2d7-4e63-945e-9065e195f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(\" Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# New version: no padding\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=384  # or 384 r\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d270bf2-6ec3-47a1-a28a-40b7812a877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=64  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fb1c65-462d-4ccc-b94c-9d36bc2c6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Loading LLaMA-2 with LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0219921470e84b62b9dd2b6fe8e464a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.24836028248556738\n"
     ]
    }
   ],
   "source": [
    "# Load Model with LoRA \n",
    "print(\" Loading LLaMA-2 with LoRA...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d948789a-c518-421b-aaa0-eb590e42a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Setting up training...\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments\n",
    "print(\" Setting up training...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=8,\n",
    "    group_by_length=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fd8767-27ea-48d1-a138-046c08c23471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3894/2913064862.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "from transformers import default_data_collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaab353-3961-4a8a-91d5-e1bc8304d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SpeedCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.last_time = time.time()\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 20 == 0:\n",
    "            now = time.time()\n",
    "            duration = now - self.last_time\n",
    "            print(f\"âš¡ Step {state.global_step} â€” {20/duration:.3f} it/s\")\n",
    "            self.last_time = now\n",
    "\n",
    "trainer.add_callback(SpeedCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c184bc6e-3db1-456b-908d-3fee6b549734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3194' max='3194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3194/3194 12:57:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.228100</td>\n",
       "      <td>1.231933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.141400</td>\n",
       "      <td>1.161807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.125100</td>\n",
       "      <td>1.140821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Step 20 â€” 0.064 it/s\n",
      "âš¡ Step 40 â€” 0.068 it/s\n",
      "âš¡ Step 60 â€” 0.075 it/s\n",
      "âš¡ Step 80 â€” 0.067 it/s\n",
      "âš¡ Step 100 â€” 0.074 it/s\n",
      "âš¡ Step 120 â€” 0.068 it/s\n",
      "âš¡ Step 140 â€” 0.068 it/s\n",
      "âš¡ Step 160 â€” 0.075 it/s\n",
      "âš¡ Step 180 â€” 0.067 it/s\n",
      "âš¡ Step 200 â€” 0.075 it/s\n",
      "âš¡ Step 220 â€” 0.068 it/s\n",
      "âš¡ Step 240 â€” 0.067 it/s\n",
      "âš¡ Step 260 â€” 0.075 it/s\n",
      "âš¡ Step 280 â€” 0.067 it/s\n",
      "âš¡ Step 300 â€” 0.073 it/s\n",
      "âš¡ Step 320 â€” 0.068 it/s\n",
      "âš¡ Step 340 â€” 0.068 it/s\n",
      "âš¡ Step 360 â€” 0.075 it/s\n",
      "âš¡ Step 380 â€” 0.067 it/s\n",
      "âš¡ Step 400 â€” 0.075 it/s\n",
      "âš¡ Step 420 â€” 0.068 it/s\n",
      "âš¡ Step 440 â€” 0.068 it/s\n",
      "âš¡ Step 460 â€” 0.075 it/s\n",
      "âš¡ Step 480 â€” 0.067 it/s\n",
      "âš¡ Step 500 â€” 0.074 it/s\n",
      "âš¡ Step 520 â€” 0.068 it/s\n",
      "âš¡ Step 540 â€” 0.068 it/s\n",
      "âš¡ Step 560 â€” 0.075 it/s\n",
      "âš¡ Step 580 â€” 0.067 it/s\n",
      "âš¡ Step 600 â€” 0.074 it/s\n",
      "âš¡ Step 620 â€” 0.068 it/s\n",
      "âš¡ Step 640 â€” 0.067 it/s\n",
      "âš¡ Step 660 â€” 0.075 it/s\n",
      "âš¡ Step 680 â€” 0.067 it/s\n",
      "âš¡ Step 700 â€” 0.075 it/s\n",
      "âš¡ Step 720 â€” 0.068 it/s\n",
      "âš¡ Step 740 â€” 0.067 it/s\n",
      "âš¡ Step 760 â€” 0.075 it/s\n",
      "âš¡ Step 780 â€” 0.067 it/s\n",
      "âš¡ Step 800 â€” 0.073 it/s\n",
      "âš¡ Step 820 â€” 0.068 it/s\n",
      "âš¡ Step 840 â€” 0.068 it/s\n",
      "âš¡ Step 860 â€” 0.075 it/s\n",
      "âš¡ Step 880 â€” 0.067 it/s\n",
      "âš¡ Step 900 â€” 0.075 it/s\n",
      "âš¡ Step 920 â€” 0.068 it/s\n",
      "âš¡ Step 940 â€” 0.068 it/s\n",
      "âš¡ Step 960 â€” 0.075 it/s\n",
      "âš¡ Step 980 â€” 0.067 it/s\n",
      "âš¡ Step 1000 â€” 0.074 it/s\n",
      "âš¡ Step 1020 â€” 0.029 it/s\n",
      "âš¡ Step 1040 â€” 0.068 it/s\n",
      "âš¡ Step 1060 â€” 0.075 it/s\n",
      "âš¡ Step 1080 â€” 0.067 it/s\n",
      "âš¡ Step 1100 â€” 0.075 it/s\n",
      "âš¡ Step 1120 â€” 0.068 it/s\n",
      "âš¡ Step 1140 â€” 0.068 it/s\n",
      "âš¡ Step 1160 â€” 0.075 it/s\n",
      "âš¡ Step 1180 â€” 0.067 it/s\n",
      "âš¡ Step 1200 â€” 0.074 it/s\n",
      "âš¡ Step 1220 â€” 0.068 it/s\n",
      "âš¡ Step 1240 â€” 0.067 it/s\n",
      "âš¡ Step 1260 â€” 0.075 it/s\n",
      "âš¡ Step 1280 â€” 0.067 it/s\n",
      "âš¡ Step 1300 â€” 0.074 it/s\n",
      "âš¡ Step 1320 â€” 0.068 it/s\n",
      "âš¡ Step 1340 â€” 0.067 it/s\n",
      "âš¡ Step 1360 â€” 0.075 it/s\n",
      "âš¡ Step 1380 â€” 0.067 it/s\n",
      "âš¡ Step 1400 â€” 0.074 it/s\n",
      "âš¡ Step 1420 â€” 0.068 it/s\n",
      "âš¡ Step 1440 â€” 0.068 it/s\n",
      "âš¡ Step 1460 â€” 0.075 it/s\n",
      "âš¡ Step 1480 â€” 0.067 it/s\n",
      "âš¡ Step 1500 â€” 0.075 it/s\n",
      "âš¡ Step 1520 â€” 0.068 it/s\n",
      "âš¡ Step 1540 â€” 0.068 it/s\n",
      "âš¡ Step 1560 â€” 0.075 it/s\n",
      "âš¡ Step 1580 â€” 0.067 it/s\n",
      "âš¡ Step 1600 â€” 0.076 it/s\n",
      "âš¡ Step 1620 â€” 0.068 it/s\n",
      "âš¡ Step 1640 â€” 0.069 it/s\n",
      "âš¡ Step 1660 â€” 0.073 it/s\n",
      "âš¡ Step 1680 â€” 0.067 it/s\n",
      "âš¡ Step 1700 â€” 0.074 it/s\n",
      "âš¡ Step 1720 â€” 0.068 it/s\n",
      "âš¡ Step 1740 â€” 0.070 it/s\n",
      "âš¡ Step 1760 â€” 0.073 it/s\n",
      "âš¡ Step 1780 â€” 0.067 it/s\n",
      "âš¡ Step 1800 â€” 0.074 it/s\n",
      "âš¡ Step 1820 â€” 0.068 it/s\n",
      "âš¡ Step 1840 â€” 0.070 it/s\n",
      "âš¡ Step 1860 â€” 0.073 it/s\n",
      "âš¡ Step 1880 â€” 0.067 it/s\n",
      "âš¡ Step 1900 â€” 0.075 it/s\n",
      "âš¡ Step 1920 â€” 0.068 it/s\n",
      "âš¡ Step 1940 â€” 0.069 it/s\n",
      "âš¡ Step 1960 â€” 0.073 it/s\n",
      "âš¡ Step 1980 â€” 0.067 it/s\n",
      "âš¡ Step 2000 â€” 0.074 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Step 2020 â€” 0.029 it/s\n",
      "âš¡ Step 2040 â€” 0.069 it/s\n",
      "âš¡ Step 2060 â€” 0.073 it/s\n",
      "âš¡ Step 2080 â€” 0.067 it/s\n",
      "âš¡ Step 2100 â€” 0.076 it/s\n",
      "âš¡ Step 2120 â€” 0.068 it/s\n",
      "âš¡ Step 2140 â€” 0.070 it/s\n",
      "âš¡ Step 2160 â€” 0.073 it/s\n",
      "âš¡ Step 2180 â€” 0.067 it/s\n",
      "âš¡ Step 2200 â€” 0.075 it/s\n",
      "âš¡ Step 2220 â€” 0.068 it/s\n",
      "âš¡ Step 2240 â€” 0.070 it/s\n",
      "âš¡ Step 2260 â€” 0.072 it/s\n",
      "âš¡ Step 2280 â€” 0.067 it/s\n",
      "âš¡ Step 2300 â€” 0.075 it/s\n",
      "âš¡ Step 2320 â€” 0.068 it/s\n",
      "âš¡ Step 2340 â€” 0.070 it/s\n",
      "âš¡ Step 2360 â€” 0.073 it/s\n",
      "âš¡ Step 2380 â€” 0.067 it/s\n",
      "âš¡ Step 2400 â€” 0.075 it/s\n",
      "âš¡ Step 2420 â€” 0.068 it/s\n",
      "âš¡ Step 2440 â€” 0.069 it/s\n",
      "âš¡ Step 2460 â€” 0.073 it/s\n",
      "âš¡ Step 2480 â€” 0.067 it/s\n",
      "âš¡ Step 2500 â€” 0.075 it/s\n",
      "âš¡ Step 2520 â€” 0.068 it/s\n",
      "âš¡ Step 2540 â€” 0.069 it/s\n",
      "âš¡ Step 2560 â€” 0.072 it/s\n",
      "âš¡ Step 2580 â€” 0.067 it/s\n",
      "âš¡ Step 2600 â€” 0.075 it/s\n",
      "âš¡ Step 2620 â€” 0.068 it/s\n",
      "âš¡ Step 2640 â€” 0.069 it/s\n",
      "âš¡ Step 2660 â€” 0.073 it/s\n",
      "âš¡ Step 2680 â€” 0.067 it/s\n",
      "âš¡ Step 2700 â€” 0.075 it/s\n",
      "âš¡ Step 2720 â€” 0.068 it/s\n",
      "âš¡ Step 2740 â€” 0.069 it/s\n",
      "âš¡ Step 2760 â€” 0.073 it/s\n",
      "âš¡ Step 2780 â€” 0.067 it/s\n",
      "âš¡ Step 2800 â€” 0.075 it/s\n",
      "âš¡ Step 2820 â€” 0.068 it/s\n",
      "âš¡ Step 2840 â€” 0.070 it/s\n",
      "âš¡ Step 2860 â€” 0.073 it/s\n",
      "âš¡ Step 2880 â€” 0.067 it/s\n",
      "âš¡ Step 2900 â€” 0.075 it/s\n",
      "âš¡ Step 2920 â€” 0.068 it/s\n",
      "âš¡ Step 2940 â€” 0.069 it/s\n",
      "âš¡ Step 2960 â€” 0.073 it/s\n",
      "âš¡ Step 2980 â€” 0.067 it/s\n",
      "âš¡ Step 3000 â€” 0.075 it/s\n",
      "âš¡ Step 3020 â€” 0.029 it/s\n",
      "âš¡ Step 3040 â€” 0.070 it/s\n",
      "âš¡ Step 3060 â€” 0.073 it/s\n",
      "âš¡ Step 3080 â€” 0.067 it/s\n",
      "âš¡ Step 3100 â€” 0.075 it/s\n",
      "âš¡ Step 3120 â€” 0.068 it/s\n",
      "âš¡ Step 3140 â€” 0.070 it/s\n",
      "âš¡ Step 3160 â€” 0.073 it/s\n",
      "âš¡ Step 3180 â€” 0.067 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3194, training_loss=1.215690178056023, metrics={'train_runtime': 46689.8955, 'train_samples_per_second': 2.189, 'train_steps_per_second': 0.068, 'total_flos': 1.4987055364791337e+18, 'train_loss': 1.215690178056023, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tart Training\n",
    "print(\" Starting fine-tuning...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04a4042-2302-4382-8083-ea2300293d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/data/llama2_qa_lora_output/final/tokenizer_config.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/special_tokens_map.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/tokenizer.model',\n",
       " '/mnt/data/llama2_qa_lora_output/final/added_tokens.json',\n",
       " '/mnt/data/llama2_qa_lora_output/final/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model + Tokenizer\n",
    "print(\" Saving model...\")\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
