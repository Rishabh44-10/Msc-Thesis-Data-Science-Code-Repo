{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74aa5a08-74ec-4bd8-9e3f-c34bf2905a69",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook implements a full pipeline for preparing telecom-specific QA datasets, fine-tuning a LLaMA-2 model with LoRA adapters, and evaluating model performance. The workflow proceeds through the following stages:\n",
    "\n",
    "Dataset Preprocessing and Filtering\n",
    "\n",
    "Parse raw JSONL data into context–question–answer triples.\n",
    "\n",
    "Apply semantic chunking using SentenceTransformers to reduce overly long contexts while retaining the answer span.\n",
    "\n",
    "Enforce token length constraints (max 2048) and filter out cases where answers are lost during truncation.\n",
    "\n",
    "Save the processed dataset for downstream fine-tuning.\n",
    "(Codeblocks 1, 8)\n",
    "\n",
    "Dataset Cleaning and Golden Set Creation\n",
    "\n",
    "Remove formatting artefacts (e.g., extra tokens such as </s>).\n",
    "\n",
    "Produce a clean, “golden” dataset ready for training.\n",
    "(Codeblocks 2, 9)\n",
    "\n",
    "Fine-Tuning Setup\n",
    "\n",
    "Load the cleaned dataset into HuggingFace Dataset objects with 90/5/5 train–validation–test splits.\n",
    "\n",
    "Tokenize inputs with the LLaMA-2 tokenizer (EOS padding, max length 2048).\n",
    "\n",
    "Configure LoRA adapters on projection layers, using 4-bit quantization for efficiency.\n",
    "(Codeblocks 3, 9)\n",
    "\n",
    "Model Training\n",
    "\n",
    "Train LLaMA-2 with LoRA using HuggingFace Trainer.\n",
    "\n",
    "Apply gradient checkpointing, mixed precision (bfloat16), paged optimizers, and cosine learning rate scheduling.\n",
    "\n",
    "Track training/validation loss with TensorBoard and save checkpoints.\n",
    "\n",
    "Visualize training vs. validation loss curves.\n",
    "(Codeblocks 4, 10)\n",
    "\n",
    "Evaluation and Inference\n",
    "\n",
    "Reload the fine-tuned model for evaluation on the held-out test set.\n",
    "\n",
    "Define custom stopping criteria to prevent over-generation.\n",
    "\n",
    "Generate predictions in batch mode and compare against gold answers.\n",
    "\n",
    "Compute Exact Match (EM) and F1 scores using the SQuAD evaluation metric.\n",
    "\n",
    "Save detailed per-example results (prompts, references, predictions, scores) to CSV.\n",
    "(Codeblocks 5, 6, 7, 10)\n",
    "\n",
    "Alternative Chunking and Post-Processing Strategies\n",
    "\n",
    "Implemented both semantic similarity–based chunking and a sliding window approach to preserve answer spans.\n",
    "\n",
    "Integrated post-processing logic to clean predictions and ensure precise span extraction.\n",
    "(Codeblocks 8–10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a81c19-db83-4b06-91c2-6ef5162085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MAX_TOKEN_LENGTH = 2048 \n",
    "\n",
    "# Paths\n",
    "input_path = Path(\"/mnt/data/Second Implementation/telequad_v4_reformatted.jsonl\")\n",
    "output_path = Path(f\"/mnt/data/Second Implementation/telequad_v4_filtered_semantic_{MAX_TOKEN_LENGTH}.jsonl\")\n",
    "\n",
    "# Load tokenizer and sentence encoder\n",
    "print(\" Loading tokenizer and sentence encoder...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data/llama2-model\")\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\" Models loaded.\")\n",
    "\n",
    "# Prompt format\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a concise and knowledgeable assistant. \"\n",
    "    \"Given a technical telecom document and a question, \"\n",
    "    \"your task is to extract the exact answer from the context without adding information.\"\n",
    ")\n",
    "\n",
    "# Semantic Chunking Logic\n",
    "def select_relevant_chunks(context: str, question: str, top_k=3) -> str:\n",
    "    \"\"\"\n",
    "    Splits the context into chunks and selects the top_k most relevant ones\n",
    "    based on cosine similarity with the question.\n",
    "    \"\"\"\n",
    "    # Split context by double newlines, a common paragraph separator.\n",
    "    chunks = [chunk.strip() for chunk in context.split(\"\\n\\n\") if len(chunk.strip().split()) > 5]\n",
    "    if not chunks:\n",
    "        return context\n",
    "\n",
    "    # Encode the question and all chunks.\n",
    "    q_emb = encoder.encode([question])\n",
    "    chunk_embs = encoder.encode(chunks)\n",
    "    \n",
    "    # Calculate cosine similarity and get the top k indices.\n",
    "    scores = cosine_similarity(q_emb, chunk_embs)[0]\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # Sort the indices to maintain the original order of chunks in the document.\n",
    "    top_indices_sorted = sorted(top_indices)\n",
    "\n",
    "    return \"\\n\\n\".join([chunks[i] for i in top_indices_sorted])\n",
    "\n",
    "# Rebuild prompt from parts\n",
    "def build_prompt(context: str, question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the final prompt string in the correct Llama-2 chat format.\n",
    "    \"\"\"\n",
    "    # The user's query combines the context and the question\n",
    "    user_prompt = (\n",
    "        f\"Use the following context to answer the question:\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # Note the corrected </SYS>> and the structure.\n",
    "    return (\n",
    "        f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n\"\n",
    "        f\"{user_prompt} [/INST] {answer}</s>\"\n",
    "    )\n",
    "\n",
    "# Process entries\n",
    "print(\" Processing and filtering entries...\")\n",
    "reformatted_entries = []\n",
    "total_count = 0\n",
    "filtered_out_count = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=num_lines, desc=\"Processing file\"):\n",
    "        total_count += 1\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            original_text = entry[\"text\"]\n",
    "\n",
    "            # Parse the original text to extract context, question, and answer\n",
    "            prompt_part, answer = original_text.split(\"[/INST]\", 1)\n",
    "            answer = answer.strip().replace(\"</s>\", \"\")\n",
    "            \n",
    "            # Skip if answer is empty\n",
    "            if not answer:\n",
    "                continue\n",
    "\n",
    "            lines = prompt_part.splitlines()\n",
    "            context_lines, question = [], \"\"\n",
    "            inside_context, inside_question = False, False\n",
    "\n",
    "            for l in lines:\n",
    "                stripped_line = l.strip()\n",
    "                if stripped_line.startswith(\"Context:\"):\n",
    "                    inside_context = True\n",
    "                    inside_question = False\n",
    "                    # Capture text on the same line as \"Context:\"\n",
    "                    context_lines.append(stripped_line.replace(\"Context:\", \"\").strip())\n",
    "                    continue\n",
    "                elif stripped_line.startswith(\"Question:\"):\n",
    "                    inside_question = True\n",
    "                    inside_context = False\n",
    "                    # Capture text on the same line as \"Question:\"\n",
    "                    question = stripped_line.replace(\"Question:\", \"\").strip()\n",
    "                    continue\n",
    "                \n",
    "                if inside_context:\n",
    "                    context_lines.append(l) # Append original line to preserve formatting\n",
    "                elif inside_question and not question:\n",
    "                    question = stripped_line\n",
    "            \n",
    "            full_context = \"\\n\".join(context_lines).strip()\n",
    "\n",
    "            if not full_context or not question:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and decide whether to shorten the context\n",
    "            temp_prompt = build_prompt(full_context, question, answer)\n",
    "            input_ids = tokenizer(temp_prompt)[\"input_ids\"]\n",
    "\n",
    "            final_context = full_context\n",
    "            \n",
    "            if len(input_ids) > MAX_TOKEN_LENGTH:\n",
    "                short_context = select_relevant_chunks(full_context, question, top_k=3)\n",
    "                \n",
    "                # --- CRITICAL VALIDATION STEP ---\n",
    "                if answer in short_context:\n",
    "                    final_context = short_context\n",
    "                else:\n",
    "                    filtered_out_count += 1\n",
    "                    continue\n",
    "            \n",
    "            # Build the final, validated prompt and add it t list\n",
    "            final_prompt = build_prompt(final_context, question, answer)\n",
    "            reformatted_entries.append({\"text\": final_prompt})\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            # Catch potential errors from malformed JSON lines or text splitting.\n",
    "            print(f\"Skipping malformed line {total_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Save output\n",
    "print(\" Saving the new dataset...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for e in reformatted_entries:\n",
    "        f.write(json.dumps(e) + \"\\n\")\n",
    "\n",
    "print(\"\\n---  Processing Complete ---\")\n",
    "print(f\"Total examples processed: {total_count}\")\n",
    "print(f\"Examples kept for training: {len(reformatted_entries)}\")\n",
    "print(f\"Examples filtered out (answer lost during chunking): {filtered_out_count}\")\n",
    "print(f\" Filtered and reformatted file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72678801-c803-49af-b4e8-97e122e706cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "input_path = Path(\"/mnt/data/Second Implementation/telequad_v4_filtered_semantic_2048.jsonl\")\n",
    "# The output is new, clean \"golden\" dataset\n",
    "output_path = Path(\"/mnt/data/Second Implementation/telequad_v4_golden.jsonl\")\n",
    "\n",
    "print(f\"Input dataset: {input_path}\")\n",
    "print(f\"Output dataset: {output_path}\")\n",
    "\n",
    "cleaned_entries = []\n",
    "\n",
    "print(\" Reading and cleaning the dataset...\")\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            text = entry.get(\"text\", \"\")\n",
    "            \n",
    "            # Split the entry into the prompt and the answer\n",
    "            prompt_part, answer_part = text.split(\"[/INST]\", 1)\n",
    "            \n",
    "            # Clean the original answer\n",
    "            original_answer = answer_part.strip().replace(\"</s>\", \"\")\n",
    "            \n",
    "            # The Cleaning Logic\n",
    "            clean_answer = original_answer\n",
    "            \n",
    "            # Rebuild the text entry with the clean answer\n",
    "            new_text = f\"{prompt_part}[/INST] {clean_answer}</s>\"\n",
    "            cleaned_entries.append({\"text\": new_text})\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"Skipping malformed line: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\" Saving {len(cleaned_entries)} cleaned entries to the golden dataset...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in cleaned_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\" Golden dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ab122-bda7-453f-b279-361f661a080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load and Combine Datasets\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "v4_path = \"/mnt/data/Second Implementation/telequad_v4_golden.jsonl\"\n",
    "\n",
    "v4_data = load_jsonl(v4_path)\n",
    "\n",
    "combined_data = v4_data\n",
    "dataset = Dataset.from_list(combined_data).shuffle(seed=42)\n",
    "\n",
    "# 90/5/5 Split\n",
    "split = dataset.train_test_split(test_size=0.10, seed=42)\n",
    "val_test = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7bc3d-2848-4e5e-b314-542d2ed648e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Tokenizer\n",
    "model_path = \"/mnt/data/llama2-model\"  \n",
    "print(\" Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Padding with eos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0770080-bc6a-44f5-94c7-ceec60a797b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d44806-525f-4e82-8005-033a728348d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=64  # Padding efficiency boost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641db188-6dd8-4788-b056-2036e25f6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model with LoRA\n",
    "print(\" Loading LLaMA-2 with LoRA...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db389fe8-91f5-4aac-b232-2fe2317de406",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/mnt/data/llama2_qa_lora_output2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02850676-23cc-4557-9c75-f8aa9a7a175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "print(\" Setting up training...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    bf16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=4,\n",
    "    group_by_length=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2108c3b-f285-4b50-9ad3-f10734cc60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Setup\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fd88b-6446-4819-9ab0-1b73a758d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Speed Logging\n",
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SpeedCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.last_time = time.time()\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 20 == 0:\n",
    "            now = time.time()\n",
    "            duration = now - self.last_time\n",
    "            print(f\"⚡ Step {state.global_step} — {20/duration:.3f} it/s\")\n",
    "            self.last_time = now\n",
    "\n",
    "trainer.add_callback(SpeedCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97194495-4bb6-4778-aa03-1331c67efdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "print(\" Starting fine-tuning...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fc841-949d-4f92-a008-98209be4a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv(\"/mnt/data/loss_history.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Training Loss\", marker='o')\n",
    "plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Validation Loss\", marker='x')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff3140-5281-4e51-85e0-18d5da598395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model\n",
    "print(\" Saving model...\")\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4980f91-11d6-41a5-91af-e16f9fb73b1a",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440d3ac-d3f4-4d3a-8c40-a2d710d75ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output2/final\" \n",
    "print(\" Loading fine-tuned model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# This is the key to preventing the model from rambling after the answer.\n",
    "class StopOnNewline(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Get the last generated token\n",
    "        last_token = input_ids[0, -1]\n",
    "        # The token ID for a newline character is 13.\n",
    "        return last_token == 13\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnNewline()])\n",
    "\n",
    "# Create the pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract prompt and reference from the original test set\n",
    "def extract_prompt_and_answer(entry):\n",
    "    try:\n",
    "        text = entry[\"text\"]\n",
    "        parts = text.split(\"[/INST]\")\n",
    "        # The prompt is everything up to and including [/INST]\n",
    "        prompt = parts[0] + \"[/INST]\"\n",
    "        # The reference is the text after, with the </s> token removed\n",
    "        reference = parts[1].strip().replace(\"</s>\", \"\")\n",
    "        return {\"prompt\": prompt, \"reference\": reference}\n",
    "    except Exception:\n",
    "        return {\"prompt\": \"\", \"reference\": \"\"}\n",
    "\n",
    "print(\" Processing test set...\")\n",
    "processed = [extract_prompt_and_answer(ex) for ex in test_dataset]\n",
    "processed = [ex for ex in processed if ex[\"prompt\"].strip() and ex[\"reference\"].strip()]\n",
    "\n",
    "# Inference\n",
    "print(\" Generating predictions with stop token...\")\n",
    "predictions = []\n",
    "batch_size = 2 \n",
    "\n",
    "for i in tqdm(range(0, len(processed), batch_size)):\n",
    "    batch_prompts = [ex[\"prompt\"] for ex in processed[i:i + batch_size]]\n",
    "\n",
    "    # Add the custom stopping_criteria\n",
    "    batch_outputs = qa_pipeline(\n",
    "        batch_prompts, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # This is the new, crucial parameter\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    for out in batch_outputs:\n",
    "        gen_text = out[0][\"generated_text\"]\n",
    "        answer = gen_text.split(\"[/INST]\")[-1].strip()\n",
    "        predictions.append(answer)\n",
    "\n",
    "# Evaluation using SQuAD metric\n",
    "print(\" Calculating metrics...\")\n",
    "references = [ex[\"reference\"] for ex in processed]\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_preds = [{\"id\": str(i), \"prediction_text\": p} for i, p in enumerate(predictions)]\n",
    "formatted_refs = [{\"id\": str(i), \"answers\": {\"text\": [r], \"answer_start\": [0]}} for i, r in enumerate(references)]\n",
    "\n",
    "results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
    "print(f\"\\n Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "print(f\" F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# Save Detailed CSV\n",
    "df = pd.DataFrame({\n",
    "    \"id\": list(range(len(predictions))),\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in processed],\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "\n",
    "# Recalculate per-item scores for the dataframe\n",
    "df[\"exact_match\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"exact_match\"] for i in range(len(predictions))]\n",
    "df[\"f1\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"f1\"] for i in range(len(predictions))]\n",
    "\n",
    "csv_path = \"/mnt/data/test_dataset_eval_results_FIXED.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\" Detailed test set results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee8ca3-3fc0-464c-adad-5e1dbac5b5f3",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512034f-ac31-485b-b3d7-9b033a6deac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output2/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Use test_dataset from memory\n",
    "\n",
    "# Extract prompt and reference\n",
    "def extract_prompt_and_answer(entry):\n",
    "    try:\n",
    "        text = entry[\"text\"]\n",
    "        parts = text.split(\"[/INST]\")\n",
    "        prompt = parts[0] + \"[/INST]\"\n",
    "        answer = parts[1].strip().replace(\"</s>\", \"\") if len(parts) > 1 else \"\"\n",
    "        return {\"prompt\": prompt, \"reference\": answer}\n",
    "    except:\n",
    "        return {\"prompt\": \"\", \"reference\": \"\"}\n",
    "\n",
    "processed = [extract_prompt_and_answer(ex) for ex in test_dataset]\n",
    "processed = [ex for ex in processed if ex[\"prompt\"].strip() and ex[\"reference\"].strip()]\n",
    "\n",
    "# Inference\n",
    "print(\" Generating predictions...\")\n",
    "predictions = []\n",
    "batch_size = 2\n",
    "\n",
    "for i in tqdm(range(0, len(processed), batch_size)):\n",
    "    batch_prompts = [ex[\"prompt\"] for ex in processed[i:i + batch_size]]\n",
    "    batch_outputs = qa_pipeline(batch_prompts, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "    for out in batch_outputs:\n",
    "        gen = out[0][\"generated_text\"].split(\"[/INST]\")[-1].strip().replace(\"</s>\", \"\")\n",
    "        predictions.append(gen)\n",
    "\n",
    "# Evaluation using SQuAD metric\n",
    "references = [ex[\"reference\"] for ex in processed]\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_preds = [{\"id\": str(i), \"prediction_text\": p} for i, p in enumerate(predictions)]\n",
    "formatted_refs = [{\"id\": str(i), \"answers\": {\"text\": [r], \"answer_start\": [0]}} for i, r in enumerate(references)]\n",
    "\n",
    "results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
    "print(f\"\\n Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "print(f\" F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# Save Detailed CSV\n",
    "df = pd.DataFrame({\n",
    "    \"id\": list(range(len(predictions))),\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in processed],\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "df[\"exact_match\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"exact_match\"] for i in range(len(predictions))]\n",
    "df[\"f1\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"f1\"] for i in range(len(predictions))]\n",
    "\n",
    "csv_path = \"/mnt/data/test_dataset_eval_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\" Detailed test set results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2a009-99d3-4adf-8d39-c411af7a59ac",
   "metadata": {},
   "source": [
    "# NEW Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685393d6-b27e-4847-9a6f-f1cf99f253da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Reload model and tokenizer ---\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output2/final\" \n",
    "print(\"🧠 Loading fine-tuned model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Define a custom stopping criteria\n",
    "class StopOnNewline(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == 13 # Token ID for newline\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnNewline()])\n",
    "\n",
    "# --- Create the pipeline ---\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Load the test dataset ---\n",
    "\n",
    "def extract_prompt_and_answer(entry):\n",
    "    try:\n",
    "        text = entry[\"text\"]\n",
    "        parts = text.split(\"[/INST]\")\n",
    "        prompt = parts[0] + \"[/INST]\"\n",
    "        reference = parts[1].strip().replace(\"</s>\", \"\")\n",
    "        return {\"prompt\": prompt, \"reference\": reference}\n",
    "    except Exception:\n",
    "        return {\"prompt\": \"\", \"reference\": \"\"}\n",
    "\n",
    "print(\" Processing test set...\")\n",
    "processed = [extract_prompt_and_answer(ex) for ex in test_dataset]\n",
    "processed = [ex for ex in processed if ex[\"prompt\"].strip() and ex[\"reference\"].strip()]\n",
    "\n",
    "# --- Inference ---\n",
    "print(\"Generating predictions with aggressive post-processing...\")\n",
    "predictions = []\n",
    "batch_size = 4\n",
    "\n",
    "for i in tqdm(range(0, len(processed), batch_size)):\n",
    "    batch_prompts = [ex[\"prompt\"] for ex in processed[i:i + batch_size]]\n",
    "    \n",
    "    batch_outputs = qa_pipeline(\n",
    "        batch_prompts, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    for out in batch_outputs:\n",
    "        gen_text = out[0][\"generated_text\"]\n",
    "        answer_long = gen_text.split(\"[/INST]\")[-1].strip()\n",
    "        \n",
    "        # Prediction Processing (CORRECTED)\n",
    "        predictions.append(answer_long)\n",
    "\n",
    "# Evaluation using SQuAD metric\n",
    "print(\" Calculating metrics...\")\n",
    "references = [ex[\"reference\"] for ex in processed]\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_preds = [{\"id\": str(i), \"prediction_text\": p} for i, p in enumerate(predictions)]\n",
    "formatted_refs = [{\"id\": str(i), \"answers\": {\"text\": [r], \"answer_start\": [0]}} for i, r in enumerate(references)]\n",
    "\n",
    "results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
    "print(f\"\\n Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "print(f\" F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# Save Detailed CSV\n",
    "df = pd.DataFrame({\n",
    "    \"id\": list(range(len(predictions))),\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in processed],\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "df[\"exact_match\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"exact_match\"] for i in range(len(predictions))]\n",
    "df[\"f1\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"f1\"] for i in range(len(predictions))]\n",
    "csv_path = \"/mnt/data/test_dataset_eval_results_FINAL.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\" Detailed test set results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc2fdd-516f-453d-8593-2cbfbcc3833c",
   "metadata": {},
   "source": [
    "# Sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576021bc-71af-4ceb-ac57-2afeeffaa80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Loading tokenizer and sentence encoder...\n",
      "✅ Models loaded.\n",
      "⏳ Processing and filtering entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|█████████████████████| 4262/4262 [00:06<00:00, 707.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving the new dataset...\n",
      "\n",
      "--- 📊 Processing Complete ---\n",
      "Total examples processed: 4262\n",
      "Examples kept for training: 4258\n",
      "Examples filtered out (answer lost during chunking): 4\n",
      "✅ Filtered and reformatted file saved to: /mnt/data/Third Implementation/telequad_v4_filtered_semantic_2048.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_TOKEN_LENGTH = 2048 \n",
    "\n",
    "# --- Paths ---\n",
    "input_path = Path(\"/mnt/data/Second Implementation/telequad_v4_reformatted.jsonl\")\n",
    "output_path = Path(f\"/mnt/data/Third Implementation/telequad_v4_filtered_semantic_{MAX_TOKEN_LENGTH}.jsonl\")\n",
    "\n",
    "# --- Load tokenizer and sentence encoder ---\n",
    "print(\" Loading tokenizer and sentence encoder...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data/llama2-model\")\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\" Models loaded.\")\n",
    "\n",
    "# Prompt format\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Extract the exact answer span from the context. \"\n",
    "    \"Do not paraphrase, summarize, or add extra information. \"\n",
    "    \"The answer must appear exactly in the context.\"\n",
    ")\n",
    "\n",
    "# Semantic Chunking Logic\n",
    "def select_relevant_chunks(context: str, answer: str, window_size=150, stride=100) -> str:\n",
    "    \"\"\"\n",
    "    Sliding window approach to ensure the answer appears in the selected chunk.\n",
    "    \"\"\"\n",
    "    words = context.split()\n",
    "    for start in range(0, len(words), stride):\n",
    "        end = start + window_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        if answer in chunk:\n",
    "            return chunk\n",
    "        if end >= len(words):\n",
    "            break\n",
    "    return None\n",
    "\n",
    "# Rebuild prompt from parts\n",
    "def build_prompt(context: str, question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the final prompt string in the correct Llama-2 chat format.\n",
    "    \"\"\"\n",
    "    # The user's query combines the context and the question.\n",
    "    user_prompt = (\n",
    "        f\"Use the following context to answer the question:\\n\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"[Instruction] Extract exact answer:\"\n",
    "    )\n",
    "\n",
    "    # Note the corrected </SYS>> and the structure.\n",
    "    return (\n",
    "        f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n\"\n",
    "        f\"{user_prompt} [/INST] {answer}</s>\"\n",
    "    )\n",
    "\n",
    "# Process entries\n",
    "print(\" Processing and filtering entries...\")\n",
    "reformatted_entries = []\n",
    "total_count = 0\n",
    "filtered_out_count = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=num_lines, desc=\"Processing file\"):\n",
    "        total_count += 1\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            original_text = entry[\"text\"]\n",
    "\n",
    "            # Parse the original text to extract context, question, and answer\n",
    "            prompt_part, answer = original_text.split(\"[/INST]\", 1)\n",
    "            answer = answer.strip().replace(\"</s>\", \"\")\n",
    "            \n",
    "            # Skip if answer is empty\n",
    "            if not answer:\n",
    "                continue\n",
    "\n",
    "            lines = prompt_part.splitlines()\n",
    "            context_lines, question = [], \"\"\n",
    "            inside_context, inside_question = False, False\n",
    "\n",
    "            for l in lines:\n",
    "                stripped_line = l.strip()\n",
    "                if stripped_line.startswith(\"Context:\"):\n",
    "                    inside_context = True\n",
    "                    inside_question = False\n",
    "                    # Capture text on the same line as \"Context:\"\n",
    "                    context_lines.append(stripped_line.replace(\"Context:\", \"\").strip())\n",
    "                    continue\n",
    "                elif stripped_line.startswith(\"Question:\"):\n",
    "                    inside_question = True\n",
    "                    inside_context = False\n",
    "                    # Capture text on the same line as \"Question:\"\n",
    "                    question = stripped_line.replace(\"Question:\", \"\").strip()\n",
    "                    continue\n",
    "                \n",
    "                if inside_context:\n",
    "                    context_lines.append(l) # Append original line to preserve formatting\n",
    "                elif inside_question and not question:\n",
    "                    question = stripped_line\n",
    "            \n",
    "            full_context = \"\\n\".join(context_lines).strip()\n",
    "\n",
    "            if not full_context or not question:\n",
    "                continue\n",
    "\n",
    "            # We build a temporary prompt to check its length\n",
    "            temp_prompt = build_prompt(full_context, question, answer)\n",
    "            input_ids = tokenizer(temp_prompt)[\"input_ids\"]\n",
    "\n",
    "            final_context = full_context\n",
    "            \n",
    "            if len(input_ids) > MAX_TOKEN_LENGTH:\n",
    "                short_context = select_relevant_chunks(full_context, answer, window_size=150, stride=100)\n",
    "                \n",
    "                if short_context is not None and answer in short_context:\n",
    "                    final_context = short_context\n",
    "                else:\n",
    "                    filtered_out_count += 1\n",
    "                    continue\n",
    "\n",
    "           \n",
    "            final_prompt = build_prompt(final_context, question, answer)\n",
    "            reformatted_entries.append({\"text\": final_prompt})\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            # Catch potential errors from malformed JSON lines or text splitting.\n",
    "            print(f\"Skipping malformed line {total_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Save output\n",
    "print(\" Saving the new dataset...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for e in reformatted_entries:\n",
    "        f.write(json.dumps(e) + \"\\n\")\n",
    "\n",
    "print(\"\\n---  Processing Complete ---\")\n",
    "print(f\"Total examples processed: {total_count}\")\n",
    "print(f\"Examples kept for training: {len(reformatted_entries)}\")\n",
    "print(f\"Examples filtered out (answer lost during chunking): {filtered_out_count}\")\n",
    "print(f\" Filtered and reformatted file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74835b56-f96b-459f-9f57-7736d58308c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset: /mnt/data/Third Implementation/telequad_v4_filtered_semantic_2048.jsonl\n",
      "Output dataset: /mnt/data/Third Implementation/telequad_v4_golden.jsonl\n",
      "⏳ Reading and cleaning the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4258/4258 [00:00<00:00, 91766.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving 4258 cleaned entries to the golden dataset...\n",
      "✅ Golden dataset created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "# The input is the dataset you've been using\n",
    "input_path = Path(\"/mnt/data/Third Implementation/telequad_v4_filtered_semantic_2048.jsonl\")\n",
    "output_path = Path(\"/mnt/data/Third Implementation/telequad_v4_golden.jsonl\")\n",
    "\n",
    "print(f\"Input dataset: {input_path}\")\n",
    "print(f\"Output dataset: {output_path}\")\n",
    "\n",
    "cleaned_entries = []\n",
    "\n",
    "print(\" Reading and cleaning the dataset...\")\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            text = entry.get(\"text\", \"\")\n",
    "            \n",
    "            # Split the entry into the prompt and the answer\n",
    "            prompt_part, answer_part = text.split(\"[/INST]\", 1)\n",
    "            \n",
    "            # Clean the original answer\n",
    "            original_answer = answer_part.strip().replace(\"</s>\", \"\")\n",
    "            \n",
    "            # The Cleaning Logic (CORRECTED)\n",
    "            clean_answer = original_answer\n",
    "            \n",
    "            # Rebuild the text entry with the clean answer\n",
    "            new_text = f\"{prompt_part}[/INST] {clean_answer}</s>\"\n",
    "            cleaned_entries.append({\"text\": new_text})\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"Skipping malformed line: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\" Saving {len(cleaned_entries)} cleaned entries to the golden dataset...\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in cleaned_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\" Golden dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d152541c-518c-4043-acb3-202c58824c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load and Combine Datasets\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "v4_path = \"/mnt/data/Third Implementation/telequad_v4_golden.jsonl\"\n",
    "\n",
    "v4_data = load_jsonl(v4_path)\n",
    "\n",
    "combined_data = v4_data \n",
    "dataset = Dataset.from_list(combined_data).shuffle(seed=42)\n",
    "\n",
    "# 90/5/5 Split\n",
    "split = dataset.train_test_split(test_size=0.10, seed=42)\n",
    "val_test = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3920c22d-222b-47fe-9667-428500f90e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "model_path = \"/mnt/data/llama2-model\"  \n",
    "print(\" Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Padding with eos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7497981d-c2d1-4beb-8af8-5db19d31b6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4231a5d6c244c8c8d483f3682e0c01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92470cfa7d8d4baab853092c9ccffd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize Data\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048 \n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8095210-c296-43c8-85f2-5f932d421957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=64  # Padding efficiency boost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9cad58-a666-45ce-802f-d1dccfc0e8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading LLaMA-2 with LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5376a624a6a64bbb9ce0fa475f792ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n"
     ]
    }
   ],
   "source": [
    "# Load Model with LoRA\n",
    "print(\" Loading LLaMA-2 with LoRA...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f9407a-7581-4792-81e4-2adcd922ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/mnt/data/llama2_qa_lora_output3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dd2fa3-b3c4-44f9-85dc-ca02055de9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Setting up training...\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments\n",
    "print(\"⚙️ Setting up training...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    bf16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=4,\n",
    "    group_by_length=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=1,\n",
    "    warmup_ratio=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8684fbe4-d222-4b35-9715-f10e7c7623ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5001/796332652.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806aa560-396d-4430-aa01-b871aed9d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Speed Logging\n",
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SpeedCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.last_time = time.time()\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 20 == 0:\n",
    "            now = time.time()\n",
    "            duration = now - self.last_time\n",
    "            print(f\"⚡ Step {state.global_step} — {20/duration:.3f} it/s\")\n",
    "            self.last_time = now\n",
    "\n",
    "trainer.add_callback(SpeedCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd86ce3d-ef5e-49cb-9a6d-c0fda369de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 7:53:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.473300</td>\n",
       "      <td>1.458470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.394700</td>\n",
       "      <td>1.381514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343900</td>\n",
       "      <td>1.322064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.266200</td>\n",
       "      <td>1.284339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.217800</td>\n",
       "      <td>1.268246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.265462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 20 — 0.034 it/s\n",
      "⚡ Step 40 — 0.070 it/s\n",
      "⚡ Step 60 — 0.043 it/s\n",
      "⚡ Step 80 — 0.051 it/s\n",
      "⚡ Step 100 — 0.091 it/s\n",
      "⚡ Step 120 — 0.034 it/s\n",
      "⚡ Step 140 — 0.068 it/s\n",
      "⚡ Step 160 — 0.044 it/s\n",
      "⚡ Step 180 — 0.053 it/s\n",
      "⚡ Step 200 — 0.092 it/s\n",
      "⚡ Step 220 — 0.036 it/s\n",
      "⚡ Step 240 — 0.089 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 260 — 0.029 it/s\n",
      "⚡ Step 280 — 0.068 it/s\n",
      "⚡ Step 300 — 0.043 it/s\n",
      "⚡ Step 320 — 0.052 it/s\n",
      "⚡ Step 340 — 0.091 it/s\n",
      "⚡ Step 360 — 0.033 it/s\n",
      "⚡ Step 380 — 0.068 it/s\n",
      "⚡ Step 400 — 0.043 it/s\n",
      "⚡ Step 420 — 0.053 it/s\n",
      "⚡ Step 440 — 0.090 it/s\n",
      "⚡ Step 460 — 0.039 it/s\n",
      "⚡ Step 480 — 0.090 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 500 — 0.030 it/s\n",
      "⚡ Step 520 — 0.070 it/s\n",
      "⚡ Step 540 — 0.042 it/s\n",
      "⚡ Step 560 — 0.053 it/s\n",
      "⚡ Step 580 — 0.091 it/s\n",
      "⚡ Step 600 — 0.034 it/s\n",
      "⚡ Step 620 — 0.069 it/s\n",
      "⚡ Step 640 — 0.044 it/s\n",
      "⚡ Step 660 — 0.053 it/s\n",
      "⚡ Step 680 — 0.090 it/s\n",
      "⚡ Step 700 — 0.036 it/s\n",
      "⚡ Step 720 — 0.087 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 740 — 0.029 it/s\n",
      "⚡ Step 760 — 0.070 it/s\n",
      "⚡ Step 780 — 0.043 it/s\n",
      "⚡ Step 800 — 0.052 it/s\n",
      "⚡ Step 820 — 0.091 it/s\n",
      "⚡ Step 840 — 0.034 it/s\n",
      "⚡ Step 860 — 0.070 it/s\n",
      "⚡ Step 880 — 0.044 it/s\n",
      "⚡ Step 900 — 0.053 it/s\n",
      "⚡ Step 920 — 0.092 it/s\n",
      "⚡ Step 940 — 0.037 it/s\n",
      "⚡ Step 960 — 0.085 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 980 — 0.030 it/s\n",
      "⚡ Step 1000 — 0.069 it/s\n",
      "⚡ Step 1020 — 0.043 it/s\n",
      "⚡ Step 1040 — 0.051 it/s\n",
      "⚡ Step 1060 — 0.091 it/s\n",
      "⚡ Step 1080 — 0.033 it/s\n",
      "⚡ Step 1100 — 0.070 it/s\n",
      "⚡ Step 1120 — 0.043 it/s\n",
      "⚡ Step 1140 — 0.053 it/s\n",
      "⚡ Step 1160 — 0.093 it/s\n",
      "⚡ Step 1180 — 0.037 it/s\n",
      "⚡ Step 1200 — 0.088 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Step 1220 — 0.030 it/s\n",
      "⚡ Step 1240 — 0.070 it/s\n",
      "⚡ Step 1260 — 0.043 it/s\n",
      "⚡ Step 1280 — 0.052 it/s\n",
      "⚡ Step 1300 — 0.091 it/s\n",
      "⚡ Step 1320 — 0.032 it/s\n",
      "⚡ Step 1340 — 0.068 it/s\n",
      "⚡ Step 1360 — 0.044 it/s\n",
      "⚡ Step 1380 — 0.053 it/s\n",
      "⚡ Step 1400 — 0.090 it/s\n",
      "⚡ Step 1420 — 0.037 it/s\n",
      "⚡ Step 1440 — 0.089 it/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=1.3788109249538845, metrics={'train_runtime': 28481.5485, 'train_samples_per_second': 0.807, 'train_steps_per_second': 0.051, 'total_flos': 6.89879865187369e+17, 'train_loss': 1.3788109249538845, 'epoch': 6.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Start Training ---\n",
    "print(\"🚀 Starting fine-tuning...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53debf20-a062-46df-9f94-0e946ae6ebaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv(\"/mnt/data/loss_history1.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Training Loss\", marker='o')\n",
    "plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Validation Loss\", marker='x')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181e79f0-0152-4ef0-bce2-59dfb0f12759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/data/llama2-model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/data/llama2_qa_lora_output3/final/tokenizer_config.json',\n",
       " '/mnt/data/llama2_qa_lora_output3/final/special_tokens_map.json',\n",
       " '/mnt/data/llama2_qa_lora_output3/final/tokenizer.model',\n",
       " '/mnt/data/llama2_qa_lora_output3/final/added_tokens.json',\n",
       " '/mnt/data/llama2_qa_lora_output3/final/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Final Model\n",
    "print(\" Saving model...\")\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f62ef00f-c78c-42b2-abf7-de86c5b57475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(raw_text):\n",
    "    answer = raw_text.split(\"[/INST]\")[-1].strip()\n",
    "    answer = re.sub(r\"[^\\w\\s\\-.,:/()]\", \"\", answer)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c47cac-8476-494f-8887-5ef2e1594800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading fine-tuned model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6b31e847514648a4d821b976d83d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Processing test set...\n",
      "🔮 Generating predictions with aggressive post-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/54 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  2%|▊                                           | 1/54 [00:08<07:49,  8.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  4%|█▋                                          | 2/54 [00:17<07:28,  8.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  6%|██▍                                         | 3/54 [00:23<06:24,  7.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  7%|███▎                                        | 4/54 [00:35<07:36,  9.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  9%|████                                        | 5/54 [00:43<07:14,  8.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 11%|████▉                                       | 6/54 [00:51<06:56,  8.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 13%|█████▋                                      | 7/54 [01:06<08:13, 10.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 15%|██████▌                                     | 8/54 [01:12<06:57,  9.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 17%|███████▎                                    | 9/54 [01:17<06:02,  8.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 19%|███████▉                                   | 10/54 [01:40<09:06, 12.42s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 20%|████████▊                                  | 11/54 [01:44<07:12, 10.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 22%|█████████▌                                 | 12/54 [01:57<07:37, 10.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 24%|██████████▎                                | 13/54 [02:02<06:08,  9.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 26%|███████████▏                               | 14/54 [02:10<05:54,  8.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 28%|███████████▉                               | 15/54 [02:17<05:19,  8.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 30%|████████████▋                              | 16/54 [02:29<05:51,  9.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 31%|█████████████▌                             | 17/54 [02:35<05:10,  8.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 33%|██████████████▎                            | 18/54 [02:47<05:45,  9.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 35%|███████████████▏                           | 19/54 [02:58<05:48,  9.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 37%|███████████████▉                           | 20/54 [03:08<05:38,  9.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 39%|████████████████▋                          | 21/54 [03:16<05:08,  9.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 41%|█████████████████▌                         | 22/54 [03:24<04:41,  8.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 43%|██████████████████▎                        | 23/54 [03:33<04:39,  9.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 44%|███████████████████                        | 24/54 [03:44<04:49,  9.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 46%|███████████████████▉                       | 25/54 [03:53<04:30,  9.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 48%|████████████████████▋                      | 26/54 [04:04<04:38,  9.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 50%|█████████████████████▌                     | 27/54 [04:11<04:02,  8.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 52%|██████████████████████▎                    | 28/54 [04:18<03:37,  8.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 54%|███████████████████████                    | 29/54 [04:23<03:04,  7.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 56%|███████████████████████▉                   | 30/54 [04:39<03:59,  9.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 57%|████████████████████████▋                  | 31/54 [04:43<03:07,  8.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 59%|█████████████████████████▍                 | 32/54 [04:54<03:15,  8.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 61%|██████████████████████████▎                | 33/54 [05:00<02:53,  8.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 63%|███████████████████████████                | 34/54 [05:06<02:26,  7.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 65%|███████████████████████████▊               | 35/54 [05:13<02:20,  7.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 67%|████████████████████████████▋              | 36/54 [05:19<02:05,  6.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 69%|█████████████████████████████▍             | 37/54 [05:25<01:51,  6.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 70%|██████████████████████████████▎            | 38/54 [05:31<01:41,  6.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 72%|███████████████████████████████            | 39/54 [05:38<01:37,  6.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 74%|███████████████████████████████▊           | 40/54 [05:43<01:29,  6.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 76%|████████████████████████████████▋          | 41/54 [05:50<01:23,  6.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 78%|█████████████████████████████████▍         | 42/54 [06:07<01:55,  9.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 80%|██████████████████████████████████▏        | 43/54 [06:19<01:52, 10.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 81%|███████████████████████████████████        | 44/54 [06:22<01:21,  8.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 83%|███████████████████████████████████▊       | 45/54 [06:25<01:00,  6.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 85%|████████████████████████████████████▋      | 46/54 [06:31<00:50,  6.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 87%|█████████████████████████████████████▍     | 47/54 [06:42<00:55,  7.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 89%|██████████████████████████████████████▏    | 48/54 [06:46<00:38,  6.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 91%|███████████████████████████████████████    | 49/54 [06:49<00:28,  5.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [06:57<00:25,  6.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 94%|████████████████████████████████████████▌  | 51/54 [07:01<00:16,  5.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 96%|█████████████████████████████████████████▍ | 52/54 [07:08<00:11,  5.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 98%|██████████████████████████████████████████▏| 53/54 [07:17<00:07,  7.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|███████████████████████████████████████████| 54/54 [07:20<00:00,  8.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Calculating metrics...\n",
      "\n",
      "✅ Exact Match (EM): 9.86\n",
      "📈 F1 Score: 27.22\n",
      "✅ Detailed test set results saved to: /mnt/data/Third Implementation/test_dataset_eval_results_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Reload model and tokenizer ---\n",
    "model_path = \"/mnt/data/llama2_qa_lora_output3/final\" \n",
    "print(\" Loading fine-tuned model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Define a custom stopping criteria\n",
    "class StopOnNewline(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == 13 # Token ID for newline\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnNewline()])\n",
    "\n",
    "# Create the pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Load the test dataset\n",
    "\n",
    "def extract_prompt_and_answer(entry):\n",
    "    try:\n",
    "        text = entry[\"text\"]\n",
    "        parts = text.split(\"[/INST]\")\n",
    "        prompt = parts[0] + \"[/INST]\"\n",
    "        reference = parts[1].strip().replace(\"</s>\", \"\")\n",
    "        return {\"prompt\": prompt, \"reference\": reference}\n",
    "    except Exception:\n",
    "        return {\"prompt\": \"\", \"reference\": \"\"}\n",
    "\n",
    "print(\" Processing test set...\")\n",
    "processed = [extract_prompt_and_answer(ex) for ex in test_dataset]\n",
    "processed = [ex for ex in processed if ex[\"prompt\"].strip() and ex[\"reference\"].strip()]\n",
    "\n",
    "# Inferenc\n",
    "print(\" Generating predictions with aggressive post-processing...\")\n",
    "predictions = []\n",
    "batch_size = 4\n",
    "\n",
    "for i in tqdm(range(0, len(processed), batch_size)):\n",
    "    batch_prompts = [ex[\"prompt\"] for ex in processed[i:i + batch_size]]\n",
    "    \n",
    "    batch_outputs = qa_pipeline(\n",
    "        batch_prompts, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    for out in batch_outputs:\n",
    "        gen_text = out[0][\"generated_text\"]\n",
    "        cleaned_answer = clean_prediction(gen_text)\n",
    "        predictions.append(cleaned_answer)\n",
    "# Evaluation using SQuAD metric\n",
    "print(\"📊 Calculating metrics...\")\n",
    "references = [ex[\"reference\"] for ex in processed]\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "formatted_preds = [{\"id\": str(i), \"prediction_text\": p} for i, p in enumerate(predictions)]\n",
    "formatted_refs = [{\"id\": str(i), \"answers\": {\"text\": [r], \"answer_start\": [0]}} for i, r in enumerate(references)]\n",
    "\n",
    "results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
    "print(f\"\\n✅ Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "print(f\"📈 F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# Save Detailed CSV\n",
    "df = pd.DataFrame({\n",
    "    \"id\": list(range(len(predictions))),\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in processed],\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "df[\"exact_match\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"exact_match\"] for i in range(len(predictions))]\n",
    "df[\"f1\"] = [squad_metric.compute(predictions=[formatted_preds[i]], references=[formatted_refs[i]])[\"f1\"] for i in range(len(predictions))]\n",
    "csv_path = \"/mnt/data/Third Implementation/test_dataset_eval_results_FINAL.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\" Detailed test set results saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
